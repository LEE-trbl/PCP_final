{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:35.797799Z",
     "start_time": "2025-08-19T16:33:35.298858Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pandas seaborn numpy matplotlib scikit-learn xgboost scikit-optimize lightgbm optuna --quiet",
   "id": "583abbb57525ff0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:35.804190Z",
     "start_time": "2025-08-19T16:33:35.802559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "print (f\"numpy version: {np.__version__}\")"
   ],
   "id": "3e6d80da8b3156fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 2.3.2\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.102121Z",
     "start_time": "2025-08-19T16:33:35.812242Z"
    }
   },
   "source": "!pip3 freeze > requirements.txt",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.108974Z",
     "start_time": "2025-08-19T16:33:36.106274Z"
    }
   },
   "cell_type": "code",
   "source": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport sklearn\nimport xgboost\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nimport random as rn\nRANDOM_SEED = 2025\nnp.random.seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\nfrom datetime import datetime\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\n\n# ==========================================================\n# 상수 정의\n# ==========================================================\nBUILDING_COL = \"building_number\"\nTIME_COL = \"date_time\"\nTARGET_COL = \"power_consumption\"\n\n# ============================================================\n# 1) Feature Sets (corrected to match feature engineering)\n# ============================================================\nTYPE1_BASE = [\n    \"temperature\", \"humidity\", \"windspeed\",\n    \"day_of_week\", \"month\", \"week\", \"is_holiday\",\n    \"sin_hour\", \"cos_hour\",\n    \"day_hour_mean\", \"day_hour_std\",\n    \"holiday_hour_mean\", \"holiday_std\",\n    \"THI\", \"WCT\", \"CDH\", \"is_peak_season\"\n]\n\nTYPE1_DETAIL = [\n    \"summer_sin\", \"summer_cos\", \"day_max_temperature\", \"day_min_temperature\",\n    \"day_mean_temperature\", \"day_diff_temperature\"\n]\n\nTYPE2_BASE = [\n    \"temperature\", \"humidity\", \"windspeed\",\n    \"day_of_week\", \"month\", \"week\", \"is_holiday\",\n    \"sin_hour\", \"cos_hour\",\n    \"day_hour_mean\", \"day_hour_std\",\n    \"holiday_hour_mean\", \"holiday_std\",\n    \"THI\", \"WCT\", \"CDH\", \"is_peak_season\",\n    \"total_area\", \"cooling_area\",\n    \"building_number\", \"building_type\"\n]\n\nTYPE2_DETAIL = [\n    \"summer_sin\", \"summer_cos\", \"day_max_temperature\", \"day_min_temperature\",\n    \"day_mean_temperature\", \"day_diff_temperature\", \"pv_temp\", \"ess_pcs_std\"\n]",
   "id": "2e94096b17ece852",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Load and Preprocessing",
   "id": "bd7f30f251e8936f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.195259Z",
     "start_time": "2025-08-19T16:33:36.111826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('../data/raw/train.csv')\n",
    "test = pd.read_csv('../data/raw/test.csv')\n",
    "building_info = pd.read_csv('../data/raw/building_info.csv')"
   ],
   "id": "613d6e4be9f2a0ce",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.222225Z",
     "start_time": "2025-08-19T16:33:36.198337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = train.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(°C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "})\n",
    "# train.drop('num_date_time', axis = 1, inplace=True)\n",
    "\n",
    "test = test.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(°C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "})\n",
    "# test.drop('num_date_time', axis = 1, inplace=True)\n",
    "\n",
    "building_info = building_info.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '건물유형': 'building_type',\n",
    "    '연면적(m2)': 'total_area',\n",
    "    '냉방면적(m2)': 'cooling_area',\n",
    "    '태양광용량(kW)': 'solar_power_capacity',\n",
    "    'ESS저장용량(kWh)': 'ess_capacity',\n",
    "    'PCS용량(kW)': 'pcs_capacity'\n",
    "})\n",
    "\n",
    "translation_dict = {\n",
    "    '건물기타': 'Other Buildings',\n",
    "    '공공': 'Public',\n",
    "    '학교': 'University',\n",
    "    '백화점': 'Department Store',\n",
    "    '병원': 'Hospital',\n",
    "    '상용': 'Commercial',\n",
    "    '아파트': 'Apartment',\n",
    "    '연구소': 'Research Institute',\n",
    "    'IDC(전화국)': 'IDC',\n",
    "    '호텔': 'Hotel'\n",
    "}\n",
    "\n",
    "building_info['building_type'] = building_info['building_type'].replace(translation_dict)\n",
    "\n",
    "building_info['solar_power_utility_binary'] = np.where(building_info.solar_power_capacity !='-',1,0)\n",
    "building_info['ess_utility_binary'] = np.where(building_info.ess_capacity !='-',1,0)\n",
    "\n",
    "building_info['solar_power_capacity'] = building_info['solar_power_capacity'].replace('-', '0').astype(float)\n",
    "building_info['ess_capacity'] = building_info['ess_capacity'].replace('-', '0').astype(float)\n",
    "\n",
    "train = pd.merge(train, building_info, on='building_number', how='left')\n",
    "test = pd.merge(test, building_info, on='building_number', how='left')"
   ],
   "id": "489a63aa46c3731c",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.226782Z",
     "start_time": "2025-08-19T16:33:36.225437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ========================\n",
    "# 이상치 마킹\n",
    "# ========================"
   ],
   "id": "fc6de531817f8b6b",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.234449Z",
     "start_time": "2025-08-19T16:33:36.230345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# outlier_detect 컬럼 초기화 (0: 정상, 1: 이상치)\n",
    "train['outlier_detect'] = 0\n",
    "test['outlier_detect'] = 0\n",
    "\n",
    "# power_consumption이 0인 경우를 이상치로 마킹 (앞뒤 1포인트 포함)\n",
    "zero_outlier_indices = train.index[train['power_consumption'] == 0].tolist()\n",
    "\n",
    "# 앞뒤 1포인트도 포함하여 확장된 이상치 인덱스 생성\n",
    "expanded_outlier_indices = set()\n",
    "for idx in zero_outlier_indices:\n",
    "    # 현재 인덱스와 앞뒤 1포인트 추가\n",
    "    for offset in [-1, 0, 1]:\n",
    "        new_idx = idx + offset\n",
    "        if new_idx >= 0 and new_idx < len(train):  # 인덱스 범위 체크\n",
    "            expanded_outlier_indices.add(new_idx)\n",
    "\n",
    "# 확장된 인덱스들을 이상치로 마킹\n",
    "train.loc[list(expanded_outlier_indices), 'outlier_detect'] = 1\n",
    "\n",
    "# 이상치 개수 확인\n",
    "outlier_count = (train['outlier_detect'] == 1).sum()\n",
    "total_count = len(train)\n",
    "print(f\"총 데이터 개수: {total_count}\")\n",
    "print(f\"power_consumption이 0인 원본 이상치 개수: {len(zero_outlier_indices)}\")\n",
    "print(f\"앞뒤 1포인트 포함 확장된 이상치 개수: {outlier_count}\")\n",
    "print(f\"확장된 이상치 비율: {outlier_count/total_count*100:.2f}%\")"
   ],
   "id": "8038ed907596a509",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터 개수: 204000\n",
      "power_consumption이 0인 원본 이상치 개수: 68\n",
      "앞뒤 1포인트 포함 확장된 이상치 개수: 106\n",
      "확장된 이상치 비율: 0.05%\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.320850Z",
     "start_time": "2025-08-19T16:33:36.238225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# IQR 방식을 이용한 이상치 탐지 (앞뒤 1포인트 포함)\n",
    "def detect_outliers_iqr_expanded(df, column, building_col='building_number'):\n",
    "    \"\"\"\n",
    "    IQR 방식으로 이상치를 탐지하는 함수\n",
    "    건물별로 따로 계산하여 더 정확한 이상치 탐지\n",
    "    이상치 발견 시 앞뒤 1포인트도 함께 이상치로 마킹\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "\n",
    "    for building in df[building_col].unique():\n",
    "        building_mask = df[building_col] == building\n",
    "        building_data = df[building_mask][column]\n",
    "        building_indices = df[building_mask].index.tolist()\n",
    "        building_indices_set = set(building_indices)  # 빠른 검색을 위해 set 사용\n",
    "\n",
    "        # 0이 아닌 값들만으로 IQR 계산 (0은 이미 이상치로 마킹됨)\n",
    "        non_zero_data = building_data[building_data > 0]\n",
    "\n",
    "        if len(non_zero_data) > 0:\n",
    "            Q1 = non_zero_data.quantile(0.25)\n",
    "            Q3 = non_zero_data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # IQR 기반 경계값 계산\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # 해당 건물의 이상치 인덱스 찾기 (0값은 제외, 이미 마킹됨)\n",
    "            building_outliers_mask = ((building_data < lower_bound) | (building_data > upper_bound)) & (building_data > 0)\n",
    "            original_outlier_indices = building_data[building_outliers_mask].index.tolist()\n",
    "\n",
    "            # 앞뒤 1포인트 확장\n",
    "            expanded_outlier_indices = set()\n",
    "            for idx in original_outlier_indices:\n",
    "                # 현재 인덱스와 앞뒤 1포인트 추가\n",
    "                for offset in [-1, 0, 1]:\n",
    "                    new_idx = idx + offset\n",
    "                    # 인덱스 범위 체크 및 같은 건물 내의 인덱스인지 확인\n",
    "                    if 0 <= new_idx < len(df) and new_idx in building_indices_set:\n",
    "                        expanded_outlier_indices.add(new_idx)\n",
    "\n",
    "            outlier_indices.extend(list(expanded_outlier_indices))\n",
    "\n",
    "            print(f\"건물 {building}: Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}\")\n",
    "            print(f\"  Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
    "            print(f\"  원본 IQR 이상치 개수: {len(original_outlier_indices)}\")\n",
    "            print(f\"  확장된 IQR 이상치 개수: {len(expanded_outlier_indices)}\")\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "# power_consumption에 대해 IQR 이상치 탐지 (확장 버전)\n",
    "print(\"=== IQR 방식 이상치 탐지 (앞뒤 1포인트 포함) ===\")\n",
    "iqr_outlier_indices = detect_outliers_iqr_expanded(train, 'power_consumption')\n",
    "\n",
    "# IQR 이상치를 outlier_detect에 마킹 (기존 0값 이상치와 합쳐짐)\n",
    "train.loc[iqr_outlier_indices, 'outlier_detect'] = 1\n",
    "\n",
    "# 최종 이상치 개수 확인\n",
    "total_outliers = (train['outlier_detect'] == 1).sum()\n",
    "zero_outliers = len(zero_outlier_indices)  # 원본 0값 이상치 개수\n",
    "iqr_outliers = len(iqr_outlier_indices)\n",
    "\n",
    "print(f\"\\n=== 최종 이상치 현황 ===\")\n",
    "print(f\"power_consumption=0 원본 이상치: {zero_outliers}개\")\n",
    "print(f\"IQR 방식 확장 이상치: {iqr_outliers}개\")\n",
    "print(f\"총 이상치 개수: {total_outliers}개\")\n",
    "print(f\"전체 데이터 대비 이상치 비율: {total_outliers/len(train)*100:.2f}%\")"
   ],
   "id": "cbc417b4899ce38b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IQR 방식 이상치 탐지 (앞뒤 1포인트 포함) ===\n",
      "건물 1: Q1=4679.16, Q3=6005.83, IQR=1326.67\n",
      "  Lower bound: 2689.16, Upper bound: 7995.83\n",
      "  원본 IQR 이상치 개수: 15\n",
      "  확장된 IQR 이상치 개수: 37\n",
      "건물 2: Q1=1263.11, Q3=1975.37, IQR=712.26\n",
      "  Lower bound: 194.72, Upper bound: 3043.76\n",
      "  원본 IQR 이상치 개수: 8\n",
      "  확장된 IQR 이상치 개수: 24\n",
      "건물 3: Q1=13938.36, Q3=21272.79, IQR=7334.43\n",
      "  Lower bound: 2936.72, Upper bound: 32274.44\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 4: Q1=663.60, Q3=1089.51, IQR=425.91\n",
      "  Lower bound: 24.74, Upper bound: 1728.38\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 5: Q1=3794.64, Q3=5225.52, IQR=1430.88\n",
      "  Lower bound: 1648.32, Upper bound: 7371.84\n",
      "  원본 IQR 이상치 개수: 1\n",
      "  확장된 IQR 이상치 개수: 3\n",
      "건물 6: Q1=1951.08, Q3=4635.36, IQR=2684.28\n",
      "  Lower bound: -2075.34, Upper bound: 8661.78\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 7: Q1=1737.19, Q3=4978.09, IQR=3240.90\n",
      "  Lower bound: -3124.15, Upper bound: 9839.43\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 8: Q1=547.80, Q3=936.60, IQR=388.80\n",
      "  Lower bound: -35.40, Upper bound: 1519.80\n",
      "  원본 IQR 이상치 개수: 37\n",
      "  확장된 IQR 이상치 개수: 62\n",
      "건물 9: Q1=1785.41, Q3=3362.04, IQR=1576.62\n",
      "  Lower bound: -579.52, Upper bound: 5726.97\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 10: Q1=4824.86, Q3=10982.41, IQR=6157.55\n",
      "  Lower bound: -4411.47, Upper bound: 20218.74\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 11: Q1=853.51, Q3=1492.42, IQR=638.91\n",
      "  Lower bound: -104.85, Upper bound: 2450.79\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 12: Q1=4885.32, Q3=7136.28, IQR=2250.96\n",
      "  Lower bound: 1508.88, Upper bound: 10512.72\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 13: Q1=1855.62, Q3=2408.28, IQR=552.66\n",
      "  Lower bound: 1026.63, Upper bound: 3237.27\n",
      "  원본 IQR 이상치 개수: 58\n",
      "  확장된 IQR 이상치 개수: 126\n",
      "건물 14: Q1=3139.51, Q3=5313.28, IQR=2173.78\n",
      "  Lower bound: -121.16, Upper bound: 8573.95\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 15: Q1=2655.24, Q3=3489.00, IQR=833.76\n",
      "  Lower bound: 1404.60, Upper bound: 4739.64\n",
      "  원본 IQR 이상치 개수: 112\n",
      "  확장된 IQR 이상치 개수: 183\n",
      "건물 16: Q1=764.58, Q3=1356.72, IQR=592.14\n",
      "  Lower bound: -123.63, Upper bound: 2244.93\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 17: Q1=2033.52, Q3=3116.88, IQR=1083.36\n",
      "  Lower bound: 408.48, Upper bound: 4741.92\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 18: Q1=584.82, Q3=2006.64, IQR=1421.82\n",
      "  Lower bound: -1547.91, Upper bound: 4139.37\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 19: Q1=459.00, Q3=2740.41, IQR=2281.41\n",
      "  Lower bound: -2963.11, Upper bound: 6162.52\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 20: Q1=1779.39, Q3=2088.81, IQR=309.42\n",
      "  Lower bound: 1315.26, Upper bound: 2552.94\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 21: Q1=2523.60, Q3=3300.82, IQR=777.22\n",
      "  Lower bound: 1357.76, Upper bound: 4466.66\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 22: Q1=1673.19, Q3=2558.61, IQR=885.42\n",
      "  Lower bound: 345.06, Upper bound: 3886.74\n",
      "  원본 IQR 이상치 개수: 12\n",
      "  확장된 IQR 이상치 개수: 24\n",
      "건물 23: Q1=1436.52, Q3=2731.68, IQR=1295.16\n",
      "  Lower bound: -506.22, Upper bound: 4674.42\n",
      "  원본 IQR 이상치 개수: 246\n",
      "  확장된 IQR 이상치 개수: 339\n",
      "건물 24: Q1=1107.74, Q3=1669.06, IQR=561.32\n",
      "  Lower bound: 265.77, Upper bound: 2511.03\n",
      "  원본 IQR 이상치 개수: 6\n",
      "  확장된 IQR 이상치 개수: 16\n",
      "건물 25: Q1=831.78, Q3=1549.44, IQR=717.66\n",
      "  Lower bound: -244.71, Upper bound: 2625.93\n",
      "  원본 IQR 이상치 개수: 27\n",
      "  확장된 IQR 이상치 개수: 45\n",
      "건물 26: Q1=752.52, Q3=2936.29, IQR=2183.77\n",
      "  Lower bound: -2523.13, Upper bound: 6211.94\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 27: Q1=527.76, Q3=2089.49, IQR=1561.73\n",
      "  Lower bound: -1814.83, Upper bound: 4432.07\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 28: Q1=640.58, Q3=983.92, IQR=343.35\n",
      "  Lower bound: 125.55, Upper bound: 1498.95\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 29: Q1=521.64, Q3=1460.34, IQR=938.70\n",
      "  Lower bound: -886.41, Upper bound: 2868.39\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 30: Q1=9665.28, Q3=9911.52, IQR=246.24\n",
      "  Lower bound: 9295.92, Upper bound: 10280.88\n",
      "  원본 IQR 이상치 개수: 56\n",
      "  확장된 IQR 이상치 개수: 78\n",
      "건물 31: Q1=606.60, Q3=958.28, IQR=351.68\n",
      "  Lower bound: 79.09, Upper bound: 1485.79\n",
      "  원본 IQR 이상치 개수: 2\n",
      "  확장된 IQR 이상치 개수: 4\n",
      "건물 32: Q1=544.74, Q3=2323.20, IQR=1778.46\n",
      "  Lower bound: -2122.95, Upper bound: 4990.89\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 33: Q1=1183.89, Q3=2306.32, IQR=1122.43\n",
      "  Lower bound: -499.76, Upper bound: 3989.98\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 34: Q1=5067.48, Q3=14419.18, IQR=9351.70\n",
      "  Lower bound: -8960.08, Upper bound: 28446.74\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 35: Q1=9262.80, Q3=9571.14, IQR=308.34\n",
      "  Lower bound: 8800.29, Upper bound: 10033.65\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 36: Q1=13542.72, Q3=14114.64, IQR=571.92\n",
      "  Lower bound: 12684.84, Upper bound: 14972.52\n",
      "  원본 IQR 이상치 개수: 12\n",
      "  확장된 IQR 이상치 개수: 18\n",
      "건물 37: Q1=1535.52, Q3=3604.92, IQR=2069.40\n",
      "  Lower bound: -1568.58, Upper bound: 6709.02\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 38: Q1=934.11, Q3=1528.92, IQR=594.81\n",
      "  Lower bound: 41.89, Upper bound: 2421.14\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 39: Q1=1715.02, Q3=2657.47, IQR=942.45\n",
      "  Lower bound: 301.35, Upper bound: 4071.15\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 40: Q1=533.25, Q3=1700.14, IQR=1166.89\n",
      "  Lower bound: -1217.09, Upper bound: 3450.49\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 41: Q1=2686.59, Q3=2739.74, IQR=53.14\n",
      "  Lower bound: 2606.87, Upper bound: 2819.45\n",
      "  원본 IQR 이상치 개수: 5\n",
      "  확장된 IQR 이상치 개수: 13\n",
      "건물 42: Q1=3610.80, Q3=4861.26, IQR=1250.46\n",
      "  Lower bound: 1735.11, Upper bound: 6736.95\n",
      "  원본 IQR 이상치 개수: 1\n",
      "  확장된 IQR 이상치 개수: 3\n",
      "건물 43: Q1=13723.65, Q3=14345.40, IQR=621.75\n",
      "  Lower bound: 12791.02, Upper bound: 15278.02\n",
      "  원본 IQR 이상치 개수: 19\n",
      "  확장된 IQR 이상치 개수: 37\n",
      "건물 44: Q1=1427.04, Q3=2123.16, IQR=696.12\n",
      "  Lower bound: 382.86, Upper bound: 3167.34\n",
      "  원본 IQR 이상치 개수: 2\n",
      "  확장된 IQR 이상치 개수: 5\n",
      "건물 45: Q1=1736.16, Q3=7931.16, IQR=6195.00\n",
      "  Lower bound: -7556.34, Upper bound: 17223.66\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 46: Q1=819.75, Q3=1209.30, IQR=389.55\n",
      "  Lower bound: 235.42, Upper bound: 1793.63\n",
      "  원본 IQR 이상치 개수: 34\n",
      "  확장된 IQR 이상치 개수: 62\n",
      "건물 47: Q1=511.38, Q3=1468.35, IQR=956.97\n",
      "  Lower bound: -924.07, Upper bound: 2903.80\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 48: Q1=3588.47, Q3=4296.31, IQR=707.85\n",
      "  Lower bound: 2526.70, Upper bound: 5358.08\n",
      "  원본 IQR 이상치 개수: 13\n",
      "  확장된 IQR 이상치 개수: 29\n",
      "건물 49: Q1=617.45, Q3=1499.38, IQR=881.93\n",
      "  Lower bound: -705.45, Upper bound: 2822.29\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 50: Q1=1414.45, Q3=2223.37, IQR=808.92\n",
      "  Lower bound: 201.07, Upper bound: 3436.75\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 51: Q1=3194.76, Q3=3372.00, IQR=177.24\n",
      "  Lower bound: 2928.90, Upper bound: 3637.86\n",
      "  원본 IQR 이상치 개수: 1\n",
      "  확장된 IQR 이상치 개수: 3\n",
      "건물 52: Q1=4982.82, Q3=5171.52, IQR=188.70\n",
      "  Lower bound: 4699.77, Upper bound: 5454.57\n",
      "  원본 IQR 이상치 개수: 176\n",
      "  확장된 IQR 이상치 개수: 227\n",
      "건물 53: Q1=1393.92, Q3=1869.48, IQR=475.56\n",
      "  Lower bound: 680.58, Upper bound: 2582.82\n",
      "  원본 IQR 이상치 개수: 22\n",
      "  확장된 IQR 이상치 개수: 32\n",
      "건물 54: Q1=1836.59, Q3=3903.03, IQR=2066.44\n",
      "  Lower bound: -1263.06, Upper bound: 7002.68\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 55: Q1=8409.42, Q3=9804.96, IQR=1395.54\n",
      "  Lower bound: 6316.11, Upper bound: 11898.27\n",
      "  원본 IQR 이상치 개수: 5\n",
      "  확장된 IQR 이상치 개수: 13\n",
      "건물 56: Q1=5433.00, Q3=5712.24, IQR=279.24\n",
      "  Lower bound: 5014.14, Upper bound: 6131.10\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 57: Q1=16646.79, Q3=19123.58, IQR=2476.78\n",
      "  Lower bound: 12931.61, Upper bound: 22838.75\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 58: Q1=914.67, Q3=2332.57, IQR=1417.90\n",
      "  Lower bound: -1212.19, Upper bound: 4459.43\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 59: Q1=466.79, Q3=1536.53, IQR=1069.74\n",
      "  Lower bound: -1137.83, Upper bound: 3141.14\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 60: Q1=3137.31, Q3=4486.77, IQR=1349.46\n",
      "  Lower bound: 1113.12, Upper bound: 6510.96\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 61: Q1=931.08, Q3=2871.84, IQR=1940.76\n",
      "  Lower bound: -1980.06, Upper bound: 5782.98\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 62: Q1=1505.52, Q3=2058.39, IQR=552.87\n",
      "  Lower bound: 676.22, Upper bound: 2887.69\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 63: Q1=639.09, Q3=1740.78, IQR=1101.69\n",
      "  Lower bound: -1013.45, Upper bound: 3393.32\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 64: Q1=10688.40, Q3=11209.50, IQR=521.10\n",
      "  Lower bound: 9906.75, Upper bound: 11991.15\n",
      "  원본 IQR 이상치 개수: 165\n",
      "  확장된 IQR 이상치 개수: 281\n",
      "건물 65: Q1=27.20, Q3=47.11, IQR=19.91\n",
      "  Lower bound: -2.67, Upper bound: 76.98\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 66: Q1=1555.00, Q3=2678.00, IQR=1123.00\n",
      "  Lower bound: -129.49, Upper bound: 4362.50\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 67: Q1=10407.60, Q3=11214.90, IQR=807.30\n",
      "  Lower bound: 9196.65, Upper bound: 12425.85\n",
      "  원본 IQR 이상치 개수: 52\n",
      "  확장된 IQR 이상치 개수: 58\n",
      "건물 68: Q1=1454.78, Q3=2436.73, IQR=981.95\n",
      "  Lower bound: -18.15, Upper bound: 3909.66\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 69: Q1=6882.78, Q3=8794.97, IQR=1912.19\n",
      "  Lower bound: 4014.49, Upper bound: 11663.26\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 70: Q1=1804.80, Q3=3090.84, IQR=1286.04\n",
      "  Lower bound: -124.26, Upper bound: 5019.90\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 71: Q1=825.12, Q3=1346.40, IQR=521.28\n",
      "  Lower bound: 43.20, Upper bound: 2128.32\n",
      "  원본 IQR 이상치 개수: 20\n",
      "  확장된 IQR 이상치 개수: 40\n",
      "건물 72: Q1=1134.78, Q3=1619.70, IQR=484.92\n",
      "  Lower bound: 407.40, Upper bound: 2347.08\n",
      "  원본 IQR 이상치 개수: 1\n",
      "  확장된 IQR 이상치 개수: 3\n",
      "건물 73: Q1=634.44, Q3=3876.96, IQR=3242.52\n",
      "  Lower bound: -4229.34, Upper bound: 8740.74\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 74: Q1=1270.52, Q3=4166.32, IQR=2895.80\n",
      "  Lower bound: -3073.18, Upper bound: 8510.03\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 75: Q1=2454.03, Q3=3454.20, IQR=1000.17\n",
      "  Lower bound: 953.77, Upper bound: 4954.46\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 76: Q1=2880.36, Q3=3364.20, IQR=483.84\n",
      "  Lower bound: 2154.60, Upper bound: 4089.96\n",
      "  원본 IQR 이상치 개수: 2\n",
      "  확장된 IQR 이상치 개수: 6\n",
      "건물 77: Q1=1040.20, Q3=1595.78, IQR=555.57\n",
      "  Lower bound: 206.85, Upper bound: 2429.13\n",
      "  원본 IQR 이상치 개수: 1\n",
      "  확장된 IQR 이상치 개수: 3\n",
      "건물 78: Q1=1084.68, Q3=1730.34, IQR=645.66\n",
      "  Lower bound: 116.19, Upper bound: 2698.83\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 79: Q1=2181.42, Q3=9703.98, IQR=7522.56\n",
      "  Lower bound: -9102.42, Upper bound: 20987.82\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 80: Q1=1756.87, Q3=2632.86, IQR=875.98\n",
      "  Lower bound: 442.89, Upper bound: 3946.83\n",
      "  원본 IQR 이상치 개수: 10\n",
      "  확장된 IQR 이상치 개수: 17\n",
      "건물 81: Q1=1180.80, Q3=1283.10, IQR=102.30\n",
      "  Lower bound: 1027.35, Upper bound: 1436.55\n",
      "  원본 IQR 이상치 개수: 74\n",
      "  확장된 IQR 이상치 개수: 108\n",
      "건물 82: Q1=515.28, Q3=1851.18, IQR=1335.90\n",
      "  Lower bound: -1488.57, Upper bound: 3855.03\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 83: Q1=2408.67, Q3=2818.26, IQR=409.59\n",
      "  Lower bound: 1794.28, Upper bound: 3432.65\n",
      "  원본 IQR 이상치 개수: 4\n",
      "  확장된 IQR 이상치 개수: 12\n",
      "건물 84: Q1=1205.94, Q3=1976.94, IQR=771.00\n",
      "  Lower bound: 49.44, Upper bound: 3133.44\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 85: Q1=17.44, Q3=34.46, IQR=17.02\n",
      "  Lower bound: -8.09, Upper bound: 59.99\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 86: Q1=649.98, Q3=2254.32, IQR=1604.34\n",
      "  Lower bound: -1756.53, Upper bound: 4660.83\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 87: Q1=1020.83, Q3=1855.80, IQR=834.97\n",
      "  Lower bound: -231.64, Upper bound: 3108.26\n",
      "  원본 IQR 이상치 개수: 21\n",
      "  확장된 IQR 이상치 개수: 41\n",
      "건물 88: Q1=828.96, Q3=4510.08, IQR=3681.12\n",
      "  Lower bound: -4692.72, Upper bound: 10031.76\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 89: Q1=6962.89, Q3=8472.52, IQR=1509.63\n",
      "  Lower bound: 4698.44, Upper bound: 10736.97\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 90: Q1=1685.10, Q3=2531.62, IQR=846.53\n",
      "  Lower bound: 415.31, Upper bound: 3801.41\n",
      "  원본 IQR 이상치 개수: 1\n",
      "  확장된 IQR 이상치 개수: 3\n",
      "건물 91: Q1=1230.90, Q3=1928.04, IQR=697.14\n",
      "  Lower bound: 185.19, Upper bound: 2973.75\n",
      "  원본 IQR 이상치 개수: 9\n",
      "  확장된 IQR 이상치 개수: 19\n",
      "건물 92: Q1=282.60, Q3=654.00, IQR=371.40\n",
      "  Lower bound: -274.50, Upper bound: 1211.10\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 93: Q1=1126.44, Q3=1812.06, IQR=685.62\n",
      "  Lower bound: 98.01, Upper bound: 2840.49\n",
      "  원본 IQR 이상치 개수: 7\n",
      "  확장된 IQR 이상치 개수: 15\n",
      "건물 94: Q1=1380.96, Q3=2060.88, IQR=679.92\n",
      "  Lower bound: 361.08, Upper bound: 3080.76\n",
      "  원본 IQR 이상치 개수: 20\n",
      "  확장된 IQR 이상치 개수: 41\n",
      "건물 95: Q1=925.80, Q3=3573.12, IQR=2647.32\n",
      "  Lower bound: -3045.18, Upper bound: 7544.10\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 96: Q1=684.24, Q3=1130.46, IQR=446.22\n",
      "  Lower bound: 14.91, Upper bound: 1799.79\n",
      "  원본 IQR 이상치 개수: 16\n",
      "  확장된 IQR 이상치 개수: 32\n",
      "건물 97: Q1=1210.80, Q3=1655.52, IQR=444.72\n",
      "  Lower bound: 543.72, Upper bound: 2322.60\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 98: Q1=1553.22, Q3=2331.18, IQR=777.96\n",
      "  Lower bound: 386.28, Upper bound: 3498.12\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "건물 99: Q1=989.91, Q3=1180.89, IQR=190.98\n",
      "  Lower bound: 703.44, Upper bound: 1467.36\n",
      "  원본 IQR 이상치 개수: 29\n",
      "  확장된 IQR 이상치 개수: 53\n",
      "건물 100: Q1=1218.33, Q3=2395.17, IQR=1176.84\n",
      "  Lower bound: -546.93, Upper bound: 4160.43\n",
      "  원본 IQR 이상치 개수: 0\n",
      "  확장된 IQR 이상치 개수: 0\n",
      "\n",
      "=== 최종 이상치 현황 ===\n",
      "power_consumption=0 원본 이상치: 68개\n",
      "IQR 방식 확장 이상치: 2115개\n",
      "총 이상치 개수: 2199개\n",
      "전체 데이터 대비 이상치 비율: 1.08%\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.369078Z",
     "start_time": "2025-08-19T16:33:36.326286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# 이상치 제거 원상복구\n",
    "# ==========================================================\n",
    "\n",
    "# 이상치 제거 원상복구 설정\n",
    "exclude_outlier_removal_buildings = [1, # 호텔\n",
    "                                     13, 15, 23, 94, # 연구소\n",
    "                                     8, 22, 46, 55, 87, # 학교\n",
    "                                     71, 25, 91, 93, #아파트\n",
    "                                     2, 51, 99, # 상용\n",
    "                                     30, 37, 43, 52, 64, # IDC(전화국)\n",
    "                                     96, # 건물기타\n",
    "                                     ]  # 이상치 제거를 하지 않을 건물 번호 리스트\n",
    "\n",
    "# 지정된 건물의 outlier_detect를 0으로 설정 (원상복구)\n",
    "for building_id in exclude_outlier_removal_buildings:\n",
    "    if building_id in train['building_number'].unique():\n",
    "        mask = train['building_number'] == building_id\n",
    "        train.loc[mask, 'outlier_detect'] = 0\n",
    "        print(f\"Building {building_id}: {mask.sum()} rows restored (outlier_detect set to 0)\")\n",
    "    else:\n",
    "        print(f\"Warning: Building {building_id} not found in dataset\")\n",
    "\n",
    "# 현재 이상치 상태 확인\n",
    "print(\"\\n=== 현재 이상치 마킹 상태 ===\")\n",
    "outlier_count_by_building = train.groupby('building_number')['outlier_detect'].sum().sort_index()\n",
    "total_outliers = train['outlier_detect'].sum()\n",
    "total_rows = len(train)\n",
    "\n",
    "print(f\"전체 데이터: {total_rows:,} rows\")\n",
    "print(f\"전체 이상치: {total_outliers:,} rows ({total_outliers/total_rows*100:.2f}%)\")\n",
    "print(f\"\\n건물별 이상치 개수:\")\n",
    "for building_id, count in outlier_count_by_building.items():\n",
    "    building_total = len(train[train['building_number'] == building_id])\n",
    "    print(f\"  Building {building_id:2d}: {count:4d} outliers ({count/building_total*100:5.2f}%)\")\n"
   ],
   "id": "ebb546a0e98bc543",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 1: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 13: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 15: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 23: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 94: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 8: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 22: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 46: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 55: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 87: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 71: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 25: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 91: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 93: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 2: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 51: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 99: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 30: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 37: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 43: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 52: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 64: 2040 rows restored (outlier_detect set to 0)\n",
      "Building 96: 2040 rows restored (outlier_detect set to 0)\n",
      "\n",
      "=== 현재 이상치 마킹 상태 ===\n",
      "전체 데이터: 204,000 rows\n",
      "전체 이상치: 407 rows (0.20%)\n",
      "\n",
      "건물별 이상치 개수:\n",
      "  Building  1:    0 outliers ( 0.00%)\n",
      "  Building  2:    0 outliers ( 0.00%)\n",
      "  Building  3:    0 outliers ( 0.00%)\n",
      "  Building  4:    0 outliers ( 0.00%)\n",
      "  Building  5:    3 outliers ( 0.15%)\n",
      "  Building  6:    0 outliers ( 0.00%)\n",
      "  Building  7:    0 outliers ( 0.00%)\n",
      "  Building  8:    0 outliers ( 0.00%)\n",
      "  Building  9:    0 outliers ( 0.00%)\n",
      "  Building 10:    0 outliers ( 0.00%)\n",
      "  Building 11:    0 outliers ( 0.00%)\n",
      "  Building 12:    0 outliers ( 0.00%)\n",
      "  Building 13:    0 outliers ( 0.00%)\n",
      "  Building 14:    0 outliers ( 0.00%)\n",
      "  Building 15:    0 outliers ( 0.00%)\n",
      "  Building 16:    0 outliers ( 0.00%)\n",
      "  Building 17:   19 outliers ( 0.93%)\n",
      "  Building 18:    0 outliers ( 0.00%)\n",
      "  Building 19:    0 outliers ( 0.00%)\n",
      "  Building 20:    0 outliers ( 0.00%)\n",
      "  Building 21:    0 outliers ( 0.00%)\n",
      "  Building 22:    0 outliers ( 0.00%)\n",
      "  Building 23:    0 outliers ( 0.00%)\n",
      "  Building 24:   16 outliers ( 0.78%)\n",
      "  Building 25:    0 outliers ( 0.00%)\n",
      "  Building 26:    0 outliers ( 0.00%)\n",
      "  Building 27:    0 outliers ( 0.00%)\n",
      "  Building 28:    0 outliers ( 0.00%)\n",
      "  Building 29:    7 outliers ( 0.34%)\n",
      "  Building 30:    0 outliers ( 0.00%)\n",
      "  Building 31:    4 outliers ( 0.20%)\n",
      "  Building 32:    0 outliers ( 0.00%)\n",
      "  Building 33:    0 outliers ( 0.00%)\n",
      "  Building 34:    0 outliers ( 0.00%)\n",
      "  Building 35:    0 outliers ( 0.00%)\n",
      "  Building 36:   18 outliers ( 0.88%)\n",
      "  Building 37:    0 outliers ( 0.00%)\n",
      "  Building 38:    0 outliers ( 0.00%)\n",
      "  Building 39:    0 outliers ( 0.00%)\n",
      "  Building 40:    0 outliers ( 0.00%)\n",
      "  Building 41:   13 outliers ( 0.64%)\n",
      "  Building 42:    3 outliers ( 0.15%)\n",
      "  Building 43:    0 outliers ( 0.00%)\n",
      "  Building 44:    5 outliers ( 0.25%)\n",
      "  Building 45:    0 outliers ( 0.00%)\n",
      "  Building 46:    0 outliers ( 0.00%)\n",
      "  Building 47:    0 outliers ( 0.00%)\n",
      "  Building 48:   29 outliers ( 1.42%)\n",
      "  Building 49:    0 outliers ( 0.00%)\n",
      "  Building 50:    0 outliers ( 0.00%)\n",
      "  Building 51:    0 outliers ( 0.00%)\n",
      "  Building 52:    0 outliers ( 0.00%)\n",
      "  Building 53:   32 outliers ( 1.57%)\n",
      "  Building 54:    0 outliers ( 0.00%)\n",
      "  Building 55:    0 outliers ( 0.00%)\n",
      "  Building 56:    0 outliers ( 0.00%)\n",
      "  Building 57:    0 outliers ( 0.00%)\n",
      "  Building 58:    0 outliers ( 0.00%)\n",
      "  Building 59:    0 outliers ( 0.00%)\n",
      "  Building 60:    0 outliers ( 0.00%)\n",
      "  Building 61:    0 outliers ( 0.00%)\n",
      "  Building 62:    0 outliers ( 0.00%)\n",
      "  Building 63:    0 outliers ( 0.00%)\n",
      "  Building 64:    0 outliers ( 0.00%)\n",
      "  Building 65:    0 outliers ( 0.00%)\n",
      "  Building 66:    0 outliers ( 0.00%)\n",
      "  Building 67:   58 outliers ( 2.84%)\n",
      "  Building 68:    3 outliers ( 0.15%)\n",
      "  Building 69:    0 outliers ( 0.00%)\n",
      "  Building 70:   26 outliers ( 1.27%)\n",
      "  Building 71:    0 outliers ( 0.00%)\n",
      "  Building 72:    4 outliers ( 0.20%)\n",
      "  Building 73:    0 outliers ( 0.00%)\n",
      "  Building 74:    0 outliers ( 0.00%)\n",
      "  Building 75:    0 outliers ( 0.00%)\n",
      "  Building 76:    9 outliers ( 0.44%)\n",
      "  Building 77:    3 outliers ( 0.15%)\n",
      "  Building 78:    3 outliers ( 0.15%)\n",
      "  Building 79:    0 outliers ( 0.00%)\n",
      "  Building 80:   17 outliers ( 0.83%)\n",
      "  Building 81:  108 outliers ( 5.29%)\n",
      "  Building 82:    0 outliers ( 0.00%)\n",
      "  Building 83:   12 outliers ( 0.59%)\n",
      "  Building 84:    0 outliers ( 0.00%)\n",
      "  Building 85:    0 outliers ( 0.00%)\n",
      "  Building 86:    0 outliers ( 0.00%)\n",
      "  Building 87:    0 outliers ( 0.00%)\n",
      "  Building 88:    3 outliers ( 0.15%)\n",
      "  Building 89:    0 outliers ( 0.00%)\n",
      "  Building 90:    3 outliers ( 0.15%)\n",
      "  Building 91:    0 outliers ( 0.00%)\n",
      "  Building 92:    3 outliers ( 0.15%)\n",
      "  Building 93:    0 outliers ( 0.00%)\n",
      "  Building 94:    0 outliers ( 0.00%)\n",
      "  Building 95:    0 outliers ( 0.00%)\n",
      "  Building 96:    0 outliers ( 0.00%)\n",
      "  Building 97:    3 outliers ( 0.15%)\n",
      "  Building 98:    3 outliers ( 0.15%)\n",
      "  Building 99:    0 outliers ( 0.00%)\n",
      "  Building 100:    0 outliers ( 0.00%)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:36.783074Z",
     "start_time": "2025-08-19T16:33:36.374004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# 특정 기간 이상치 마킹\n",
    "# ==========================================================\n",
    "\n",
    "def mark_outlier_periods(df):\n",
    "    \"\"\"\n",
    "    reference/xgb_type.py에서 확인한 특정 기간들을 이상치로 마킹\n",
    "    기존에는 데이터를 drop했지만, 여기서는 outlier_detect=1로 마킹\n",
    "\n",
    "    기간 형식:\n",
    "    - 날짜만: ['2024-07-07', '2024-07-08']  # 전체 날짜\n",
    "    - 시간까지: ['2024-07-07 14:00', '2024-07-08 09:00']  # 특정 시간대\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== 특정 기간 이상치 마킹 시작 ===\")\n",
    "\n",
    "    # date_time 컬럼이 datetime 타입인지 확인하고 변환\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date_time']):\n",
    "        print(\"date_time을 datetime 타입으로 변환 중...\")\n",
    "        df['date_time'] = pd.to_datetime(df['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "    initial_outliers = (df['outlier_detect'] == 1).sum()\n",
    "\n",
    "    # 1. 건물기타 (Other Buildings) 등.\n",
    "    other_buildings_config = [\n",
    "        {'building_id': 7, 'periods': [\n",
    "            ['2024-07-07 10:00', '2024-07-08 11:00'],\n",
    "            ['2024-07-12 14:00', '2024-08-06 03:00']\n",
    "        ]},\n",
    "        {'building_id': 26, 'periods': [\n",
    "            ['2024-06-17 14:00', '2024-06-18 11:00']\n",
    "        ]},\n",
    "        {'building_id': 69, 'periods': [\n",
    "            ['2024-06-02', '2024-06-02']\n",
    "        ]},\n",
    "        {'building_id': 82, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-17 14:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 2. IDC(전화국) 등\n",
    "    idc_config = [\n",
    "        {'building_id': 30, 'periods': [\n",
    "            ['2024-07-13 20:00', '2024-07-13 20:00'],\n",
    "            ['2024-07-25 00:00', '2024-07-25 00:00']\n",
    "        ]},\n",
    "        {'building_id': 36, 'periods': [\n",
    "            ['2024-07-21 00:00', '2024-07-21 23:00']\n",
    "        ]},\n",
    "        {'building_id': 43, 'periods': [\n",
    "            ['2024-06-10 17:00', '2024-06-10 18:00'],\n",
    "            ['2024-08-12 16:00', '2024-08-12 17:00']\n",
    "        ]},\n",
    "        {'building_id': 52, 'periods': [\n",
    "            ['2024-08-10 00:00', '2024-08-10 02:00']\n",
    "        ]},\n",
    "        {'building_id': 57, 'periods': [\n",
    "            ['2024-06-01', '2024-06-07']\n",
    "        ]},\n",
    "        {'building_id': 67, 'periods': [\n",
    "            ['2024-07-26', '2024-07-27'],\n",
    "            ['2024-08-01 15:00', '2024-08-01 17:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 3. 병원 (Hospital) 등.\n",
    "    hospital_config = [\n",
    "        {'building_id': 17, 'periods': [\n",
    "            ['2024-06-25 20:00', '2024-06-26 08:00']\n",
    "        ]},\n",
    "        {'building_id': 44, 'periods': [\n",
    "            ['2024-06-06 12:00', '2024-06-06 14:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 4. 상용 (Commercial) 등\n",
    "    commercial_config = [\n",
    "        {'building_id': 20, 'periods': [\n",
    "            ['2024-06-01 10:00', '2024-06-01 11:00'],\n",
    "            ['2024-06-09 10:00', '2024-06-09 10:00']\n",
    "        ]},\n",
    "        {'building_id': 41, 'periods': [\n",
    "            ['2024-07-17 09:00', '2024-07-17 15:00']\n",
    "        ]},\n",
    "        {'building_id': 51, 'periods': [\n",
    "            ['2024-06-30', '2024-06-30']\n",
    "        ]},\n",
    "    ]\n",
    "\n",
    "    # 5. 아파트 (Apartment) 등\n",
    "    apartment_config = [\n",
    "        {'building_id': 25, 'periods': [\n",
    "            ['2024-07-04 12:00', '2024-07-04 14:00']\n",
    "        ]},\n",
    "        {'building_id': 65, 'periods': [\n",
    "            ['2024-06-01', '2024-06-09']\n",
    "        ]},\n",
    "        {'building_id': 70, 'periods': [\n",
    "            ['2024-06-04 09:00', '2024-06-05 08:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 6. 연구소 (Research Institute) 등.\n",
    "    research_config = [\n",
    "        {'building_id': 49, 'periods': [\n",
    "            ['2024-06-15 09:00', '2024-06-15 11:00'],\n",
    "            ['2024-07-06', '2024-07-07'],\n",
    "            ['2024-08-17', '2024-08-18'],\n",
    "            ['2024-08-22', '2024-08-22'],\n",
    "        ]},\n",
    "        {'building_id': 53, 'periods': [\n",
    "            ['2024-06-14 16:00', '2024-06-17 10:00'],\n",
    "            ['2024-08-18 15:00', '2024-08-19 09:00']\n",
    "        ]},\n",
    "        {'building_id': 83, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-17 14:00'],\n",
    "        ]},\n",
    "        {'building_id': 94, 'periods': [\n",
    "            ['2024-07-26 18:00', '2024-08-05 05:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 7. 학교 (University) 등.\n",
    "    university_config = [\n",
    "        {'building_id': 8, 'periods': [\n",
    "            ['2024-07-21 08:00', '2024-07-21 11:00'],\n",
    "            ['2024-08-24 09:00', '2024-08-24 23:00']\n",
    "        ]},\n",
    "        {'building_id': 12, 'periods': [\n",
    "            ['2024-07-21 08:00', '2024-07-21 11:00'],\n",
    "            ['2024-08-24 08:00', '2024-08-24 10:00']\n",
    "        ]},\n",
    "        {'building_id': 55, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-17 14:00'],\n",
    "        ]},\n",
    "        {'building_id': 87, 'periods': [\n",
    "            ['2024-06-01', '2024-06-30']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 8. 호텔 (Hotel) 등\n",
    "    hotel_config = [\n",
    "        {'building_id': 89, 'periods': [\n",
    "            ['2024-07-12 00:00', '2024-07-12 23:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 9. 공공 (Public) 등\n",
    "    public_config = [\n",
    "        {'building_id': 38, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-17 15:00']\n",
    "        ]},\n",
    "        {'building_id': 72, 'periods': [\n",
    "            ['2024-06-11 00:00', '2024-06-11 02:00']\n",
    "        ]},\n",
    "        {'building_id': 92, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-18 04:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 10. 백화점 (Department Store) 등\n",
    "    department_store_config = [\n",
    "        {'building_id': 19, 'periods': [\n",
    "            ['2024-07-31 13:00', '2024-07-31 16:00']\n",
    "        ]},\n",
    "        {'building_id': 32, 'periods': [\n",
    "            ['2024-07-08 09:00', '2024-07-08 10:00']\n",
    "        ]},\n",
    "        {'building_id': 40, 'periods': [\n",
    "            ['2024-07-14 00:00', '2024-07-14 01:00']\n",
    "        ]},\n",
    "        {'building_id': 45, 'periods': [\n",
    "            ['2024-07-04 00:00', '2024-07-04 03:00']\n",
    "        ]},\n",
    "        {'building_id': 73, 'periods': [\n",
    "            ['2024-07-08 22:00', '2024-07-08 22:00']\n",
    "        ]},\n",
    "        {'building_id': 79, 'periods': [\n",
    "            ['2024-08-19 03:00', '2024-08-19 05:00']\n",
    "        ]},\n",
    "        {'building_id': 95, 'periods': [\n",
    "            ['2024-08-05 10:00', '2024-08-05 11:00']\n",
    "        ]},\n",
    "        # ===================== 월요일 제거 ==========================\n",
    "        #\n",
    "        # {'building_id': 19, 'periods': [\n",
    "        #     ['2024-06-10 00:00', '2024-06-10 23:00'],\n",
    "        #     ['2024-07-08 00:00', '2024-07-08 23:00'],\n",
    "        #     ['2024-08-19 00:00', '2024-08-19 23:00'],\n",
    "        # ]},\n",
    "        #\n",
    "        # {'building_id': 45, 'periods': [\n",
    "        #     ['2024-06-10 00:00', '2024-06-10 23:00'],\n",
    "        #     ['2024-07-08 00:00', '2024-07-08 23:00'],\n",
    "        #     ['2024-08-19 00:00', '2024-08-19 23:00'],\n",
    "        # ]},\n",
    "        #\n",
    "        # {'building_id': 54, 'periods': [\n",
    "        #     ['2024-06-17 00:00', '2024-06-17 23:00'],\n",
    "        #     ['2024-07-01 00:00', '2024-07-01 23:00'],\n",
    "        #     ['2024-08-19 00:00', '2024-08-19 23:00'],\n",
    "        # ]},\n",
    "        #\n",
    "        # {'building_id': 74, 'periods': [\n",
    "        #     ['2024-06-17 00:00', '2024-06-17 23:00'],\n",
    "        #     ['2024-07-01 00:00', '2024-07-01 23:00'],\n",
    "        # ]},\n",
    "        #\n",
    "        # {'building_id': 79, 'periods': [\n",
    "        #     ['2024-06-17 00:00', '2024-06-17 23:00'],\n",
    "        #     ['2024-07-01 00:00', '2024-07-01 23:00'],\n",
    "        #     ['2024-08-19 00:00', '2024-08-19 23:00'],\n",
    "        # ]},\n",
    "        # {'building_id': 95, 'periods': [\n",
    "        #     ['2024-07-08 00:00', '2024-07-08 23:00'],\n",
    "        #     ['2024-08-05 00:00', '2024-08-05 23:00'],\n",
    "        # ]},\n",
    "    ]\n",
    "\n",
    "    # 모든 설정을 통합 (10개 건물 타입 전체)\n",
    "    all_configs = [\n",
    "        ('건물기타 (Other Buildings)', other_buildings_config),\n",
    "        ('IDC(전화국)', idc_config),\n",
    "        ('병원 (Hospital)', hospital_config),\n",
    "        ('상용 (Commercial)', commercial_config),\n",
    "        ('아파트 (Apartment)', apartment_config),\n",
    "        ('연구소 (Research Institute)', research_config),\n",
    "        ('학교 (University)', university_config),\n",
    "        ('호텔 (Hotel)', hotel_config),\n",
    "        ('공공 (Public)', public_config),\n",
    "        ('백화점 (Department Store)', department_store_config)\n",
    "    ]\n",
    "\n",
    "    total_marked = 0\n",
    "\n",
    "    for building_type_name, configs in all_configs:\n",
    "        print(f\"\\\\n--- {building_type_name} 처리 ---\")\n",
    "        type_marked = 0\n",
    "\n",
    "        if not configs:  # 빈 리스트인 경우\n",
    "            print(f\"  {building_type_name}: 설정된 이상치 기간 없음\")\n",
    "            continue\n",
    "\n",
    "        for config in configs:\n",
    "            building_id = config['building_id']\n",
    "            periods = config['periods']\n",
    "\n",
    "            building_marked = 0\n",
    "            for period in periods:\n",
    "                # 시간이 포함되어 있는지 확인\n",
    "                if len(period[0].split()) > 1:  # 시간이 포함된 경우\n",
    "                    start_datetime = pd.to_datetime(period[0])\n",
    "                    end_datetime = pd.to_datetime(period[1])\n",
    "\n",
    "                    # 정확한 시간까지 비교\n",
    "                    condition = (df['building_number'] == building_id) & \\\n",
    "                               (df['date_time'] >= start_datetime) & \\\n",
    "                               (df['date_time'] <= end_datetime)\n",
    "\n",
    "                    print(f\"  건물 {building_id}: {period[0]} ~ {period[1]} (시간 포함)\", end=\" | \")\n",
    "                else:  # 날짜만 있는 경우 (기존 방식)\n",
    "                    start_date = pd.to_datetime(period[0]).date()\n",
    "                    end_date = pd.to_datetime(period[1]).date()\n",
    "\n",
    "                    # 해당 건물과 기간에 맞는 조건\n",
    "                    condition = (df['building_number'] == building_id) & \\\n",
    "                               (df['date_time'].dt.date >= start_date) & \\\n",
    "                               (df['date_time'].dt.date <= end_date)\n",
    "\n",
    "                    print(f\"  건물 {building_id}: {period[0]} ~ {period[1]} (전체 날짜)\", end=\" | \")\n",
    "\n",
    "                marked_count = condition.sum()\n",
    "                df.loc[condition, 'outlier_detect'] = 1\n",
    "                building_marked += marked_count\n",
    "                print(f\"{marked_count}개 마킹\")\n",
    "\n",
    "            type_marked += building_marked\n",
    "\n",
    "        total_marked += type_marked\n",
    "        print(f\"  {building_type_name} 총 마킹: {type_marked}개\")\n",
    "\n",
    "    final_outliers = (df['outlier_detect'] == 1).sum()\n",
    "    newly_marked = final_outliers - initial_outliers\n",
    "\n",
    "    print(f\"\\\\n=== 특정 기간 이상치 마킹 완료 ===\")\n",
    "    print(f\"기존 이상치: {initial_outliers}개\")\n",
    "    print(f\"새로 마킹된 이상치: {newly_marked}개\")\n",
    "    print(f\"최종 이상치: {final_outliers}개\")\n",
    "    print(f\"전체 데이터 대비 이상치 비율: {final_outliers/len(df)*100:.2f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# date_time 컬럼을 먼저 datetime 타입으로 변환\n",
    "print(\"=== 날짜 데이터 타입 확인 및 변환 ===\")\n",
    "if not pd.api.types.is_datetime64_any_dtype(train['date_time']):\n",
    "    print(\"train 데이터의 date_time을 datetime 타입으로 변환 중...\")\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "if not pd.api.types.is_datetime64_any_dtype(test['date_time']):\n",
    "    print(\"test 데이터의 date_time을 datetime 타입으로 변환 중...\")\n",
    "    test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "# train 데이터에 특정 기간 이상치 마킹 적용\n",
    "train = mark_outlier_periods(train)\n",
    "\n",
    "# test 데이터에도 outlier_detect 컬럼이 있는지 확인하고 같은 기간 마킹\n",
    "if 'outlier_detect' in test.columns:\n",
    "    print(\"\\\\n=== Test 데이터에도 같은 기간 이상치 마킹 ===\")\n",
    "    test = mark_outlier_periods(test)\n",
    "else:\n",
    "    print(\"\\\\nTest 데이터에는 outlier_detect 컬럼이 없어 건너뜁니다.\")"
   ],
   "id": "1de6dde35097c4fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 날짜 데이터 타입 확인 및 변환 ===\n",
      "train 데이터의 date_time을 datetime 타입으로 변환 중...\n",
      "test 데이터의 date_time을 datetime 타입으로 변환 중...\n",
      "=== 특정 기간 이상치 마킹 시작 ===\n",
      "\\n--- 건물기타 (Other Buildings) 처리 ---\n",
      "  건물 7: 2024-07-07 10:00 ~ 2024-07-08 11:00 (시간 포함) | 26개 마킹\n",
      "  건물 7: 2024-07-12 14:00 ~ 2024-08-06 03:00 (시간 포함) | 590개 마킹\n",
      "  건물 26: 2024-06-17 14:00 ~ 2024-06-18 11:00 (시간 포함) | 22개 마킹\n",
      "  건물 69: 2024-06-02 ~ 2024-06-02 (전체 날짜) | 24개 마킹\n",
      "  건물 82: 2024-07-17 14:00 ~ 2024-07-17 14:00 (시간 포함) | 1개 마킹\n",
      "  건물기타 (Other Buildings) 총 마킹: 663개\n",
      "\\n--- IDC(전화국) 처리 ---\n",
      "  건물 30: 2024-07-13 20:00 ~ 2024-07-13 20:00 (시간 포함) | 1개 마킹\n",
      "  건물 30: 2024-07-25 00:00 ~ 2024-07-25 00:00 (시간 포함) | 1개 마킹\n",
      "  건물 36: 2024-07-21 00:00 ~ 2024-07-21 23:00 (시간 포함) | 24개 마킹\n",
      "  건물 43: 2024-06-10 17:00 ~ 2024-06-10 18:00 (시간 포함) | 2개 마킹\n",
      "  건물 43: 2024-08-12 16:00 ~ 2024-08-12 17:00 (시간 포함) | 2개 마킹\n",
      "  건물 52: 2024-08-10 00:00 ~ 2024-08-10 02:00 (시간 포함) | 3개 마킹\n",
      "  건물 57: 2024-06-01 ~ 2024-06-07 (전체 날짜) | 168개 마킹\n",
      "  건물 67: 2024-07-26 ~ 2024-07-27 (전체 날짜) | 48개 마킹\n",
      "  건물 67: 2024-08-01 15:00 ~ 2024-08-01 17:00 (시간 포함) | 3개 마킹\n",
      "  IDC(전화국) 총 마킹: 252개\n",
      "\\n--- 병원 (Hospital) 처리 ---\n",
      "  건물 17: 2024-06-25 20:00 ~ 2024-06-26 08:00 (시간 포함) | 13개 마킹\n",
      "  건물 44: 2024-06-06 12:00 ~ 2024-06-06 14:00 (시간 포함) | 3개 마킹\n",
      "  병원 (Hospital) 총 마킹: 16개\n",
      "\\n--- 상용 (Commercial) 처리 ---\n",
      "  건물 20: 2024-06-01 10:00 ~ 2024-06-01 11:00 (시간 포함) | 2개 마킹\n",
      "  건물 20: 2024-06-09 10:00 ~ 2024-06-09 10:00 (시간 포함) | 1개 마킹\n",
      "  건물 41: 2024-07-17 09:00 ~ 2024-07-17 15:00 (시간 포함) | 7개 마킹\n",
      "  건물 51: 2024-06-30 ~ 2024-06-30 (전체 날짜) | 24개 마킹\n",
      "  상용 (Commercial) 총 마킹: 34개\n",
      "\\n--- 아파트 (Apartment) 처리 ---\n",
      "  건물 25: 2024-07-04 12:00 ~ 2024-07-04 14:00 (시간 포함) | 3개 마킹\n",
      "  건물 65: 2024-06-01 ~ 2024-06-09 (전체 날짜) | 216개 마킹\n",
      "  건물 70: 2024-06-04 09:00 ~ 2024-06-05 08:00 (시간 포함) | 24개 마킹\n",
      "  아파트 (Apartment) 총 마킹: 243개\n",
      "\\n--- 연구소 (Research Institute) 처리 ---\n",
      "  건물 49: 2024-06-15 09:00 ~ 2024-06-15 11:00 (시간 포함) | 3개 마킹\n",
      "  건물 49: 2024-07-06 ~ 2024-07-07 (전체 날짜) | 48개 마킹\n",
      "  건물 49: 2024-08-17 ~ 2024-08-18 (전체 날짜) | 48개 마킹\n",
      "  건물 49: 2024-08-22 ~ 2024-08-22 (전체 날짜) | 24개 마킹\n",
      "  건물 53: 2024-06-14 16:00 ~ 2024-06-17 10:00 (시간 포함) | 67개 마킹\n",
      "  건물 53: 2024-08-18 15:00 ~ 2024-08-19 09:00 (시간 포함) | 19개 마킹\n",
      "  건물 83: 2024-07-17 14:00 ~ 2024-07-17 14:00 (시간 포함) | 1개 마킹\n",
      "  건물 94: 2024-07-26 18:00 ~ 2024-08-05 05:00 (시간 포함) | 228개 마킹\n",
      "  연구소 (Research Institute) 총 마킹: 438개\n",
      "\\n--- 학교 (University) 처리 ---\n",
      "  건물 8: 2024-07-21 08:00 ~ 2024-07-21 11:00 (시간 포함) | 4개 마킹\n",
      "  건물 8: 2024-08-24 09:00 ~ 2024-08-24 23:00 (시간 포함) | 15개 마킹\n",
      "  건물 12: 2024-07-21 08:00 ~ 2024-07-21 11:00 (시간 포함) | 4개 마킹\n",
      "  건물 12: 2024-08-24 08:00 ~ 2024-08-24 10:00 (시간 포함) | 3개 마킹\n",
      "  건물 55: 2024-07-17 14:00 ~ 2024-07-17 14:00 (시간 포함) | 1개 마킹\n",
      "  건물 87: 2024-06-01 ~ 2024-06-30 (전체 날짜) | 720개 마킹\n",
      "  학교 (University) 총 마킹: 747개\n",
      "\\n--- 호텔 (Hotel) 처리 ---\n",
      "  건물 89: 2024-07-12 00:00 ~ 2024-07-12 23:00 (시간 포함) | 24개 마킹\n",
      "  호텔 (Hotel) 총 마킹: 24개\n",
      "\\n--- 공공 (Public) 처리 ---\n",
      "  건물 38: 2024-07-17 14:00 ~ 2024-07-17 15:00 (시간 포함) | 2개 마킹\n",
      "  건물 72: 2024-06-11 00:00 ~ 2024-06-11 02:00 (시간 포함) | 3개 마킹\n",
      "  건물 92: 2024-07-17 14:00 ~ 2024-07-18 04:00 (시간 포함) | 15개 마킹\n",
      "  공공 (Public) 총 마킹: 20개\n",
      "\\n--- 백화점 (Department Store) 처리 ---\n",
      "  건물 19: 2024-07-31 13:00 ~ 2024-07-31 16:00 (시간 포함) | 4개 마킹\n",
      "  건물 32: 2024-07-08 09:00 ~ 2024-07-08 10:00 (시간 포함) | 2개 마킹\n",
      "  건물 40: 2024-07-14 00:00 ~ 2024-07-14 01:00 (시간 포함) | 2개 마킹\n",
      "  건물 45: 2024-07-04 00:00 ~ 2024-07-04 03:00 (시간 포함) | 4개 마킹\n",
      "  건물 73: 2024-07-08 22:00 ~ 2024-07-08 22:00 (시간 포함) | 1개 마킹\n",
      "  건물 79: 2024-08-19 03:00 ~ 2024-08-19 05:00 (시간 포함) | 3개 마킹\n",
      "  건물 95: 2024-08-05 10:00 ~ 2024-08-05 11:00 (시간 포함) | 2개 마킹\n",
      "  백화점 (Department Store) 총 마킹: 18개\n",
      "\\n=== 특정 기간 이상치 마킹 완료 ===\n",
      "기존 이상치: 407개\n",
      "새로 마킹된 이상치: 2338개\n",
      "최종 이상치: 2745개\n",
      "전체 데이터 대비 이상치 비율: 1.35%\n",
      "\\n=== Test 데이터에도 같은 기간 이상치 마킹 ===\n",
      "=== 특정 기간 이상치 마킹 시작 ===\n",
      "\\n--- 건물기타 (Other Buildings) 처리 ---\n",
      "  건물 7: 2024-07-07 10:00 ~ 2024-07-08 11:00 (시간 포함) | 0개 마킹\n",
      "  건물 7: 2024-07-12 14:00 ~ 2024-08-06 03:00 (시간 포함) | 0개 마킹\n",
      "  건물 26: 2024-06-17 14:00 ~ 2024-06-18 11:00 (시간 포함) | 0개 마킹\n",
      "  건물 69: 2024-06-02 ~ 2024-06-02 (전체 날짜) | 0개 마킹\n",
      "  건물 82: 2024-07-17 14:00 ~ 2024-07-17 14:00 (시간 포함) | 0개 마킹\n",
      "  건물기타 (Other Buildings) 총 마킹: 0개\n",
      "\\n--- IDC(전화국) 처리 ---\n",
      "  건물 30: 2024-07-13 20:00 ~ 2024-07-13 20:00 (시간 포함) | 0개 마킹\n",
      "  건물 30: 2024-07-25 00:00 ~ 2024-07-25 00:00 (시간 포함) | 0개 마킹\n",
      "  건물 36: 2024-07-21 00:00 ~ 2024-07-21 23:00 (시간 포함) | 0개 마킹\n",
      "  건물 43: 2024-06-10 17:00 ~ 2024-06-10 18:00 (시간 포함) | 0개 마킹\n",
      "  건물 43: 2024-08-12 16:00 ~ 2024-08-12 17:00 (시간 포함) | 0개 마킹\n",
      "  건물 52: 2024-08-10 00:00 ~ 2024-08-10 02:00 (시간 포함) | 0개 마킹\n",
      "  건물 57: 2024-06-01 ~ 2024-06-07 (전체 날짜) | 0개 마킹\n",
      "  건물 67: 2024-07-26 ~ 2024-07-27 (전체 날짜) | 0개 마킹\n",
      "  건물 67: 2024-08-01 15:00 ~ 2024-08-01 17:00 (시간 포함) | 0개 마킹\n",
      "  IDC(전화국) 총 마킹: 0개\n",
      "\\n--- 병원 (Hospital) 처리 ---\n",
      "  건물 17: 2024-06-25 20:00 ~ 2024-06-26 08:00 (시간 포함) | 0개 마킹\n",
      "  건물 44: 2024-06-06 12:00 ~ 2024-06-06 14:00 (시간 포함) | 0개 마킹\n",
      "  병원 (Hospital) 총 마킹: 0개\n",
      "\\n--- 상용 (Commercial) 처리 ---\n",
      "  건물 20: 2024-06-01 10:00 ~ 2024-06-01 11:00 (시간 포함) | 0개 마킹\n",
      "  건물 20: 2024-06-09 10:00 ~ 2024-06-09 10:00 (시간 포함) | 0개 마킹\n",
      "  건물 41: 2024-07-17 09:00 ~ 2024-07-17 15:00 (시간 포함) | 0개 마킹\n",
      "  건물 51: 2024-06-30 ~ 2024-06-30 (전체 날짜) | 0개 마킹\n",
      "  상용 (Commercial) 총 마킹: 0개\n",
      "\\n--- 아파트 (Apartment) 처리 ---\n",
      "  건물 25: 2024-07-04 12:00 ~ 2024-07-04 14:00 (시간 포함) | 0개 마킹\n",
      "  건물 65: 2024-06-01 ~ 2024-06-09 (전체 날짜) | 0개 마킹\n",
      "  건물 70: 2024-06-04 09:00 ~ 2024-06-05 08:00 (시간 포함) | 0개 마킹\n",
      "  아파트 (Apartment) 총 마킹: 0개\n",
      "\\n--- 연구소 (Research Institute) 처리 ---\n",
      "  건물 49: 2024-06-15 09:00 ~ 2024-06-15 11:00 (시간 포함) | 0개 마킹\n",
      "  건물 49: 2024-07-06 ~ 2024-07-07 (전체 날짜) | 0개 마킹\n",
      "  건물 49: 2024-08-17 ~ 2024-08-18 (전체 날짜) | 0개 마킹\n",
      "  건물 49: 2024-08-22 ~ 2024-08-22 (전체 날짜) | 0개 마킹\n",
      "  건물 53: 2024-06-14 16:00 ~ 2024-06-17 10:00 (시간 포함) | 0개 마킹\n",
      "  건물 53: 2024-08-18 15:00 ~ 2024-08-19 09:00 (시간 포함) | 0개 마킹\n",
      "  건물 83: 2024-07-17 14:00 ~ 2024-07-17 14:00 (시간 포함) | 0개 마킹\n",
      "  건물 94: 2024-07-26 18:00 ~ 2024-08-05 05:00 (시간 포함) | 0개 마킹\n",
      "  연구소 (Research Institute) 총 마킹: 0개\n",
      "\\n--- 학교 (University) 처리 ---\n",
      "  건물 8: 2024-07-21 08:00 ~ 2024-07-21 11:00 (시간 포함) | 0개 마킹\n",
      "  건물 8: 2024-08-24 09:00 ~ 2024-08-24 23:00 (시간 포함) | 0개 마킹\n",
      "  건물 12: 2024-07-21 08:00 ~ 2024-07-21 11:00 (시간 포함) | 0개 마킹\n",
      "  건물 12: 2024-08-24 08:00 ~ 2024-08-24 10:00 (시간 포함) | 0개 마킹\n",
      "  건물 55: 2024-07-17 14:00 ~ 2024-07-17 14:00 (시간 포함) | 0개 마킹\n",
      "  건물 87: 2024-06-01 ~ 2024-06-30 (전체 날짜) | 0개 마킹\n",
      "  학교 (University) 총 마킹: 0개\n",
      "\\n--- 호텔 (Hotel) 처리 ---\n",
      "  건물 89: 2024-07-12 00:00 ~ 2024-07-12 23:00 (시간 포함) | 0개 마킹\n",
      "  호텔 (Hotel) 총 마킹: 0개\n",
      "\\n--- 공공 (Public) 처리 ---\n",
      "  건물 38: 2024-07-17 14:00 ~ 2024-07-17 15:00 (시간 포함) | 0개 마킹\n",
      "  건물 72: 2024-06-11 00:00 ~ 2024-06-11 02:00 (시간 포함) | 0개 마킹\n",
      "  건물 92: 2024-07-17 14:00 ~ 2024-07-18 04:00 (시간 포함) | 0개 마킹\n",
      "  공공 (Public) 총 마킹: 0개\n",
      "\\n--- 백화점 (Department Store) 처리 ---\n",
      "  건물 19: 2024-07-31 13:00 ~ 2024-07-31 16:00 (시간 포함) | 0개 마킹\n",
      "  건물 32: 2024-07-08 09:00 ~ 2024-07-08 10:00 (시간 포함) | 0개 마킹\n",
      "  건물 40: 2024-07-14 00:00 ~ 2024-07-14 01:00 (시간 포함) | 0개 마킹\n",
      "  건물 45: 2024-07-04 00:00 ~ 2024-07-04 03:00 (시간 포함) | 0개 마킹\n",
      "  건물 73: 2024-07-08 22:00 ~ 2024-07-08 22:00 (시간 포함) | 0개 마킹\n",
      "  건물 79: 2024-08-19 03:00 ~ 2024-08-19 05:00 (시간 포함) | 0개 마킹\n",
      "  건물 95: 2024-08-05 10:00 ~ 2024-08-05 11:00 (시간 포함) | 0개 마킹\n",
      "  백화점 (Department Store) 총 마킹: 0개\n",
      "\\n=== 특정 기간 이상치 마킹 완료 ===\n",
      "기존 이상치: 0개\n",
      "새로 마킹된 이상치: 0개\n",
      "최종 이상치: 0개\n",
      "전체 데이터 대비 이상치 비율: 0.00%\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "feature engineering",
   "id": "836249f510ad8950"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:37.205464Z",
     "start_time": "2025-08-19T16:33:36.791277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# Feature Engineering - Type 1 & Type 2 Features\n",
    "# Based on FEATURE.md specifications\n",
    "# ==========================================================\n",
    "\n",
    "print(\"=== Feature Engineering 시작 ===\")\n",
    "\n",
    "# 날짜 변환 확인\n",
    "if not pd.api.types.is_datetime64_any_dtype(train['date_time']):\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
    "if not pd.api.types.is_datetime64_any_dtype(test['date_time']):\n",
    "    test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "# ==========================================================\n",
    "# 기본 시간 피처 생성\n",
    "# ==========================================================\n",
    "print(\"1. 기본 시간 피처 생성...\")\n",
    "\n",
    "# 기본 시간 피처\n",
    "for df in [train, test]:\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day_of_week'] = df['date_time'].dt.dayofweek  # 월요일=0, 일요일=6\n",
    "    df['week'] = df['date_time'].dt.isocalendar().week  # 주차\n",
    "\n",
    "# ==========================================================\n",
    "# 건물별 휴일 피처 (is_holiday) - HOLIDAY.md 기반\n",
    "# ==========================================================\n",
    "print(\"2. 건물별 휴일 피처 생성...\")\n",
    "\n",
    "def apply_building_specific_holidays(df):\n",
    "    \"\"\"HOLIDAY.md에 따른 건물별 휴일 설정\"\"\"\n",
    "    \n",
    "    # 초기화\n",
    "    df['is_holiday'] = 0\n",
    "    \n",
    "    # 휴일 설정 정의 (HOLIDAY.md 기반)\n",
    "    holiday_config = {\n",
    "        # Commercial\n",
    "        'Commercial': {\n",
    "            'buildings': [2, 6, 16, 20, 51, 86],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        'Commercial_56': {\n",
    "            'buildings': [56],\n",
    "            'holidays': ['8/15']\n",
    "        },\n",
    "        \n",
    "        # Department Store\n",
    "        'Department_Store_bimonthly_sunday': {\n",
    "            'buildings': [27, 40, 59, 63],\n",
    "            'holidays': ['bimonthly_sunday']  # 2번째, 4번째 일요일\n",
    "        },\n",
    "        'Department_Store_bimonthly_monday': {\n",
    "            'buildings': [32],\n",
    "            'holidays': ['bimonthly_monday']  # 2번째, 4번째 월요일\n",
    "        },\n",
    "        'Department_Store_45': {\n",
    "            'buildings': [45],\n",
    "            'holidays': ['6/10', '7/8', '8/19']\n",
    "        },\n",
    "        'Department_Store_54': {\n",
    "            'buildings': [54],\n",
    "            'holidays': ['6/17', '7/8', '8/19']\n",
    "        },\n",
    "        'Department_Store_74': {\n",
    "            'buildings': [74],\n",
    "            'holidays': ['6/17', '7/1']\n",
    "        },\n",
    "        'Department_Store_79': {\n",
    "            'buildings': [79],\n",
    "            'holidays': ['6/17', '7/1', '8/19']\n",
    "        },\n",
    "        'Department_Store_95': {\n",
    "            'buildings': [95],\n",
    "            'holidays': ['7/8', '8/5']\n",
    "        },\n",
    "        \n",
    "        # Hospital\n",
    "        'Hospital': {\n",
    "            'buildings': 'all_hospital',  # 모든 병원\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        \n",
    "        # IDC\n",
    "        'IDC_43_52': {\n",
    "            'buildings': [43, 52],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        'IDC_64_67': {\n",
    "            'buildings': [64, 67],\n",
    "            'holidays': ['8/15']\n",
    "        },\n",
    "        \n",
    "        # Other Buildings\n",
    "        'Other_Buildings': {\n",
    "            'buildings': [47, 67],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        \n",
    "        # Public\n",
    "        'Public': {\n",
    "            'buildings': [38, 50, 66, 68, 72, 80],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        \n",
    "        # Research Institute\n",
    "        'Research_Institute_basic': {\n",
    "            'buildings': [13, 15, 37, 49, 53, 62, 83],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        'Research_Institute_extended': {\n",
    "            'buildings': [23, 94],\n",
    "            'holidays': ['6/6', '6/7', '8/15', '8/16']\n",
    "        },\n",
    "        \n",
    "        # University\n",
    "        'University': {\n",
    "            'buildings': [5, 8, 12, 14, 22, 24, 46, 55, 60, 87],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 각 설정 적용\n",
    "    for config_name, config in holiday_config.items():\n",
    "        buildings = config['buildings']\n",
    "        holidays = config['holidays']\n",
    "        \n",
    "        # 건물 선택\n",
    "        if buildings == 'all_hospital':\n",
    "            # 모든 병원 건물 선택 (building_type == 'Hospital')\n",
    "            building_mask = df['building_type'] == 'Hospital'\n",
    "        else:\n",
    "            # 특정 건물 번호들 선택\n",
    "            building_mask = df['building_number'].isin(buildings)\n",
    "        \n",
    "        # 각 휴일 적용\n",
    "        for holiday in holidays:\n",
    "            if holiday == 'bimonthly_sunday':\n",
    "                # 2번째, 4번째 일요일\n",
    "                is_sunday = df['date_time'].dt.dayofweek == 6\n",
    "                week_of_month = ((df['date_time'].dt.day - 1) // 7)\n",
    "                is_2nd_or_4th_week = (week_of_month == 1) | (week_of_month == 3)\n",
    "                holiday_condition = building_mask & is_sunday & is_2nd_or_4th_week\n",
    "                \n",
    "            elif holiday == 'bimonthly_monday':\n",
    "                # 2번째, 4번째 월요일\n",
    "                is_monday = df['date_time'].dt.dayofweek == 0\n",
    "                week_of_month = ((df['date_time'].dt.day - 1) // 7)\n",
    "                is_2nd_or_4th_week = (week_of_month == 1) | (week_of_month == 3)\n",
    "                holiday_condition = building_mask & is_monday & is_2nd_or_4th_week\n",
    "                \n",
    "            else:\n",
    "                # 특정 날짜 (MM/DD 형식)\n",
    "                month, day = map(int, holiday.split('/'))\n",
    "                date_condition = (df['date_time'].dt.month == month) & (df['date_time'].dt.day == day)\n",
    "                holiday_condition = building_mask & date_condition\n",
    "            \n",
    "            # 휴일 마킹\n",
    "            df.loc[holiday_condition, 'is_holiday'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# train과 test에 휴일 설정 적용\n",
    "train = apply_building_specific_holidays(train)\n",
    "test = apply_building_specific_holidays(test)\n",
    "\n",
    "print(f\"Train 휴일 데이터: {(train['is_holiday'] == 1).sum()}개\")\n",
    "print(f\"Test 휴일 데이터: {(test['is_holiday'] == 1).sum()}개\")\n",
    "\n",
    "# ==========================================================\n",
    "# 주기적 시간 피처 (sin_hour, cos_hour)\n",
    "# ==========================================================\n",
    "print(\"3. 주기적 시간 피처 생성...\")\n",
    "\n",
    "for df in [train, test]:\n",
    "    # 24시간 주기\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "\n",
    "# ==========================================================\n",
    "# 일별 온도 통계 피처\n",
    "# ==========================================================\n",
    "print(\"4. 일별 온도 통계 피처 생성...\")\n",
    "\n",
    "def calculate_daily_temperature_stats(dataframe):\n",
    "    \"\"\"일별 온도 통계 계산\"\"\"\n",
    "    # 건물별, 월별, 일별로 온도 통계 계산\n",
    "    daily_temp_stats = dataframe.groupby(['building_number', 'month', 'day'])['temperature'].agg([\n",
    "        ('day_max_temperature', 'max'),\n",
    "        ('day_min_temperature', 'min'),\n",
    "        ('day_mean_temperature', 'mean')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # 원본 데이터와 merge\n",
    "    merged = dataframe.merge(daily_temp_stats, on=['building_number', 'month', 'day'], how='left')\n",
    "    \n",
    "    # 일교차 계산\n",
    "    merged['day_diff_temperature'] = merged['day_max_temperature'] - merged['day_min_temperature']\n",
    "    \n",
    "    return merged\n",
    "\n",
    "train = calculate_daily_temperature_stats(train)\n",
    "test = calculate_daily_temperature_stats(test)\n",
    "\n",
    "# ==========================================================\n",
    "# 여름 계절 피처 (summer_sin, summer_cos)\n",
    "# ==========================================================\n",
    "print(\"5. 여름 계절 피처 생성...\")\n",
    "\n",
    "for df in [train, test]:\n",
    "    # 6월~8월을 여름으로 간주, 월별로 주기적 변화 표현\n",
    "    # 6월=0, 7월=1, 8월=2 -> 정규화하여 [0, 2π] 범위로 변환\n",
    "    summer_months = {6: 0, 7: 1, 8: 2}\n",
    "    df['summer_phase'] = df['month'].map(summer_months)\n",
    "    df['summer_sin'] = np.sin(2 * np.pi * df['summer_phase'] / 3.0)\n",
    "    df['summer_cos'] = np.cos(2 * np.pi * df['summer_phase'] / 3.0)\n",
    "    df = df.drop('summer_phase', axis=1)\n",
    "\n",
    "# ==========================================================\n",
    "# 날씨 지수 피처 (THI, WCT, CDH)\n",
    "# ==========================================================\n",
    "print(\"6. 날씨 지수 피처 생성...\")\n",
    "\n",
    "# THI (Temperature-Humidity Index) 계산\n",
    "for df in [train, test]:\n",
    "    df['THI'] = 9/5 * df['temperature'] - 0.55 * (1 - df['humidity']/100) * (9/5 * df['temperature'] - 26) + 32\n",
    "\n",
    "# WCT (Wind Chill Temperature) 계산\n",
    "for df in [train, test]:\n",
    "    df['WCT'] = 13.12 + 0.6125 * df['temperature'] - 11.37 * (df['windspeed'] ** 0.16) + \\\n",
    "                0.3965 * (df['windspeed'] ** 0.16) * df['temperature']\n",
    "\n",
    "# CDH (Cooling Degrees Hours) 계산\n",
    "def calculate_cdh(temperatures):\n",
    "    \"\"\"CDH 계산 - 26도를 기준으로 누적 냉방도 계산\"\"\"\n",
    "    # 수정: 26도 이하일 때는 0으로 처리\n",
    "    temp_diff = np.maximum(temperatures - 26, 0)\n",
    "    cumsum = np.cumsum(temp_diff)\n",
    "    # 11시간 이후부터는 sliding window 적용\n",
    "    return np.concatenate((cumsum[:11], cumsum[11:] - cumsum[:-11]))\n",
    "\n",
    "def calculate_and_add_cdh(dataframe):\n",
    "    \"\"\"건물별로 CDH 계산 후 추가\"\"\"\n",
    "    cdhs = []\n",
    "    for building_id in range(1, 101):\n",
    "        building_data = dataframe[dataframe['building_number'] == building_id]['temperature'].values\n",
    "        if len(building_data) > 0:\n",
    "            cdh = calculate_cdh(building_data)\n",
    "            cdhs.extend(cdh)\n",
    "        # 수정: else 블록 제거 (애초에 building_data가 비어있으면 위 조건문에 안 들어감)\n",
    "    return cdhs\n",
    "\n",
    "train['CDH'] = calculate_and_add_cdh(train)\n",
    "test['CDH'] = calculate_and_add_cdh(test)\n",
    "\n",
    "# ==========================================================\n",
    "# 성수기 피처 (is_peak_season)\n",
    "# ==========================================================\n",
    "print(\"7. 성수기 피처 생성...\")\n",
    "\n",
    "# 호텔 성수기 설정 (reference 기반)\n",
    "peak_season_config = {\n",
    "    9: [{'start': '2024-07-13', 'end': '2024-08-31'}],\n",
    "    10: [{'start': '2024-07-04', 'end': '2024-08-22'}],\n",
    "    28: [{'start': '2024-07-18', 'end': '2024-08-31'}],\n",
    "    77: [{'start': '2024-07-18', 'end': '2024-08-31'}],\n",
    "    89: [{'start': '2024-07-17', 'end': '2024-08-31'}],\n",
    "    98: [{'start': '2024-07-15', 'end': '2024-08-31'}],\n",
    "    100: [{'start': '2024-07-15', 'end': '2024-08-31'}],\n",
    "}\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['is_peak_season'] = 0\n",
    "    for building_id, seasons in peak_season_config.items():\n",
    "        for season in seasons:\n",
    "            start_date = pd.to_datetime(season['start'])\n",
    "            end_date = pd.to_datetime(season['end'])\n",
    "            condition = (\n",
    "                (df['building_number'] == building_id) &\n",
    "                (df['date_time'] >= start_date) &\n",
    "                (df['date_time'] <= end_date)\n",
    "            )\n",
    "            df.loc[condition, 'is_peak_season'] = 1\n",
    "\n",
    "# ==========================================================\n",
    "# 통계 피처 계산용 클린 데이터 생성\n",
    "# ==========================================================\n",
    "print(\"8. 통계 피처 계산용 데이터 준비...\")\n",
    "\n",
    "# 이상치가 제거된 데이터로 통계 피처 계산\n",
    "train_for_stats = train[train['outlier_detect'] == 0].copy()\n",
    "\n",
    "# ==========================================================\n",
    "# 통계 피처 생성 (day_hour_mean, day_hour_std, holiday_hour_mean, holiday_std)\n",
    "# ==========================================================\n",
    "print(\"9. 통계 피처 생성...\")\n",
    "\n",
    "# 요일별 시간 평균/표준편차 (power_consumption 기준)\n",
    "day_hour_stats = train_for_stats.groupby(['building_number', 'hour', 'day_of_week'])['power_consumption'].agg([\n",
    "    ('day_hour_mean', 'mean'),\n",
    "    ('day_hour_std', 'std')\n",
    "]).reset_index()\n",
    "\n",
    "# NaN 처리\n",
    "day_hour_stats['day_hour_std'] = day_hour_stats['day_hour_std'].fillna(0)\n",
    "\n",
    "# 휴일 시간 평균/표준편차 - 안전한 처리\n",
    "holiday_data = train_for_stats[train_for_stats['is_holiday'] == 1]\n",
    "print(f\"휴일 데이터 개수: {len(holiday_data)}\")\n",
    "\n",
    "if len(holiday_data) > 0:\n",
    "    holiday_hour_stats = holiday_data.groupby(['building_number', 'hour'])['power_consumption'].agg([\n",
    "        ('holiday_hour_mean', 'mean'),\n",
    "        ('holiday_std', 'std')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # NaN 처리\n",
    "    holiday_hour_stats['holiday_std'] = holiday_hour_stats['holiday_std'].fillna(0)\n",
    "    print(f\"휴일 통계 생성: {len(holiday_hour_stats)}개\")\n",
    "else:\n",
    "    # 휴일 데이터가 없으면 빈 데이터프레임 생성\n",
    "    holiday_hour_stats = pd.DataFrame(columns=['building_number', 'hour', 'holiday_hour_mean', 'holiday_std'])\n",
    "    print(\"휴일 데이터가 없어서 빈 통계 생성\")\n",
    "\n",
    "# train과 test에 통계 피처 병합\n",
    "print(\"10. 통계 피처 병합...\")\n",
    "\n",
    "# 요일별 시간 통계 병합\n",
    "train = train.merge(day_hour_stats, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "test = test.merge(day_hour_stats, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "\n",
    "# 휴일 통계 초기화\n",
    "train['holiday_hour_mean'] = 0.0\n",
    "train['holiday_std'] = 0.0\n",
    "test['holiday_hour_mean'] = 0.0\n",
    "test['holiday_std'] = 0.0\n",
    "\n",
    "# 휴일 통계가 있으면 병합\n",
    "if len(holiday_hour_stats) > 0:\n",
    "    for df in [train, test]:\n",
    "        holiday_mask = df['is_holiday'] == 1\n",
    "        if holiday_mask.sum() > 0:\n",
    "            holiday_df = df[holiday_mask][['building_number', 'hour']].merge(\n",
    "                holiday_hour_stats, on=['building_number', 'hour'], how='left'\n",
    "            )\n",
    "            \n",
    "            # 성공적으로 병합된 경우만 업데이트\n",
    "            if len(holiday_df) > 0:\n",
    "                df.loc[holiday_mask, 'holiday_hour_mean'] = holiday_df['holiday_hour_mean'].fillna(0).values\n",
    "                df.loc[holiday_mask, 'holiday_std'] = holiday_df['holiday_std'].fillna(0).values\n",
    "\n",
    "# ==========================================================\n",
    "# Type 2 전용 피처 (building features)\n",
    "# ==========================================================\n",
    "print(\"11. Type 2 전용 건물 피처 확인...\")\n",
    "\n",
    "# total_area, cooling_area는 이미 존재 (building_info에서 병합됨)\n",
    "# building_number, building_type도 이미 존재\n",
    "print(\"건물 관련 피처들이 이미 존재합니다.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Type 2 detail_2 전용 피처\n",
    "# ==========================================================\n",
    "print(\"12. Type 2 detail_2 전용 피처 생성...\")\n",
    "\n",
    "# pv_temp (태양광 발전 온도 상호작용)\n",
    "for df in [train, test]:\n",
    "    df['pv_temp'] = df['solar_power_capacity'] * df['temperature']\n",
    "\n",
    "# ess_pcs_std (ESS 관련 표준편차)\n",
    "# 건물별 ESS 용량의 표준편차를 시간대별로 계산\n",
    "if len(train_for_stats) > 0:\n",
    "    ess_stats = train_for_stats.groupby(['building_number', 'hour'])['ess_capacity'].agg([\n",
    "        ('ess_pcs_std', 'std')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # 표준편차가 NaN인 경우 0으로 채움 (단일 값인 경우)\n",
    "    ess_stats['ess_pcs_std'] = ess_stats['ess_pcs_std'].fillna(0)\n",
    "    \n",
    "    # train과 test에 병합\n",
    "    train = train.merge(ess_stats, on=['building_number', 'hour'], how='left')\n",
    "    test = test.merge(ess_stats, on=['building_number', 'hour'], how='left')\n",
    "    \n",
    "    # 병합되지 않은 값들은 0으로 채움\n",
    "    train['ess_pcs_std'] = train['ess_pcs_std'].fillna(0)\n",
    "    test['ess_pcs_std'] = test['ess_pcs_std'].fillna(0)\n",
    "else:\n",
    "    # 통계 데이터가 없으면 0으로 초기화\n",
    "    train['ess_pcs_std'] = 0.0\n",
    "    test['ess_pcs_std'] = 0.0\n",
    "\n",
    "# ==========================================================\n",
    "# 피처 정리 및 확인\n",
    "# ==========================================================\n",
    "print(\"13. 피처 생성 완료 및 확인...\")\n",
    "\n",
    "# NaN 값 처리\n",
    "for df in [train, test]:\n",
    "    # 통계 피처의 NaN은 0으로 채움 (통계 정보가 없는 경우)\n",
    "    stats_columns = ['day_hour_mean', 'day_hour_std', 'holiday_hour_mean', 'holiday_std']\n",
    "    for col in stats_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "# Type 1 base_1 피처 목록 확인\n",
    "type1_base1_features = [\n",
    "    'temperature', 'humidity', 'windspeed', 'day_of_week', 'month', 'week',\n",
    "    'is_holiday', 'sin_hour', 'cos_hour', 'day_hour_mean', 'day_hour_std',\n",
    "    'holiday_hour_mean', 'holiday_std', 'THI', 'WCT', 'CDH', 'is_peak_season'\n",
    "]\n",
    "\n",
    "# Type 1 detail_1 추가 피처\n",
    "type1_detail1_features = [\n",
    "    'summer_sin', 'summer_cos', 'day_max_temperature', 'day_min_temperature',\n",
    "    'day_mean_temperature', 'day_diff_temperature'\n",
    "]\n",
    "\n",
    "# Type 2 base_2 피처 (Type 1 base_1 + building features)\n",
    "type2_base2_features = type1_base1_features + [\n",
    "    'total_area', 'cooling_area', 'building_number', 'building_type'\n",
    "]\n",
    "\n",
    "# Type 2 detail_2 추가 피처\n",
    "type2_detail2_features = [\n",
    "    'summer_sin', 'summer_cos', 'day_max_temperature', 'day_min_temperature',\n",
    "    'day_mean_temperature', 'day_diff_temperature', 'pv_temp', 'ess_pcs_std'\n",
    "]\n",
    "\n",
    "print(\"=== 피처 생성 완료 ===\")\n",
    "print(f\"Train 데이터 shape: {train.shape}\")\n",
    "print(f\"Test 데이터 shape: {test.shape}\")\n",
    "print(f\"Type 1 base_1 피처 수: {len(type1_base1_features)}\")\n",
    "print(f\"Type 1 detail_1 추가 피처 수: {len(type1_detail1_features)}\")\n",
    "print(f\"Type 2 base_2 피처 수: {len(type2_base2_features)}\")\n",
    "print(f\"Type 2 detail_2 추가 피처 수: {len(type2_detail2_features)}\")\n",
    "\n",
    "# 피처별 존재 여부 확인\n",
    "print(\"\\n=== 피처 존재 여부 확인 ===\")\n",
    "all_required_features = list(set(type1_base1_features + type1_detail1_features + type2_base2_features + type2_detail2_features))\n",
    "missing_features = []\n",
    "\n",
    "for feature in all_required_features:\n",
    "    if feature in train.columns:\n",
    "        print(f\"✓ {feature}\")\n",
    "    else:\n",
    "        print(f\"✗ {feature} - MISSING!\")\n",
    "        missing_features.append(feature)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n누락된 피처: {missing_features}\")\n",
    "else:\n",
    "    print(\"\\n모든 필수 피처가 생성되었습니다!\")\n",
    "\n",
    "# 건물별 휴일 데이터 확인\n",
    "print(f\"\\n=== 건물별 휴일 데이터 확인 ===\")\n",
    "holiday_by_building_type = train[train['is_holiday'] == 1].groupby('building_type').size()\n",
    "for building_type, count in holiday_by_building_type.items():\n",
    "    print(f\"{building_type}: {count}개\")\n",
    "\n",
    "holiday_count_train = (train['is_holiday'] == 1).sum()\n",
    "holiday_count_test = (test['is_holiday'] == 1).sum()\n",
    "print(f\"\\nTrain 총 휴일 데이터: {holiday_count_train}개\")\n",
    "print(f\"Test 총 휴일 데이터: {holiday_count_test}개\")"
   ],
   "id": "d9eea5dcfac5f882",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Feature Engineering 시작 ===\n",
      "1. 기본 시간 피처 생성...\n",
      "2. 건물별 휴일 피처 생성...\n",
      "Train 휴일 데이터: 3168개\n",
      "Test 휴일 데이터: 120개\n",
      "3. 주기적 시간 피처 생성...\n",
      "4. 일별 온도 통계 피처 생성...\n",
      "5. 여름 계절 피처 생성...\n",
      "6. 날씨 지수 피처 생성...\n",
      "7. 성수기 피처 생성...\n",
      "8. 통계 피처 계산용 데이터 준비...\n",
      "9. 통계 피처 생성...\n",
      "휴일 데이터 개수: 3129\n",
      "휴일 통계 생성: 1344개\n",
      "10. 통계 피처 병합...\n",
      "11. Type 2 전용 건물 피처 확인...\n",
      "건물 관련 피처들이 이미 존재합니다.\n",
      "12. Type 2 detail_2 전용 피처 생성...\n",
      "13. 피처 생성 완료 및 확인...\n",
      "=== 피처 생성 완료 ===\n",
      "Train 데이터 shape: (204000, 44)\n",
      "Test 데이터 shape: (16800, 41)\n",
      "Type 1 base_1 피처 수: 17\n",
      "Type 1 detail_1 추가 피처 수: 6\n",
      "Type 2 base_2 피처 수: 21\n",
      "Type 2 detail_2 추가 피처 수: 8\n",
      "\n",
      "=== 피처 존재 여부 확인 ===\n",
      "✓ cos_hour\n",
      "✓ windspeed\n",
      "✓ THI\n",
      "✓ humidity\n",
      "✓ total_area\n",
      "✓ month\n",
      "✓ building_type\n",
      "✓ day_max_temperature\n",
      "✓ building_number\n",
      "✓ day_min_temperature\n",
      "✓ day_hour_mean\n",
      "✓ is_peak_season\n",
      "✓ summer_cos\n",
      "✓ sin_hour\n",
      "✓ holiday_hour_mean\n",
      "✓ is_holiday\n",
      "✓ day_diff_temperature\n",
      "✓ week\n",
      "✓ cooling_area\n",
      "✓ day_hour_std\n",
      "✓ CDH\n",
      "✓ day_mean_temperature\n",
      "✓ holiday_std\n",
      "✓ pv_temp\n",
      "✓ ess_pcs_std\n",
      "✓ day_of_week\n",
      "✓ WCT\n",
      "✓ summer_sin\n",
      "✓ temperature\n",
      "\n",
      "모든 필수 피처가 생성되었습니다!\n",
      "\n",
      "=== 건물별 휴일 데이터 확인 ===\n",
      "Commercial: 312개\n",
      "Department Store: 912개\n",
      "Hospital: 432개\n",
      "IDC: 168개\n",
      "Other Buildings: 48개\n",
      "Public: 288개\n",
      "Research Institute: 528개\n",
      "University: 480개\n",
      "\n",
      "Train 총 휴일 데이터: 3168개\n",
      "Test 총 휴일 데이터: 120개\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:37.246049Z",
     "start_time": "2025-08-19T16:33:37.209674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"=== 데이터 타입 수정 ===\")\n",
    "\n",
    "# XGBoost 호환 데이터 타입으로 변환\n",
    "def fix_dtypes_for_xgb(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 숫자형 컬럼들을 float32로 변환\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'int32', 'uint32', 'uint64']).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "        print(f\"  {col}: {df[col].dtype} -> float32\")\n",
    "\n",
    "    # 범주형 컬럼 처리\n",
    "    if 'building_type' in df.columns:\n",
    "        df['building_type'] = df['building_type'].astype('category')\n",
    "\n",
    "    return df\n",
    "# train과 test 데이터 타입 수정\n",
    "train = fix_dtypes_for_xgb(train)\n",
    "test = fix_dtypes_for_xgb(test)\n",
    "\n",
    "print(f\"Train dtypes 확인:\")\n",
    "print(train.select_dtypes(include=['object', 'int64']).dtypes)\n",
    "print(f\"\\nTest dtypes 확인:\")\n",
    "print(test.select_dtypes(include=['object', 'int64']).dtypes)"
   ],
   "id": "ef1c4235d541ef21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터 타입 수정 ===\n",
      "  building_number: float32 -> float32\n",
      "  solar_power_utility_binary: float32 -> float32\n",
      "  ess_utility_binary: float32 -> float32\n",
      "  outlier_detect: float32 -> float32\n",
      "  hour: float32 -> float32\n",
      "  day: float32 -> float32\n",
      "  month: float32 -> float32\n",
      "  day_of_week: float32 -> float32\n",
      "  week: float32 -> float32\n",
      "  is_holiday: float32 -> float32\n",
      "  summer_phase: float32 -> float32\n",
      "  is_peak_season: float32 -> float32\n",
      "  building_number: float32 -> float32\n",
      "  solar_power_utility_binary: float32 -> float32\n",
      "  ess_utility_binary: float32 -> float32\n",
      "  outlier_detect: float32 -> float32\n",
      "  hour: float32 -> float32\n",
      "  day: float32 -> float32\n",
      "  month: float32 -> float32\n",
      "  day_of_week: float32 -> float32\n",
      "  week: float32 -> float32\n",
      "  is_holiday: float32 -> float32\n",
      "  summer_phase: float32 -> float32\n",
      "  is_peak_season: float32 -> float32\n",
      "Train dtypes 확인:\n",
      "num_date_time    object\n",
      "pcs_capacity     object\n",
      "dtype: object\n",
      "\n",
      "Test dtypes 확인:\n",
      "num_date_time    object\n",
      "pcs_capacity     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:37.251953Z",
     "start_time": "2025-08-19T16:33:37.249920Z"
    }
   },
   "cell_type": "code",
   "source": "train.columns",
   "id": "b59b1d788fa1244b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_date_time', 'building_number', 'date_time', 'temperature',\n",
       "       'rainfall', 'windspeed', 'humidity', 'sunshine', 'solar_radiation',\n",
       "       'power_consumption', 'building_type', 'total_area', 'cooling_area',\n",
       "       'solar_power_capacity', 'ess_capacity', 'pcs_capacity',\n",
       "       'solar_power_utility_binary', 'ess_utility_binary', 'outlier_detect',\n",
       "       'hour', 'day', 'month', 'day_of_week', 'week', 'is_holiday', 'sin_hour',\n",
       "       'cos_hour', 'day_max_temperature', 'day_min_temperature',\n",
       "       'day_mean_temperature', 'day_diff_temperature', 'summer_phase',\n",
       "       'summer_sin', 'summer_cos', 'THI', 'WCT', 'CDH', 'is_peak_season',\n",
       "       'day_hour_mean', 'day_hour_std', 'holiday_hour_mean', 'holiday_std',\n",
       "       'pv_temp', 'ess_pcs_std'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:37.265731Z",
     "start_time": "2025-08-19T16:33:37.264008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xgboost\n",
    "\n",
    "print (f\"xgboost version: {xgboost.__version__}\")"
   ],
   "id": "6250929deb1fcb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost version: 3.0.4\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T16:33:52.110732Z",
     "start_time": "2025-08-19T16:33:37.277441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Energy Load Forecasting — A→D Pipeline (Hybrid HPO + Early Stopping + Run Folder Saving)\n",
    "\n",
    "FIXED VERSION v31:\n",
    "- LightGBM feature_name mismatch error 해결\n",
    "- 모든 알고리즘에서 동일한 전처리 방식 사용\n",
    "- LightGBM 카테고리컬 변수 내장 처리 활용\n",
    "- MLP만 별도 StandardScaler 적용\n",
    "- 테스트 모드 추가 (빠른 검증용)\n",
    "- NaN 처리 강화\n",
    "\n",
    "What's new vs prior version:\n",
    "- HPO backend by stage: stage 1–3 -> random, stage 4–6 -> optuna (auto mapping)\n",
    "- Optional Bayes backend (skopt) via FORCE_BACKEND='bayes'\n",
    "- Early stopping:\n",
    "  * CV fold: XGB/LGBM use eval_set on each fold; MLP uses early_stopping=True\n",
    "  * Final refit: hold out last 10% (time-based) as eval for XGB/LGBM, MLP uses internal val\n",
    "- Per-building vs Global: different HPO budgets\n",
    "- Saving outputs under runs/{YYYYMMDD_HHMM}_{tag}/\n",
    "- TEST MODE for quick validation (3-5 minutes)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# 0) Global Config\n",
    "# ============================================================\n",
    "TARGET_COL = \"power_consumption\"\n",
    "TIME_COL = \"num_date_time\"         # submission ID와 동일한 시간/ID 컬럼 (이미 조합되어 있다고 가정)\n",
    "BUILDING_COL = \"building_number\"\n",
    "\n",
    "# CV defaults\n",
    "DEFAULT_CV_TYPE = \"kfold\"      # 'kfold' | 'timesplit'\n",
    "DEFAULT_N_SPLITS = 5\n",
    "DEFAULT_GAP = 0\n",
    "DEFAULT_RANDOM_STATE = 42\n",
    "\n",
    "# Seeds (stage 5/6)\n",
    "DEFAULT_SEEDS = [13, 21, 42, 77, 123]\n",
    "\n",
    "# Search metric & alpha for weighted MSE\n",
    "DEFAULT_SEARCH_METRIC = \"weighted_mse\"  # 'weighted_mse'|'smape'|'mae'|'mse'\n",
    "DEFAULT_ALPHA = 3.0\n",
    "\n",
    "# Early stopping config\n",
    "ES_ROUNDS_XGB = 100\n",
    "ES_ROUNDS_LGBM = 100\n",
    "MLP_EARLY_STOP = True\n",
    "MLP_N_NO_CHANGE = 20\n",
    "MLP_VAL_FRAC = 0.1\n",
    "\n",
    "# HPO budgets (n_trials) by scope & backend\n",
    "BUDGETS = {\n",
    "    \"per_building\": {\"random\": 1, \"optuna\": 1, \"bayes\": 1},\n",
    "    \"global\":       {\"random\": 1, \"optuna\":1, \"bayes\": 1},\n",
    "}\n",
    "\n",
    "# TEST MODE CONFIG - 빠른 테스트를 위한 설정\n",
    "TEST_MODE_CONFIG = {\n",
    "    \"sample_frac\": 0.1,           # 데이터 10%만 사용\n",
    "    \"max_buildings\": 3,           # 최대 3개 건물만 테스트\n",
    "    \"cv_splits\": 2,               # CV 2-fold로 축소\n",
    "    \"hpo_trials\": 1,              # HPO 1회만 시도\n",
    "    \"seeds\": [42],                # seed 1개만 사용\n",
    "    \"es_rounds_xgb\": 10,          # early stopping rounds 축소\n",
    "    \"es_rounds_lgbm\": 10,\n",
    "    \"mlp_max_iter\": 50,           # MLP iterations 축소\n",
    "    \"mlp_n_no_change\": 5,\n",
    "}\n",
    "\n",
    "# Backend mapping by stage\n",
    "HPO_BACKEND_FOR_STAGE = {\n",
    "    1: \"random\", 2: \"random\", 3: \"random\",\n",
    "    4: \"random\", 5: \"random\", 6: \"random\", #optuna\n",
    "}\n",
    "# To force a backend globally (e.g., 'bayes'), set to 'random'|'optuna'|'bayes' or None to auto\n",
    "FORCE_BACKEND: Optional[str] = None\n",
    "\n",
    "# ============================================================\n",
    "# 1) Feature Sets (as provided)\n",
    "# ============================================================\n",
    "TYPE1_BASE = [\n",
    "    \"temperature\", \"humidity\", \"windspeed\",\n",
    "    \"day_of_week\", \"month\", \"week\", \"is_holiday\",\n",
    "    \"sin_hour\", \"cos_hour\",\n",
    "    \"day_hour_mean\", \"day_hour_std\",\n",
    "    \"holiday_hour_mean\", \"holiday_std\",\n",
    "    \"THI\", \"WCT\", \"CDH\"\n",
    "]\n",
    "TYPE1_DETAIL = [\n",
    "    \"summer_sin\", \"summer_cos\",\n",
    "    \"day_max_temperature\", \"day_min_temperature\",\n",
    "    \"day_mean_temperature\", \"day_diff_temperature\"\n",
    "]\n",
    "\n",
    "TYPE2_BASE = [\n",
    "    \"temperature\", \"humidity\", \"windspeed\",\n",
    "    \"day_of_week\", \"month\", \"week\", \"is_holiday\",\n",
    "    \"sin_hour\", \"cos_hour\",\n",
    "    \"day_hour_mean\", \"day_hour_std\",\n",
    "    \"holiday_hour_mean\", \"holiday_std\",\n",
    "    \"THI\", \"WCT\", \"CDH\",\n",
    "    \"total_area\", \"cooling_area\",\n",
    "    \"building_number\", \"building_type\"\n",
    "]\n",
    "TYPE2_DETAIL = [\n",
    "    \"summer_sin\", \"summer_cos\",\n",
    "    \"day_max_temperature\", \"day_min_temperature\",\n",
    "    \"day_mean_temperature\", \"day_diff_temperature\",\n",
    "    \"pv_temp\", \"ess_pcs_std\"\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    name: str            # 'A'|'B'|'C'|'D'\n",
    "    scope: str           # 'per_building' | 'global'\n",
    "    features: List[str]\n",
    "\n",
    "def build_model_specs() -> List[ModelSpec]:\n",
    "    \"\"\"\n",
    "    아키텍처에 맞게 모델 스펙 정의:\n",
    "    - 모델 A: 개별 건물별 학습 (Type 1: base_1 feature)\n",
    "    - 모델 B: 개별 건물별 학습 (Type 1: base_1 + detail_1 feature)\n",
    "    - 모델 C: 전체 데이터로 학습 (Type 2: base_2 feature만)\n",
    "    - 모델 D: 전체 데이터로 학습 (Type 2: base_2 + detail_2 feature)\n",
    "    \"\"\"\n",
    "    return [\n",
    "        ModelSpec(\"A\", \"per_building\", TYPE1_BASE),                    # 개별 건물, base_1\n",
    "        ModelSpec(\"B\", \"per_building\", TYPE1_BASE + TYPE1_DETAIL),     # 개별 건물, base_1 + detail_1\n",
    "        ModelSpec(\"C\", \"global\", TYPE2_BASE),                          # 전체, base_2만\n",
    "        ModelSpec(\"D\", \"global\", TYPE2_BASE + TYPE2_DETAIL),           # 전체, base_2 + detail_2\n",
    "    ]\n",
    "\n",
    "# ============================================================\n",
    "# 2) Utils & Metrics\n",
    "# ============================================================\n",
    "def sanitize_features(df: pd.DataFrame, features: List[str]) -> List[str]:\n",
    "    present = [c for c in features if c in df.columns]\n",
    "    missing = [c for c in features if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[warn] Missing features ignored: {missing}\")\n",
    "    return present\n",
    "\n",
    "def load_data(train_path: str, test_path: str, binfo_path: Optional[str]=None, submission_path: Optional[str]=None):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    binfo = pd.read_csv(binfo_path) if binfo_path else None\n",
    "    sub = pd.read_csv(submission_path) if submission_path else None\n",
    "    return train, test, binfo, sub\n",
    "\n",
    "def merge_static(df: pd.DataFrame, binfo: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    if binfo is None:\n",
    "        return df\n",
    "    if BUILDING_COL not in df.columns:\n",
    "        raise KeyError(f\"'{BUILDING_COL}' not in df\")\n",
    "    return df.merge(binfo, on=BUILDING_COL, how=\"left\")\n",
    "\n",
    "def smape(y_true: np.ndarray, y_pred: np.ndarray, eps: float=1e-6) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) + eps\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "def weighted_mse(y_true: np.ndarray, y_pred: np.ndarray, alpha: float=DEFAULT_ALPHA, eps: float=1e-6) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    w = (np.abs(y_true) + eps) ** float(alpha)\n",
    "    se = (y_true - y_pred) ** 2\n",
    "    return float(np.average(se, weights=w))\n",
    "\n",
    "def make_search_scorer(metric: str = DEFAULT_SEARCH_METRIC):\n",
    "    metric = metric.lower()\n",
    "    def _score(estimator, X, y):\n",
    "        y_hat = estimator.predict(X)\n",
    "        if metric == \"weighted_mse\":\n",
    "            alpha = estimator.get_params().get(\"meta__alpha\", DEFAULT_ALPHA)\n",
    "            return -weighted_mse(y, y_hat, alpha=alpha)  # sklearn: higher is better\n",
    "        elif metric == \"smape\":\n",
    "            return -smape(y, y_hat)\n",
    "        elif metric == \"mae\":\n",
    "            return -mean_absolute_error(y, y_hat)\n",
    "        elif metric == \"mse\":\n",
    "            return -float(np.mean((y - y_hat) ** 2))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown search metric\")\n",
    "    return _score\n",
    "\n",
    "def _metric_value(y_true: np.ndarray, y_pred: np.ndarray, metric: str, alpha: float) -> float:\n",
    "    metric = metric.lower()\n",
    "    if metric == \"weighted_mse\":\n",
    "        return weighted_mse(y_true, y_pred, alpha=alpha)\n",
    "    if metric == \"smape\":\n",
    "        return smape(y_true, y_pred)\n",
    "    if metric == \"mae\":\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    if metric == \"mse\":\n",
    "        return float(np.mean((y_true - y_pred) ** 2))\n",
    "    raise ValueError(\"Unknown metric\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Preprocess & Models (+alpha carrier) - FIXED\n",
    "# ============================================================\n",
    "class AlphaCarrier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, alpha: float = DEFAULT_ALPHA):\n",
    "        self.alpha = alpha\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "def prepare_categorical_features(df: pd.DataFrame, categorical_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    FIX: 카테고리컬 변수를 수치형으로 변환하여 feature_name 불일치 문제 해결\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        if col in df_processed.columns:\n",
    "            if df_processed[col].dtype == 'object' or df_processed[col].dtype.name == 'category':\n",
    "                # LabelEncoder 사용하여 카테고리컬 변수를 수치형으로 변환\n",
    "                le = LabelEncoder()\n",
    "                df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "def make_pipeline(algo: str, feature_cols: List[str], categorical_cols: List[str], random_state: int, alpha: float=DEFAULT_ALPHA) -> Pipeline:\n",
    "    \"\"\"\n",
    "    FIXED: 모든 알고리즘에서 동일한 전처리 방식 사용\n",
    "    - XGBoost, LightGBM: 카테고리컬 변수를 수치형으로 변환 후 내장 카테고리컬 처리\n",
    "    - MLP: StandardScaler 추가 적용\n",
    "    \"\"\"\n",
    "    meta = (\"meta\", AlphaCarrier(alpha=alpha))\n",
    "\n",
    "    if algo == \"xgb\":\n",
    "        est = XGBRegressor(\n",
    "            n_estimators=1000, learning_rate=0.05, max_depth=8,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            tree_method=\"hist\", random_state=random_state, n_jobs=-1,\n",
    "            enable_categorical=True  # XGBoost 내장 카테고리컬 처리 활성화\n",
    "        )\n",
    "        return Pipeline([meta, (\"est\", est)])\n",
    "\n",
    "    elif algo == \"lgbm\":\n",
    "        est = LGBMRegressor(\n",
    "            n_estimators=1000, learning_rate=0.05, num_leaves=64,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=random_state, n_jobs=-1, verbosity=-1,\n",
    "            # LightGBM 내장 카테고리컬 처리는 fit 시에 categorical_feature 파라미터로 지정\n",
    "        )\n",
    "        return Pipeline([meta, (\"est\", est)])\n",
    "\n",
    "    elif algo == \"mlp\":\n",
    "        # MLP만 StandardScaler 적용\n",
    "        scaler = (\"scaler\", StandardScaler())\n",
    "        est = (\"est\", MLPRegressor(\n",
    "            hidden_layer_sizes=(256, 128), activation=\"relu\",\n",
    "            solver=\"adam\", learning_rate_init=1e-3, max_iter=500,\n",
    "            early_stopping=MLP_EARLY_STOP, n_iter_no_change=MLP_N_NO_CHANGE,\n",
    "            validation_fraction=MLP_VAL_FRAC, random_state=random_state\n",
    "        ))\n",
    "        return Pipeline([meta, scaler, est])\n",
    "\n",
    "    raise ValueError(f\"Unknown algo: {algo}\")\n",
    "\n",
    "def default_search_spaces() -> Dict[str, Dict[str, List]]:\n",
    "    alpha_grid = [3.0] #\n",
    "    return {\n",
    "        \"xgb\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__n_estimators\": [1200],\n",
    "            \"est__learning_rate\": [0.03],\n",
    "            \"est__max_depth\": [9],\n",
    "            \"est__subsample\": [0.6],\n",
    "            \"est__colsample_bytree\": [0.45],\n",
    "        },\n",
    "        \"lgbm\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__n_estimators\": [1200],\n",
    "            \"est__learning_rate\": [0.03],\n",
    "            \"est__max_depth\": [9],\n",
    "            \"est__subsample\": [0.6],\n",
    "            \"est__colsample_bytree\": [0.45],\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__hidden_layer_sizes\": [(64, 32)],\n",
    "            \"est__learning_rate_init\": [1e-3],\n",
    "            \"est__max_iter\": [500],\n",
    "        },\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# TEST MODE FUNCTIONS - 빠른 테스트를 위한 함수들\n",
    "# ============================================================\n",
    "\n",
    "def create_test_submission_template(sub_template: pd.DataFrame, test_buildings: List) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    테스트용 submission template 생성 - 테스트 건물만 포함\n",
    "    \"\"\"\n",
    "    if BUILDING_COL in sub_template.columns:\n",
    "        # building_number가 있는 경우 필터링\n",
    "        test_sub = sub_template[sub_template[BUILDING_COL].isin(test_buildings)].copy()\n",
    "    else:\n",
    "        # building_number가 없는 경우 TIME_COL에서 건물 정보 추출 시도\n",
    "        # TIME_COL 형태가 \"building_time\" 같은 패턴이라고 가정\n",
    "        test_sub = sub_template.copy()\n",
    "        # 실제 구조에 맞게 조정 필요\n",
    "\n",
    "    print(f\"[test] Original submission template: {len(sub_template)} rows\")\n",
    "    print(f\"[test] Test submission template: {len(test_sub)} rows\")\n",
    "    print(f\"[test] Test buildings: {test_buildings}\")\n",
    "\n",
    "    return test_sub\n",
    "\n",
    "def create_test_data(train_df: pd.DataFrame, test_df: pd.DataFrame, config: dict = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    테스트용 작은 데이터셋 생성\n",
    "    - 데이터 샘플링으로 크기 축소\n",
    "    - 건물 수 제한\n",
    "    - 시간 순서 유지하면서 최근 데이터 위주로 샘플링\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = TEST_MODE_CONFIG\n",
    "\n",
    "    # 건물 수 제한\n",
    "    available_buildings = train_df[BUILDING_COL].unique()\n",
    "    if len(available_buildings) > config[\"max_buildings\"]:\n",
    "        selected_buildings = available_buildings[:config[\"max_buildings\"]]\n",
    "        train_df = train_df[train_df[BUILDING_COL].isin(selected_buildings)]\n",
    "        test_df = test_df[test_df[BUILDING_COL].isin(selected_buildings)]\n",
    "\n",
    "    # 데이터 샘플링 (시간 순서 유지하면서 최근 데이터 위주)\n",
    "    sample_frac = config[\"sample_frac\"]\n",
    "\n",
    "    def sample_by_building(df, frac):\n",
    "        sampled_parts = []\n",
    "        for building_id, building_data in df.groupby(BUILDING_COL):\n",
    "            # 시간 순서대로 정렬 후 마지막 frac 비율만 사용\n",
    "            building_data = building_data.sort_values(TIME_COL)\n",
    "            n_samples = max(int(len(building_data) * frac), 10)  # 최소 10개는 유지\n",
    "            n_samples = min(n_samples, len(building_data))\n",
    "            sampled = building_data.tail(n_samples)\n",
    "            sampled_parts.append(sampled)\n",
    "        return pd.concat(sampled_parts, ignore_index=True)\n",
    "\n",
    "    train_small = sample_by_building(train_df, sample_frac)\n",
    "    test_small = sample_by_building(test_df, sample_frac)\n",
    "\n",
    "    print(f\"[TEST] Original data: train={len(train_df)}, test={len(test_df)}\")\n",
    "    print(f\"[TEST] Sampled data: train={len(train_small)}, test={len(test_small)}\")\n",
    "    print(f\"[TEST] Buildings: {sorted(train_small[BUILDING_COL].unique())}\")\n",
    "\n",
    "    return train_small, test_small\n",
    "\n",
    "def make_test_pipeline(algo: str, feature_cols: List[str], categorical_cols: List[str], random_state: int, alpha: float=DEFAULT_ALPHA, test_config: dict = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    테스트용 파이프라인 생성 (빠른 학습을 위해 파라미터 조정)\n",
    "    \"\"\"\n",
    "    if test_config is None:\n",
    "        test_config = TEST_MODE_CONFIG\n",
    "\n",
    "    meta = (\"meta\", AlphaCarrier(alpha=alpha))\n",
    "\n",
    "    if algo == \"xgb\":\n",
    "        est = XGBRegressor(\n",
    "            n_estimators=100,  # 축소\n",
    "            learning_rate=0.1,  # 빠른 학습\n",
    "            max_depth=4,  # 축소\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            tree_method=\"hist\",\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            enable_categorical=True\n",
    "        )\n",
    "        return Pipeline([meta, (\"est\", est)])\n",
    "\n",
    "    elif algo == \"lgbm\":\n",
    "        est = LGBMRegressor(\n",
    "            n_estimators=100,  # 축소\n",
    "            learning_rate=0.1,  # 빠른 학습\n",
    "            num_leaves=16,  # 축소\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1,\n",
    "        )\n",
    "        return Pipeline([meta, (\"est\", est)])\n",
    "\n",
    "    elif algo == \"mlp\":\n",
    "        scaler = (\"scaler\", StandardScaler())\n",
    "        est = (\"est\", MLPRegressor(\n",
    "            hidden_layer_sizes=(32, 16),  # 축소\n",
    "            activation=\"relu\",\n",
    "            solver=\"adam\",\n",
    "            learning_rate_init=1e-2,  # 빠른 학습\n",
    "            max_iter=test_config[\"mlp_max_iter\"],\n",
    "            early_stopping=MLP_EARLY_STOP,\n",
    "            n_iter_no_change=test_config[\"mlp_n_no_change\"],\n",
    "            validation_fraction=MLP_VAL_FRAC,\n",
    "            random_state=random_state\n",
    "        ))\n",
    "        return Pipeline([meta, scaler, est])\n",
    "\n",
    "    raise ValueError(f\"Unknown algo: {algo}\")\n",
    "\n",
    "def get_test_search_spaces() -> Dict[str, Dict[str, List]]:\n",
    "    \"\"\"테스트용 축소된 search space\"\"\"\n",
    "    alpha_grid = [3.0]\n",
    "    return {\n",
    "        \"xgb\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__n_estimators\": [50, 100],  # 축소\n",
    "            \"est__learning_rate\": [0.1],\n",
    "            \"est__max_depth\": [3, 4],  # 축소\n",
    "        },\n",
    "        \"lgbm\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__n_estimators\": [50, 100],  # 축소\n",
    "            \"est__learning_rate\": [0.1],\n",
    "            \"est__num_leaves\": [8, 16],  # 축소\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__hidden_layer_sizes\": [(16, 8), (32, 16)],  # 축소\n",
    "            \"est__learning_rate_init\": [1e-2],\n",
    "            \"est__max_iter\": [30, 50],  # 축소\n",
    "        },\n",
    "    }\n",
    "\n",
    "def test_single_model_spec(spec: ModelSpec, algo: str, train_df: pd.DataFrame, test_df: pd.DataFrame, test_config: dict = None) -> bool:\n",
    "    \"\"\"\n",
    "    단일 모델 스펙이 제대로 학습되는지 테스트\n",
    "    \"\"\"\n",
    "    if test_config is None:\n",
    "        test_config = TEST_MODE_CONFIG\n",
    "\n",
    "    print(f\"\\n[TEST] Testing Spec {spec.name} ({spec.scope}) with {algo}...\")\n",
    "\n",
    "    try:\n",
    "        feat_cols = sanitize_features(train_df, spec.features)\n",
    "        categorical = [\"building_type\"] if spec.scope == \"global\" else []\n",
    "\n",
    "        # 테스트용 파이프라인 생성 함수 사용\n",
    "        def _pipe(rs: int) -> Pipeline:\n",
    "            return make_test_pipeline(algo=algo, feature_cols=feat_cols, categorical_cols=categorical, random_state=rs, test_config=test_config)\n",
    "\n",
    "        if spec.scope == \"per_building\":\n",
    "            # 첫 번째 건물만 테스트\n",
    "            first_building = train_df[BUILDING_COL].iloc[0]\n",
    "            tr_b = train_df[train_df[BUILDING_COL] == first_building]\n",
    "            te_b = test_df[test_df[BUILDING_COL] == first_building]\n",
    "\n",
    "            if len(tr_b) < 10:\n",
    "                print(f\"[TEST] Skipping building {first_building} - insufficient data\")\n",
    "                return True\n",
    "\n",
    "            tr_b_processed = prepare_categorical_features(tr_b[feat_cols], categorical)\n",
    "            te_b_processed = prepare_categorical_features(te_b[feat_cols], categorical)\n",
    "\n",
    "            X_tr = tr_b_processed\n",
    "            y_tr = tr_b[TARGET_COL].values\n",
    "            X_te = te_b_processed\n",
    "\n",
    "        else:  # global\n",
    "            train_processed = prepare_categorical_features(train_df[feat_cols], categorical)\n",
    "            test_processed = prepare_categorical_features(test_df[feat_cols], categorical)\n",
    "\n",
    "            X_tr = train_processed\n",
    "            y_tr = train_df[TARGET_COL].values\n",
    "            X_te = test_processed\n",
    "\n",
    "        # 간단한 학습 테스트\n",
    "        pipe = _pipe(42)\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "\n",
    "        # 예측 테스트\n",
    "        train_pred = pipe.predict(X_tr)\n",
    "        test_pred = pipe.predict(X_te)\n",
    "\n",
    "        # 성능 체크\n",
    "        train_smape = smape(y_tr, train_pred)\n",
    "        print(f\"[TEST] ✅ Spec {spec.name}-{algo}: Train SMAPE = {train_smape:.2f}%\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST] ❌ Spec {spec.name}-{algo} FAILED: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# ============================================================\n",
    "# 4) CV Splitter\n",
    "# ============================================================\n",
    "def make_splitter(cv_type: str=DEFAULT_CV_TYPE, n_splits: int=DEFAULT_N_SPLITS, gap: int=DEFAULT_GAP):\n",
    "    if cv_type.lower() == \"kfold\":\n",
    "        return KFold(n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE)\n",
    "    if cv_type.lower() == \"timesplit\":\n",
    "        return TimeSeriesSplit(n_splits=n_splits, gap=gap)\n",
    "    raise ValueError(\"cv_type must be 'kfold' or 'timesplit'\")\n",
    "\n",
    "def make_safe_splitter(X, cv_type: str=DEFAULT_CV_TYPE, n_splits: int=DEFAULT_N_SPLITS, gap: int=DEFAULT_GAP):\n",
    "    \"\"\"\n",
    "    FIXED: 데이터 크기에 맞는 안전한 CV splitter 생성\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    # 최소 각 fold에 2개 이상의 샘플이 있도록 조정\n",
    "    safe_n_splits = min(n_splits, max(2, n_samples // 2))\n",
    "\n",
    "    if safe_n_splits != n_splits:\n",
    "        print(f\"[cv] Adjusting n_splits from {n_splits} to {safe_n_splits} due to small dataset size ({n_samples} samples)\")\n",
    "\n",
    "    return make_splitter(cv_type=cv_type, n_splits=safe_n_splits, gap=gap)\n",
    "\n",
    "# ============================================================\n",
    "# 5) HPO Backends (random / optuna / bayes) with Early Stopping per fold - FIXED\n",
    "# ============================================================\n",
    "def _fit_with_es(est: Pipeline, algo: str, X_tr, y_tr, X_val=None, y_val=None, categorical_cols=None, verbose: int=0):\n",
    "    \"\"\"\n",
    "    FIXED: LightGBM 카테고리컬 변수 처리 개선\n",
    "    \"\"\"\n",
    "    fit_params = {}\n",
    "\n",
    "    if algo == \"xgb\":\n",
    "        if X_val is not None and y_val is not None:\n",
    "            est.set_params(\n",
    "                est__early_stopping_rounds=ES_ROUNDS_XGB\n",
    "            )\n",
    "            fit_params.update({\n",
    "                \"est__eval_set\": [(X_val, y_val)],\n",
    "                \"est__verbose\": verbose,\n",
    "            })\n",
    "    elif algo == \"lgbm\":\n",
    "        if X_val is not None and y_val is not None:\n",
    "            est.set_params(\n",
    "                est__early_stopping_rounds=ES_ROUNDS_LGBM\n",
    "            )\n",
    "            fit_params.update({\n",
    "                \"est__eval_set\": [(X_val, y_val)],\n",
    "            })\n",
    "\n",
    "        # FIXED: LightGBM 카테고리컬 변수 지정 (feature 이름이 아닌 인덱스 사용)\n",
    "        if categorical_cols:\n",
    "            cat_indices = []\n",
    "            for i, col in enumerate(X_tr.columns):\n",
    "                if col in categorical_cols:\n",
    "                    cat_indices.append(i)\n",
    "            if cat_indices:\n",
    "                fit_params[\"est__categorical_feature\"] = cat_indices\n",
    "\n",
    "    # MLP early stopping already in params\n",
    "    est.fit(X_tr, y_tr, **fit_params)\n",
    "    return est\n",
    "\n",
    "def _sample_from_list(rng, values):\n",
    "    return values[rng.integers(0, len(values))]\n",
    "\n",
    "def run_random_search_cv(est, X, y, param_space, cv, search_metric, algo, categorical_cols=None, n_iter=50, random_state=42):\n",
    "    \"\"\"Manual random search so we can pass per-fold eval_set for early stopping.\"\"\"\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    scorer = make_search_scorer(search_metric)\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    for t in range(n_iter):\n",
    "        # Random param dict\n",
    "        params = {k: _sample_from_list(rng, v) for k, v in param_space.items()}\n",
    "        est_ = clone(est).set_params(**params)\n",
    "\n",
    "        # inner-CV with ES\n",
    "        fold_scores = []\n",
    "        for tr_idx, va_idx in cv.split(X):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "            est_fold = clone(est_)\n",
    "\n",
    "            _fit_with_es(est_fold, algo, X_tr, y_tr, X_va, y_va, categorical_cols=categorical_cols, verbose=0)\n",
    "            fold_scores.append(scorer(est_fold, X_va, y_va))\n",
    "\n",
    "        mean_sc = float(np.mean(fold_scores))\n",
    "        if mean_sc > best_score:\n",
    "            best_score, best_params = mean_sc, params\n",
    "\n",
    "    best_est = clone(est).set_params(**best_params)\n",
    "    # fit on full data w/o val (or small split in final-refit step)\n",
    "    _fit_with_es(best_est, algo, X, y, categorical_cols=categorical_cols)\n",
    "    return best_est, best_params\n",
    "\n",
    "def run_optuna_cv(est, X, y, param_space, cv, search_metric, algo, categorical_cols=None, n_iter=50, random_state=42):\n",
    "    import optuna\n",
    "    from sklearn.base import clone\n",
    "    scorer = make_search_scorer(search_metric)\n",
    "\n",
    "    def suggest_from_list(trial, name, values):\n",
    "        v0 = values[0]\n",
    "        # try numeric ranges\n",
    "        if all(isinstance(v, (int, np.integer)) for v in values):\n",
    "            return trial.suggest_int(name, int(min(values)), int(max(values)))\n",
    "        if all(isinstance(v, (float, np.floating)) for v in values):\n",
    "            return trial.suggest_float(name, float(min(values)), float(max(values)))\n",
    "        return trial.suggest_categorical(name, values)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {k: suggest_from_list(trial, k, vals) for k, vals in param_space.items()}\n",
    "        est_ = clone(est).set_params(**params)\n",
    "        scores = []\n",
    "        for tr_idx, va_idx in cv.split(X):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "            est_fold = clone(est_)\n",
    "            _fit_with_es(est_fold, algo, X_tr, y_tr, X_va, y_va, categorical_cols=categorical_cols, verbose=0)\n",
    "            score = scorer(est_fold, X_va, y_va)\n",
    "            scores.append(score)\n",
    "            trial.report(score, step=len(scores))\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner, sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "    study.optimize(objective, n_trials=n_iter, gc_after_trial=True)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_est = clone(est).set_params(**best_params)\n",
    "    _fit_with_es(best_est, algo, X, y, categorical_cols=categorical_cols)\n",
    "    return best_est, best_params\n",
    "\n",
    "def run_bayes_cv(est, X, y, param_space, cv, search_metric, algo, categorical_cols=None, n_iter=50, random_state=42):\n",
    "    \"\"\"Bayesian optimization via skopt.gp_minimize with Categorical dims (simple & robust).\"\"\"\n",
    "    from skopt import gp_minimize\n",
    "    from skopt.space import Categorical\n",
    "    from sklearn.base import clone\n",
    "    scorer = make_search_scorer(search_metric)\n",
    "\n",
    "    # FIXED: n_iter가 너무 작으면 random search로 fallback\n",
    "    if n_iter < 10:\n",
    "        print(f\"[bayes] n_iter={n_iter} < 10, falling back to random search\")\n",
    "        return run_random_search_cv(est, X, y, param_space, cv, search_metric, algo, categorical_cols, n_iter, random_state)\n",
    "\n",
    "    dims = []\n",
    "    keys = []\n",
    "    for k, vals in param_space.items():\n",
    "        dims.append(Categorical(vals, name=k))\n",
    "        keys.append(k)\n",
    "\n",
    "    def objective(list_vals):\n",
    "        params = {k: v for k, v in zip(keys, list_vals)}\n",
    "        est_ = clone(est).set_params(**params)\n",
    "        scores = []\n",
    "        for tr_idx, va_idx in cv.split(X):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "            est_fold = clone(est_)\n",
    "            _fit_with_es(est_fold, algo, X_tr, y_tr, X_va, y_va, categorical_cols=categorical_cols, verbose=0)\n",
    "            score = scorer(est_fold, X_va, y_va)\n",
    "            scores.append(score)\n",
    "        # gp_minimize minimizes → return negative (we want to maximize scorer)\n",
    "        return -float(np.mean(scores))\n",
    "\n",
    "    res = gp_minimize(\n",
    "        objective, dimensions=dims, n_calls=n_iter, random_state=random_state, verbose=False\n",
    "    )\n",
    "    best_params = {k: v for k, v in zip(keys, res.x)}\n",
    "    best_est = clone(est).set_params(**best_params)\n",
    "    _fit_with_es(best_est, algo, X, y, categorical_cols=categorical_cols)\n",
    "    return best_est, best_params\n",
    "\n",
    "def run_hpo_backend(est, X, y, param_space, cv, search_metric, backend, algo, categorical_cols=None, n_iter=50, random_state=42):\n",
    "    backend = backend.lower()\n",
    "    if backend == \"random\":\n",
    "        return run_random_search_cv(est, X, y, param_space, cv, search_metric, algo, categorical_cols=categorical_cols, n_iter=n_iter, random_state=random_state)\n",
    "    if backend == \"optuna\":\n",
    "        return run_optuna_cv(est, X, y, param_space, cv, search_metric, algo, categorical_cols=categorical_cols, n_iter=n_iter, random_state=random_state)\n",
    "    if backend == \"bayes\":\n",
    "        return run_bayes_cv(est, X, y, param_space, cv, search_metric, algo, categorical_cols=categorical_cols, n_iter=n_iter, random_state=random_state)\n",
    "    raise ValueError(\"backend must be 'random'|'optuna'|'bayes'\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) OOF Training + Final Refit with ES + Predictions - FIXED\n",
    "# ============================================================\n",
    "def oof_fit_predict(\n",
    "    pipeline: Pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    do_search: bool,\n",
    "    algo: str,\n",
    "    param_spaces: Dict[str, Dict[str, List]],\n",
    "    outer_splitter,\n",
    "    inner_cv_type: str,\n",
    "    inner_splits: int,\n",
    "    random_state: int,\n",
    "    search_metric: str,\n",
    "    search_backend: str,\n",
    "    search_n_iter: int,\n",
    "    categorical_cols: List[str] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Pipeline, List[Dict], List[float]]:\n",
    "    \"\"\"Return OOF, test_pred(mean-of-fold), last_est, fold best_params, fold metric values.\"\"\"\n",
    "    oof = np.zeros(len(X))\n",
    "    test_fold_preds = []\n",
    "    last_est = None\n",
    "    fold_smapes = []\n",
    "    fold_best_params: List[Dict] = []\n",
    "    fold_metric_vals: List[float] = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(outer_splitter.split(X)):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "        est = clone(pipeline)\n",
    "        chosen_params = {}\n",
    "        if do_search:\n",
    "            inner_splitter = make_safe_splitter(X_tr, cv_type=inner_cv_type, n_splits=inner_splits, gap=0)\n",
    "            est, chosen_params = run_hpo_backend(\n",
    "                est, X_tr, y_tr,\n",
    "                param_space=param_spaces.get(algo, {}),\n",
    "                cv=inner_splitter,\n",
    "                search_metric=search_metric,\n",
    "                backend=search_backend,\n",
    "                algo=algo,\n",
    "                categorical_cols=categorical_cols,\n",
    "                n_iter=search_n_iter,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            # no search: just fit with ES on val\n",
    "            est = _fit_with_es(est, algo, X_tr, y_tr, X_val, y_val, categorical_cols=categorical_cols, verbose=0)\n",
    "\n",
    "        # Validate\n",
    "        y_hat_val = est.predict(X_val)\n",
    "        oof[val_idx] = y_hat_val\n",
    "        sm = smape(y_val, y_hat_val)\n",
    "        alpha_here = est.get_params().get(\"meta__alpha\", DEFAULT_ALPHA)\n",
    "        mv = _metric_value(y_val, y_hat_val, search_metric, alpha_here)\n",
    "        fold_smapes.append(sm)\n",
    "        fold_metric_vals.append(mv)\n",
    "        fold_best_params.append(chosen_params)\n",
    "        print(f\"[val] fold {fold+1}/{getattr(outer_splitter, 'n_splits', '?')}  SMAPE={sm:.3f}% | {search_metric}={mv:.6f}\")\n",
    "\n",
    "        # Test via current fold model (for completeness; final_refit will also run)\n",
    "        test_fold_preds.append(est.predict(X_test))\n",
    "        last_est = est\n",
    "\n",
    "    print(f\"[val] mean SMAPE: {np.mean(fold_smapes):.3f}% (±{np.std(fold_smapes):.3f})\")\n",
    "    test_pred = np.mean(test_fold_preds, axis=0)\n",
    "    return oof, test_pred, last_est, fold_best_params, fold_metric_vals\n",
    "\n",
    "def _final_refit_with_holdout(est: Pipeline, algo: str, X: pd.DataFrame, y: np.ndarray, categorical_cols: List[str] = None, holdout_frac: float=0.1):\n",
    "    \"\"\"Time-based final refit with early stopping using last `holdout_frac` of rows.\"\"\"\n",
    "    # n = len(X)\n",
    "    # if n >= 20 and holdout_frac > 0:\n",
    "    #     k = max(int(n * (1 - holdout_frac)), n - 48)  # at least 48 points in holdout if possible\n",
    "    #     X_tr, y_tr = X.iloc[:k], y[:k]\n",
    "    #     X_val, y_val = X.iloc[k:], y[k:]\n",
    "    #     est = _fit_with_es(est, algo, X_tr, y_tr, X_val, y_val, categorical_cols=categorical_cols, verbose=0)\n",
    "    # else:\n",
    "    #     _fit_with_es(est, algo, X, y, categorical_cols=categorical_cols)  # fallback\n",
    "    return _fit_with_es(est, algo, X, y, categorical_cols=categorical_cols)\n",
    "\n",
    "def train_predict_for_spec(\n",
    "    spec: ModelSpec,\n",
    "    algo: str,\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    do_search: bool,\n",
    "    seeds: List[int],\n",
    "    cv_type: str,\n",
    "    n_splits: int,\n",
    "    param_spaces: Dict[str, Dict[str, List]],\n",
    "    gap: int,\n",
    "    inner_cv_type: Optional[str],\n",
    "    inner_splits: Optional[int],\n",
    "    search_metric: str,\n",
    "    search_backend: str,\n",
    "    search_n_iter: int,\n",
    "    do_final_refit: bool=True,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Returns (pred_df, oof_df) with test predictions from final refit models.\"\"\"\n",
    "    inner_cv_type = inner_cv_type or cv_type\n",
    "    inner_splits = inner_splits or 3\n",
    "    feat_cols = sanitize_features(train_df, spec.features)\n",
    "    categorical = [\"building_type\"] if spec.scope == \"global\" else []\n",
    "\n",
    "    def _pipe(rs: int) -> Pipeline:\n",
    "        return make_pipeline(algo=algo, feature_cols=feat_cols, categorical_cols=categorical, random_state=rs, alpha=DEFAULT_ALPHA)\n",
    "\n",
    "    all_pred_parts = []\n",
    "    all_oof_parts = []\n",
    "\n",
    "    if spec.scope == \"per_building\":\n",
    "        for b_id, tr_b in train_df.groupby(BUILDING_COL):\n",
    "            te_b = test_df.loc[test_df[BUILDING_COL] == b_id]\n",
    "            if te_b.empty:\n",
    "                print(f\"[warn] Building {b_id} not found in test data, skipping...\")\n",
    "                continue\n",
    "\n",
    "            # FIXED: 카테고리컬 변수 전처리\n",
    "            tr_b_processed = prepare_categorical_features(tr_b[feat_cols], categorical)\n",
    "            te_b_processed = prepare_categorical_features(te_b[feat_cols], categorical)\n",
    "\n",
    "            X_tr = tr_b_processed\n",
    "            y_tr = tr_b[TARGET_COL].values\n",
    "            X_te = te_b_processed\n",
    "\n",
    "            # 최소 데이터 체크\n",
    "            if len(X_tr) < 5:\n",
    "                print(f\"[warn] Building {b_id} has insufficient training data ({len(X_tr)} samples), filling with mean...\")\n",
    "                # 다른 건물들의 평균으로 예측값 생성\n",
    "                if all_pred_parts:  # 이미 처리된 건물이 있는 경우\n",
    "                    mean_pred = np.mean([df[f\"pred_{spec.name}_{algo}\"].mean() for df in all_pred_parts])\n",
    "                else:\n",
    "                    mean_pred = y_tr.mean() if len(y_tr) > 0 else 0.0\n",
    "\n",
    "                pred_b = te_b[[TIME_COL, BUILDING_COL]].copy()\n",
    "                pred_b[f\"pred_{spec.name}_{algo}\"] = mean_pred\n",
    "                all_pred_parts.append(pred_b)\n",
    "\n",
    "                oof_b = tr_b[[TIME_COL, BUILDING_COL]].copy()\n",
    "                oof_b[\"oof\"] = mean_pred\n",
    "                all_oof_parts.append(oof_b)\n",
    "                continue\n",
    "\n",
    "            outer_splitter = make_safe_splitter(X_tr, cv_type=cv_type, n_splits=n_splits, gap=gap)\n",
    "\n",
    "            seed_oofs = []\n",
    "            seed_final_preds = []\n",
    "            for s in seeds:\n",
    "                pipe = _pipe(s)\n",
    "                oof, _, last_est, fold_params, fold_scores = oof_fit_predict(\n",
    "                    pipe, X_tr, y_tr, X_te, do_search, algo, param_spaces,\n",
    "                    outer_splitter, inner_cv_type, inner_splits, s,\n",
    "                    search_metric=search_metric,\n",
    "                    search_backend=search_backend,\n",
    "                    search_n_iter=search_n_iter,\n",
    "                    categorical_cols=categorical,\n",
    "                )\n",
    "                seed_oofs.append(oof)\n",
    "\n",
    "                best_idx = int(np.argmin(fold_scores)) if fold_scores else 0\n",
    "                best_params = fold_params[best_idx] if fold_params else {}\n",
    "\n",
    "                if do_final_refit:\n",
    "                    final_model = clone(_pipe(s))\n",
    "                    if best_params:\n",
    "                        final_model.set_params(**best_params)\n",
    "                    final_model = _final_refit_with_holdout(final_model, algo, X_tr, y_tr, categorical_cols=categorical, holdout_frac=0.1)\n",
    "                    train_pred_full = final_model.predict(X_tr)\n",
    "                    f_smape = smape(y_tr, train_pred_full)\n",
    "                    f_r2 = r2_score(y_tr, train_pred_full)\n",
    "                    print(f\"[final-fit][Spec {spec.name} | Algo {algo} | B{b_id}] Final Train SMAPE={f_smape:.4f}% | R2={f_r2:.4f} | Params={best_params}\")\n",
    "                    seed_final_preds.append(final_model.predict(X_te))\n",
    "                else:\n",
    "                    seed_final_preds.append(last_est.predict(X_te))\n",
    "\n",
    "            te_avg = np.mean(seed_final_preds, axis=0)\n",
    "            oof_avg = np.mean(seed_oofs, axis=0)\n",
    "\n",
    "            pred_b = te_b[[TIME_COL, BUILDING_COL]].copy()\n",
    "            pred_b[f\"pred_{spec.name}_{algo}\"] = te_avg\n",
    "            all_pred_parts.append(pred_b)\n",
    "\n",
    "            oof_b = tr_b[[TIME_COL, BUILDING_COL]].copy()\n",
    "            oof_b[\"oof\"] = oof_avg\n",
    "            all_oof_parts.append(oof_b)\n",
    "\n",
    "        # FIXED: 빈 리스트 체크\n",
    "        if not all_pred_parts:\n",
    "            print(f\"[warn] No buildings processed for {spec.name}_{algo}, creating empty dataframes\")\n",
    "            pred_df = pd.DataFrame(columns=[TIME_COL, BUILDING_COL, f\"pred_{spec.name}_{algo}\"])\n",
    "            oof_df = pd.DataFrame(columns=[TIME_COL, BUILDING_COL, \"oof\"])\n",
    "        else:\n",
    "            pred_df = pd.concat(all_pred_parts, ignore_index=True)\n",
    "            oof_df = pd.concat(all_oof_parts, ignore_index=True)\n",
    "\n",
    "    else:  # global\n",
    "        # FIXED: 카테고리컬 변수 전처리\n",
    "        train_processed = prepare_categorical_features(train_df[feat_cols], categorical)\n",
    "        test_processed = prepare_categorical_features(test_df[feat_cols], categorical)\n",
    "\n",
    "        X_tr = train_processed\n",
    "        y_tr = train_df[TARGET_COL].values\n",
    "        X_te = test_processed\n",
    "        outer_splitter = make_safe_splitter(X_tr, cv_type=cv_type, n_splits=n_splits, gap=gap)\n",
    "\n",
    "        seed_oofs = []\n",
    "        seed_final_preds = []\n",
    "        for s in seeds:\n",
    "            pipe = _pipe(s)\n",
    "            oof, _, last_est, fold_params, fold_scores = oof_fit_predict(\n",
    "                pipe, X_tr, y_tr, X_te, do_search, algo, param_spaces,\n",
    "                outer_splitter, inner_cv_type, inner_splits, s,\n",
    "                search_metric=search_metric,\n",
    "                search_backend=search_backend,\n",
    "                search_n_iter=search_n_iter,\n",
    "                categorical_cols=categorical,\n",
    "            )\n",
    "            seed_oofs.append(oof)\n",
    "\n",
    "            best_idx = int(np.argmin(fold_scores)) if fold_scores else 0\n",
    "            best_params = fold_params[best_idx] if fold_params else {}\n",
    "\n",
    "            if do_final_refit:\n",
    "                final_model = clone(_pipe(s))\n",
    "                if best_params:\n",
    "                    final_model.set_params(**best_params)\n",
    "                final_model = _final_refit_with_holdout(final_model, algo, X_tr, y_tr, categorical_cols=categorical, holdout_frac=0.1)\n",
    "                train_pred_full = final_model.predict(X_tr)\n",
    "                f_smape = smape(y_tr, train_pred_full)\n",
    "                f_r2 = r2_score(y_tr, train_pred_full)\n",
    "                print(f\"[final-fit][Spec {spec.name} | Algo {algo} | GLOBAL] Final Train SMAPE={f_smape:.4f}% | R2={f_r2:.4f} | Params={best_params}\")\n",
    "                seed_final_preds.append(final_model.predict(X_te))\n",
    "            else:\n",
    "                seed_final_preds.append(last_est.predict(X_te))\n",
    "\n",
    "        te_avg = np.mean(seed_final_preds, axis=0)\n",
    "        oof_avg = np.mean(seed_oofs, axis=0)\n",
    "\n",
    "        pred_df = test_df[[TIME_COL, BUILDING_COL]].copy()\n",
    "        pred_df[f\"pred_{spec.name}_{algo}\"] = te_avg\n",
    "\n",
    "        oof_df = train_df[[TIME_COL, BUILDING_COL]].copy()\n",
    "        oof_df[\"oof\"] = oof_avg\n",
    "\n",
    "    return pred_df, oof_df\n",
    "\n",
    "# ============================================================\n",
    "# 7) Ensembling\n",
    "# ============================================================\n",
    "from functools import reduce\n",
    "def simple_mean_ensemble(pred_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    FIXED: 테스트 모드 고려한 앙상블 함수\n",
    "    \"\"\"\n",
    "    if not pred_dfs:\n",
    "        raise ValueError(\"Empty prediction dataframes list\")\n",
    "\n",
    "    print(f\"[ensemble] Merging {len(pred_dfs)} prediction DataFrames\")\n",
    "    for i, df in enumerate(pred_dfs):\n",
    "        pred_cols_here = [c for c in df.columns if c.startswith('pred_')]\n",
    "        print(f\"[ensemble] DataFrame {i}: shape={df.shape}, pred_cols={pred_cols_here}\")\n",
    "\n",
    "    # FIXED: 빈 DataFrame 처리\n",
    "    non_empty_dfs = [df for df in pred_dfs if len(df) > 0]\n",
    "    if not non_empty_dfs:\n",
    "        print(f\"[ensemble] All dataframes are empty, creating dummy result\")\n",
    "        return pd.DataFrame({TIME_COL: [], BUILDING_COL: [], \"answer\": []})\n",
    "\n",
    "    merged = reduce(lambda l, r: l.merge(r, on=[TIME_COL, BUILDING_COL], how=\"outer\"), non_empty_dfs)\n",
    "    pred_cols = [c for c in merged.columns if c.startswith(\"pred_\")]\n",
    "\n",
    "    print(f\"[ensemble] Merged shape: {merged.shape}, pred_cols: {pred_cols}\")\n",
    "\n",
    "    if not pred_cols:\n",
    "        print(f\"[ensemble] No prediction columns found, creating zero predictions\")\n",
    "        merged[\"answer\"] = 0.0\n",
    "    else:\n",
    "        merged[\"answer\"] = merged[pred_cols].mean(axis=1)\n",
    "\n",
    "    # NaN 체크 - 테스트 모드에서는 경고만\n",
    "    nan_count = merged[\"answer\"].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"[ensemble] INFO: {nan_count} NaN values in answer column (normal in test mode)\")\n",
    "\n",
    "        # 테스트 모드가 아닌 경우에만 NaN 대체\n",
    "        if nan_count == len(merged):  # 모든 값이 NaN인 경우만 대체\n",
    "            print(f\"[ensemble] All values are NaN, replacing with 0\")\n",
    "            merged[\"answer\"] = 0.0\n",
    "\n",
    "    result = merged[[TIME_COL, BUILDING_COL, \"answer\"]]\n",
    "    print(f\"[ensemble] Final result shape: {result.shape}\")\n",
    "    return result\n",
    "\n",
    "def stacking_ensemble(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    oof_pack: List[Tuple[pd.DataFrame, str]],\n",
    "    pred_pack: List[Tuple[pd.DataFrame, str]],\n",
    "    meta_model = None,\n",
    ") -> pd.DataFrame:\n",
    "    meta_model = meta_model or Ridge(alpha=1.0, random_state=DEFAULT_RANDOM_STATE)\n",
    "    # Build oof matrix\n",
    "    oof_merged = None; names = []\n",
    "    for df, name in oof_pack:\n",
    "        names.append(name)\n",
    "        df2 = df.rename(columns={\"oof\": name})\n",
    "        oof_merged = df2 if oof_merged is None else oof_merged.merge(df2, on=[TIME_COL, BUILDING_COL], how=\"left\")\n",
    "    oof_merged = oof_merged.merge(train_df[[TIME_COL, BUILDING_COL, TARGET_COL]], on=[TIME_COL, BUILDING_COL], how=\"left\")\n",
    "\n",
    "    pred_merged = None\n",
    "    for df, name in pred_pack:\n",
    "        df2 = df.rename(columns={name: name})\n",
    "        pred_merged = df2 if pred_merged is None else pred_merged.merge(df2, on=[TIME_COL, BUILDING_COL], how=\"left\")\n",
    "\n",
    "    X_meta = oof_merged[names].values\n",
    "    y_meta = oof_merged[TARGET_COL].values\n",
    "    X_meta_test = pred_merged[names].values\n",
    "\n",
    "    meta_model.fit(X_meta, y_meta)\n",
    "    meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "    out = pred_merged[[TIME_COL, BUILDING_COL]].copy()\n",
    "    out[\"answer\"] = meta_pred\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 8) Stage Runner + Backend auto-select + Save helpers\n",
    "# ============================================================\n",
    "def _backend_for_stage(stage: int) -> str:\n",
    "    if FORCE_BACKEND:\n",
    "        return FORCE_BACKEND\n",
    "    return HPO_BACKEND_FOR_STAGE.get(stage, \"random\")\n",
    "\n",
    "def _budget_for(scope: str, backend: str) -> int:\n",
    "    return BUDGETS[scope][backend]\n",
    "\n",
    "def run_stage(\n",
    "    stage: int,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    submission_template: pd.DataFrame,\n",
    "    algorithms: List[str],\n",
    "    do_search: bool,\n",
    "    seeds: List[int],\n",
    "    cv_type: str,\n",
    "    n_splits: int,\n",
    "    param_spaces: Dict[str, Dict[str, List]],\n",
    "    stacking: bool = False,\n",
    "    gap: int = DEFAULT_GAP,\n",
    "    inner_cv_type: Optional[str] = None,\n",
    "    inner_splits: Optional[int] = None,\n",
    "    search_metric: str = DEFAULT_SEARCH_METRIC,\n",
    "    force_backend: Optional[str] = None,\n",
    ") -> Tuple[pd.DataFrame, List[pd.DataFrame], List[Tuple[pd.DataFrame, str]]]:\n",
    "    specs = build_model_specs()\n",
    "    preds = []; oofs = []\n",
    "\n",
    "    # backend choice\n",
    "    backend = force_backend or _backend_for_stage(stage)\n",
    "\n",
    "    for spec in specs:\n",
    "        # per-building vs global budget\n",
    "        search_n_iter = _budget_for(spec.scope, backend)\n",
    "        for algo in algorithms:\n",
    "            print(f\"\\n=== Training Spec={spec.name}({spec.scope}) Algo={algo} | backend={backend} trials={search_n_iter} ===\")\n",
    "            pred_df, oof_df = train_predict_for_spec(\n",
    "                spec=spec, algo=algo, train_df=train, test_df=test,\n",
    "                do_search=do_search, seeds=seeds,\n",
    "                cv_type=cv_type, n_splits=n_splits,\n",
    "                param_spaces=param_spaces, gap=gap,\n",
    "                inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "                search_metric=search_metric, search_backend=backend,\n",
    "                search_n_iter=search_n_iter,\n",
    "                do_final_refit=True,\n",
    "            )\n",
    "            preds.append(pred_df)\n",
    "            oofs.append((oof_df, f\"{spec.name}_{algo}\"))\n",
    "\n",
    "    if stacking:\n",
    "        # Build pred_pack with unified names\n",
    "        pred_pack = []\n",
    "        for (oof_df, name), pred_df in zip(oofs, preds):\n",
    "            col = [c for c in pred_df.columns if c.startswith(\"pred_\")][0]\n",
    "            pred_pack.append((pred_df.rename(columns={col: name}), name))\n",
    "        final = stacking_ensemble(train, test, oof_pack=oofs, pred_pack=pred_pack, meta_model=Ridge(alpha=1.0))\n",
    "    else:\n",
    "        final = simple_mean_ensemble(preds)\n",
    "\n",
    "    # FIXED: submission merge 개선 - 테스트 모드 고려\n",
    "    print(f\"[stage] Final ensemble shape: {final.shape}\")\n",
    "    print(f\"[stage] Final ensemble columns: {final.columns.tolist()}\")\n",
    "    print(f\"[stage] Submission template shape: {submission_template.shape}\")\n",
    "    print(f\"[stage] Submission template columns: {submission_template.columns.tolist()}\")\n",
    "\n",
    "    sub = submission_template.copy()\n",
    "    if TIME_COL in sub.columns:\n",
    "        # left join으로 submission template 기준 매칭\n",
    "        out = sub[[TIME_COL]].merge(final[[TIME_COL, \"answer\"]], on=TIME_COL, how=\"left\")\n",
    "\n",
    "        # NaN 체크 및 처리 - 테스트 모드에서는 유연하게\n",
    "        nan_count = out[\"answer\"].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            print(f\"[stage] INFO: {nan_count}/{len(out)} rows have NaN in answer column\")\n",
    "\n",
    "            # submission template에는 있지만 prediction에는 없는 TIME_COL 값들 확인\n",
    "            missing_times = set(sub[TIME_COL]) - set(final[TIME_COL])\n",
    "            if missing_times:\n",
    "                print(f\"[stage] Missing TIME_COL values in predictions: {len(missing_times)} values\")\n",
    "                print(f\"[stage] This is normal in test mode with limited buildings\")\n",
    "\n",
    "            # production 모드에서만 NaN 대체 (전체의 50% 이상이 NaN인 경우는 테스트 모드로 간주)\n",
    "            nan_ratio = nan_count / len(out)\n",
    "            if nan_ratio < 0.5:  # 50% 미만이 NaN인 경우만 대체 (production 모드)\n",
    "                mean_val = out[\"answer\"].mean()\n",
    "                if pd.isna(mean_val):\n",
    "                    mean_val = 0.0\n",
    "                print(f\"[stage] Replacing NaN with mean value: {mean_val}\")\n",
    "                out[\"answer\"] = out[\"answer\"].fillna(mean_val)\n",
    "            else:\n",
    "                print(f\"[stage] High NaN ratio ({nan_ratio:.1%}) - keeping NaN for test mode\")\n",
    "    else:\n",
    "        raise KeyError(\"submission_template must include TIME_COL\")\n",
    "\n",
    "    print(f\"[stage] Final output shape: {out.shape}\")\n",
    "    print(f\"[stage] Final NaN count: {out['answer'].isna().sum()}\")\n",
    "    return out, preds, oofs\n",
    "\n",
    "# ---- Saving helpers ----\n",
    "def make_run_dir(tag: str = \"v1\") -> str:\n",
    "    dt = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    run_dir = pathlib.Path(\"runs\") / f\"{dt}_{tag}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return str(run_dir)\n",
    "\n",
    "def save_artifacts(run_dir: str, final_sub: pd.DataFrame, stage_name: str, preds: List[pd.DataFrame], oofs: List[Tuple[pd.DataFrame, str]], config: dict):\n",
    "    # submission\n",
    "    f_sub = os.path.join(run_dir, f\"submission_{stage_name}.csv\")\n",
    "    final_sub.to_csv(f_sub, index=False)\n",
    "    print(f\"[save] {f_sub}\")\n",
    "\n",
    "    # preds & oofs\n",
    "    for i, df in enumerate(preds):\n",
    "        df.to_csv(os.path.join(run_dir, f\"preds_{stage_name}_{i}.csv\"), index=False)\n",
    "    for (df, name) in oofs:\n",
    "        df.to_csv(os.path.join(run_dir, f\"oof_{stage_name}_{name}.csv\"), index=False)\n",
    "\n",
    "    # config\n",
    "    with open(os.path.join(run_dir, f\"config_{stage_name}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[save] artifacts saved to {run_dir}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9) Stage Shortcuts\n",
    "# ============================================================\n",
    "def stage_1_xgb_mean(train, test, sub, **kwargs):\n",
    "    return run_stage(1, train, test, sub, algorithms=[\"xgb\"], do_search=False, seeds=[DEFAULT_RANDOM_STATE], **kwargs)\n",
    "\n",
    "def stage_2_xgb_search_mean(train, test, sub, **kwargs):\n",
    "    return run_stage(2, train, test, sub, algorithms=[\"xgb\"], do_search=True, seeds=[DEFAULT_RANDOM_STATE], **kwargs)\n",
    "\n",
    "def stage_3_triple_mean(train, test, sub, **kwargs):\n",
    "    return run_stage(3, train, test, sub, algorithms=[\"xgb\",\"lgbm\",\"mlp\"], do_search=False, seeds=[DEFAULT_RANDOM_STATE], **kwargs)\n",
    "\n",
    "def stage_4_triple_search_mean(train, test, sub, **kwargs):\n",
    "    return run_stage(4, train, test, sub, algorithms=[\"xgb\", \"lgbm\"], do_search=True, seeds=[DEFAULT_RANDOM_STATE], **kwargs) #\"xgb\",,\"mlp\"\n",
    "\n",
    "def stage_5_triple_search_seedavg_mean(train, test, sub, seeds: List[int]=DEFAULT_SEEDS, **kwargs):\n",
    "    return run_stage(5, train, test, sub, algorithms=[\"xgb\",\"lgbm\",\"mlp\"], do_search=True, seeds=seeds, **kwargs)\n",
    "\n",
    "def stage_6_triple_search_seedavg_stacking(train, test, sub, seeds: List[int]=DEFAULT_SEEDS, **kwargs):\n",
    "    return run_stage(6, train, test, sub, algorithms=[\"xgb\",\"lgbm\",\"mlp\"], do_search=True, seeds=seeds, stacking=True, **kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# TEST MODE FUNCTIONS - 테스트 및 검증용\n",
    "# ============================================================\n",
    "\n",
    "def test_all_backends(train_df: pd.DataFrame, test_df: pd.DataFrame, test_config: dict = None) -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    모든 HPO 백엔드가 제대로 작동하는지 테스트\n",
    "    \"\"\"\n",
    "    if test_config is None:\n",
    "        test_config = TEST_MODE_CONFIG\n",
    "\n",
    "    print(f\"\\n[TEST] Testing HPO backends...\")\n",
    "\n",
    "    # 첫 번째 스펙과 알고리즘으로 테스트\n",
    "    spec = build_model_specs()[0]  # 모델 A\n",
    "    algo = \"xgb\"\n",
    "\n",
    "    feat_cols = sanitize_features(train_df, spec.features)\n",
    "    categorical = []\n",
    "\n",
    "    # 글로벌 데이터로 빠른 테스트\n",
    "    train_processed = prepare_categorical_features(train_df[feat_cols], categorical)\n",
    "    X_tr = train_processed\n",
    "    y_tr = train_df[TARGET_COL].values\n",
    "\n",
    "    # 작은 CV split - 데이터 크기에 맞게 조정\n",
    "    cv = make_safe_splitter(X_tr, cv_type=\"kfold\", n_splits=test_config[\"cv_splits\"])\n",
    "    search_spaces = get_test_search_spaces()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for backend in [\"random\", \"optuna\", \"bayes\"]:\n",
    "        try:\n",
    "            print(f\"[TEST] Testing {backend} backend...\")\n",
    "            est = make_test_pipeline(algo, feat_cols, categorical, 42, test_config=test_config)\n",
    "\n",
    "            if backend == \"bayes\":\n",
    "                try:\n",
    "                    import skopt\n",
    "                except ImportError:\n",
    "                    print(f\"[TEST] ⚠️  {backend} backend skipped - skopt not installed\")\n",
    "                    results[backend] = True\n",
    "                    continue\n",
    "\n",
    "            if backend == \"optuna\":\n",
    "                try:\n",
    "                    import optuna\n",
    "                except ImportError:\n",
    "                    print(f\"[TEST] ⚠️  {backend} backend skipped - optuna not installed\")\n",
    "                    results[backend] = True\n",
    "                    continue\n",
    "\n",
    "            best_est, best_params = run_hpo_backend(\n",
    "                est, X_tr, y_tr,\n",
    "                param_space=search_spaces.get(algo, {}),\n",
    "                cv=cv,\n",
    "                search_metric=\"mae\",  # 간단한 메트릭\n",
    "                backend=backend,\n",
    "                algo=algo,\n",
    "                categorical_cols=categorical,\n",
    "                n_iter=1,  # 1회만 테스트\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "            print(f\"[TEST] ✅ {backend} backend works\")\n",
    "            results[backend] = True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[TEST] ❌ {backend} backend FAILED: {str(e)}\")\n",
    "            results[backend] = False\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_stage_test(stage: int, train: pd.DataFrame, test: pd.DataFrame, sub: pd.DataFrame, test_config: dict = None) -> bool:\n",
    "    \"\"\"\n",
    "    특정 스테이지가 처음부터 끝까지 제대로 돌아가는지 테스트\n",
    "    \"\"\"\n",
    "    if test_config is None:\n",
    "        test_config = TEST_MODE_CONFIG\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🧪 TESTING STAGE {stage}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    try:\n",
    "        # 테스트 데이터 생성\n",
    "        train_small, test_small = create_test_data(train, test, test_config)\n",
    "\n",
    "        # FIXED: 테스트용 submission template 생성\n",
    "        test_buildings = sorted(train_small[BUILDING_COL].unique())\n",
    "        test_sub = create_test_submission_template(sub, test_buildings)\n",
    "\n",
    "        # 만약 submission template에 building_number가 없다면 전체 사용\n",
    "        if len(test_sub) == 0:\n",
    "            print(f\"[TEST] Could not filter submission template, using original\")\n",
    "            test_sub = sub.copy()\n",
    "\n",
    "        # 스테이지별 설정\n",
    "        stage_configs = {\n",
    "            1: {\"algorithms\": [\"xgb\"], \"do_search\": False, \"seeds\": test_config[\"seeds\"]},\n",
    "            2: {\"algorithms\": [\"xgb\"], \"do_search\": True, \"seeds\": test_config[\"seeds\"]},\n",
    "            3: {\"algorithms\": [\"xgb\", \"lgbm\"], \"do_search\": False, \"seeds\": test_config[\"seeds\"]},  # MLP 제외 (빠른 테스트)\n",
    "            4: {\"algorithms\": [\"xgb\", \"lgbm\"], \"do_search\": True, \"seeds\": test_config[\"seeds\"]},\n",
    "            5: {\"algorithms\": [\"xgb\", \"lgbm\"], \"do_search\": True, \"seeds\": test_config[\"seeds\"]},\n",
    "            6: {\"algorithms\": [\"xgb\", \"lgbm\"], \"do_search\": True, \"seeds\": test_config[\"seeds\"], \"stacking\": True},\n",
    "        }\n",
    "\n",
    "        config = stage_configs.get(stage, stage_configs[4])\n",
    "\n",
    "        # 테스트용 파라미터 설정\n",
    "        test_param_spaces = get_test_search_spaces()\n",
    "\n",
    "        # 임시로 원래 함수들을 테스트 버전으로 교체\n",
    "        global BUDGETS, ES_ROUNDS_XGB, ES_ROUNDS_LGBM\n",
    "        original_budgets = BUDGETS.copy()\n",
    "        original_es_xgb = ES_ROUNDS_XGB\n",
    "        original_es_lgbm = ES_ROUNDS_LGBM\n",
    "\n",
    "        # 테스트 설정 적용\n",
    "        BUDGETS = {\n",
    "            \"per_building\": {\"random\": 1, \"optuna\": 1, \"bayes\": 1},\n",
    "            \"global\": {\"random\": 1, \"optuna\": 1, \"bayes\": 1},\n",
    "        }\n",
    "        ES_ROUNDS_XGB = test_config[\"es_rounds_xgb\"]\n",
    "        ES_ROUNDS_LGBM = test_config[\"es_rounds_lgbm\"]\n",
    "\n",
    "        # make_pipeline을 테스트 버전으로 임시 교체\n",
    "        import types\n",
    "        global make_pipeline\n",
    "        original_make_pipeline = make_pipeline\n",
    "        make_pipeline = lambda *args, **kwargs: make_test_pipeline(*args, **kwargs, test_config=test_config)\n",
    "\n",
    "        try:\n",
    "            final_sub, preds, oofs = run_stage(\n",
    "                stage=stage,\n",
    "                train=train_small,\n",
    "                test=test_small,\n",
    "                submission_template=test_sub,  # FIXED: 테스트용 submission template 사용\n",
    "                algorithms=config[\"algorithms\"],\n",
    "                do_search=config[\"do_search\"],\n",
    "                seeds=config[\"seeds\"],\n",
    "                cv_type=\"kfold\",\n",
    "                n_splits=test_config[\"cv_splits\"],\n",
    "                param_spaces=test_param_spaces,\n",
    "                stacking=config.get(\"stacking\", False),\n",
    "                gap=0,\n",
    "                inner_cv_type=\"kfold\",\n",
    "                inner_splits=2,\n",
    "                search_metric=\"mae\",  # 간단한 메트릭\n",
    "                force_backend=\"random\",  # 가장 안정적인 백엔드\n",
    "            )\n",
    "\n",
    "            # 결과 검증 - 테스트 모드에 맞게 완화\n",
    "            assert len(final_sub) > 0, \"Empty submission\"\n",
    "            assert \"answer\" in final_sub.columns, \"Missing answer column\"\n",
    "\n",
    "            # FIXED: 테스트 모드에서는 일부 NaN 허용 (다른 건물 데이터 없음)\n",
    "            nan_count = final_sub[\"answer\"].isna().sum()\n",
    "            total_count = len(final_sub)\n",
    "            nan_ratio = nan_count / total_count if total_count > 0 else 0\n",
    "\n",
    "            if nan_ratio > 0.8:  # 80% 이상이 NaN이면 문제\n",
    "                raise AssertionError(f\"Too many NaN values: {nan_count}/{total_count} ({nan_ratio:.1%})\")\n",
    "            elif nan_count > 0:\n",
    "                print(f\"[TEST] ⚠️  {nan_count}/{total_count} ({nan_ratio:.1%}) NaN values - acceptable for test mode\")\n",
    "\n",
    "            print(f\"[TEST] ✅ Stage {stage} completed successfully!\")\n",
    "            print(f\"[TEST] Final submission shape: {final_sub.shape}\")\n",
    "            print(f\"[TEST] Generated {len(preds)} prediction DataFrames\")\n",
    "            print(f\"[TEST] Generated {len(oofs)} OOF DataFrames\")\n",
    "            print(f\"[TEST] NaN ratio: {nan_ratio:.1%}\")\n",
    "\n",
    "            return True\n",
    "\n",
    "        finally:\n",
    "            # 원래 설정 복구\n",
    "            BUDGETS = original_budgets\n",
    "            ES_ROUNDS_XGB = original_es_xgb\n",
    "            ES_ROUNDS_LGBM = original_es_lgbm\n",
    "            make_pipeline = original_make_pipeline\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[TEST] ❌ Stage {stage} FAILED: {str(e)}\")\n",
    "        import traceback\n",
    "        print(f\"[TEST] Error details:\\n{traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "def run_full_pipeline_test(train: pd.DataFrame, test: pd.DataFrame, sub: pd.DataFrame, stages: List[int] = None, test_config: dict = None) -> Dict[int, bool]:\n",
    "    \"\"\"\n",
    "    여러 스테이지에 대해 전체 파이프라인 테스트 실행\n",
    "    \"\"\"\n",
    "    if stages is None:\n",
    "        stages = [1, 2, 3, 4]  # 기본적으로 1-4 스테이지 테스트\n",
    "\n",
    "    if test_config is None:\n",
    "        test_config = TEST_MODE_CONFIG\n",
    "\n",
    "    print(f\"\\n🚀 STARTING FULL PIPELINE TEST\")\n",
    "    print(f\"Stages to test: {stages}\")\n",
    "    print(f\"Test config: {test_config}\")\n",
    "\n",
    "    # 1. 개별 모델 스펙 테스트\n",
    "    print(f\"\\n📋 Testing individual model specs...\")\n",
    "    specs = build_model_specs()\n",
    "    algorithms = [\"xgb\", \"lgbm\"]  # MLP는 시간이 오래 걸려서 제외\n",
    "\n",
    "    train_small, test_small = create_test_data(train, test, test_config)\n",
    "\n",
    "    spec_results = {}\n",
    "    for spec in specs:\n",
    "        for algo in algorithms:\n",
    "            key = f\"{spec.name}_{algo}\"\n",
    "            spec_results[key] = test_single_model_spec(spec, algo, train_small, test_small, test_config)\n",
    "\n",
    "    # 2. HPO 백엔드 테스트\n",
    "    print(f\"\\n🔧 Testing HPO backends...\")\n",
    "    backend_results = test_all_backends(train_small, test_small, test_config)\n",
    "\n",
    "    # 3. 스테이지별 테스트\n",
    "    print(f\"\\n🏃 Testing stages...\")\n",
    "    stage_results = {}\n",
    "    for stage in stages:\n",
    "        stage_results[stage] = run_stage_test(stage, train, test, sub, test_config)\n",
    "\n",
    "    # 4. 결과 요약\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📊 TEST RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    print(f\"\\n🔸 Model Specs:\")\n",
    "    for key, result in spec_results.items():\n",
    "        status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "        print(f\"   {key}: {status}\")\n",
    "\n",
    "    print(f\"\\n🔸 HPO Backends:\")\n",
    "    for backend, result in backend_results.items():\n",
    "        status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "        print(f\"   {backend}: {status}\")\n",
    "\n",
    "    print(f\"\\n🔸 Stages:\")\n",
    "    for stage, result in stage_results.items():\n",
    "        status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "        print(f\"   Stage {stage}: {status}\")\n",
    "\n",
    "    # 전체 성공 여부\n",
    "    all_passed = all(spec_results.values()) and all(backend_results.values()) and all(stage_results.values())\n",
    "    overall_status = \"🎉 ALL TESTS PASSED!\" if all_passed else \"⚠️  SOME TESTS FAILED\"\n",
    "    print(f\"\\n{overall_status}\")\n",
    "\n",
    "    return {\n",
    "        \"specs\": spec_results,\n",
    "        \"backends\": backend_results,\n",
    "        \"stages\": stage_results,\n",
    "        \"overall\": all_passed\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 🧪 QUICK TEST FUNCTIONS - 빠른 개별 테스트용\n",
    "# ============================================================\n",
    "\n",
    "def quick_test_single_stage(stage: int, train_path: str, test_path: str, sub_path: str = None, binfo_path: str = None):\n",
    "    \"\"\"\n",
    "    특정 스테이지 하나만 빠르게 테스트 (1-2분 소요)\n",
    "\n",
    "    Usage:\n",
    "        quick_test_single_stage(4, \"../data/raw/train.csv\", \"../data/raw/test.csv\")\n",
    "        # submission 파일이 없어도 자동 생성됨\n",
    "    \"\"\"\n",
    "    print(f\"🧪 Quick testing Stage {stage}...\")\n",
    "\n",
    "    train, test, binfo, sub = load_data(train_path, test_path, binfo_path, sub_path)\n",
    "    if binfo is not None:\n",
    "        train = merge_static(train, binfo)\n",
    "        test = merge_static(test, binfo)\n",
    "\n",
    "    result = run_stage_test(stage, train, test, sub, TEST_MODE_CONFIG)\n",
    "\n",
    "    if result:\n",
    "        print(f\"✅ Stage {stage} test PASSED!\")\n",
    "    else:\n",
    "        print(f\"❌ Stage {stage} test FAILED!\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def quick_test_model_specs(train_path: str, test_path: str, binfo_path: str = None):\n",
    "    \"\"\"\n",
    "    모든 모델 스펙이 학습되는지 빠르게 테스트 (2-3분 소요)\n",
    "\n",
    "    Usage:\n",
    "        quick_test_model_specs(\"../data/raw/train.csv\", \"../data/raw/test.csv\")\n",
    "    \"\"\"\n",
    "    print(f\"🧪 Quick testing all model specs...\")\n",
    "\n",
    "    train, test, binfo, _ = load_data(train_path, test_path, binfo_path, None)\n",
    "    if binfo is not None:\n",
    "        train = merge_static(train, binfo)\n",
    "        test = merge_static(test, binfo)\n",
    "\n",
    "    train_small, test_small = create_test_data(train, test, TEST_MODE_CONFIG)\n",
    "\n",
    "    specs = build_model_specs()\n",
    "    algorithms = [\"xgb\", \"lgbm\"]\n",
    "\n",
    "    all_passed = True\n",
    "    for spec in specs:\n",
    "        for algo in algorithms:\n",
    "            result = test_single_model_spec(spec, algo, train_small, test_small, TEST_MODE_CONFIG)\n",
    "            if not result:\n",
    "                all_passed = False\n",
    "\n",
    "    if all_passed:\n",
    "        print(\"✅ All model specs test PASSED!\")\n",
    "    else:\n",
    "        print(\"❌ Some model specs test FAILED!\")\n",
    "\n",
    "    return all_passed\n",
    "\n",
    "# ============================================================\n",
    "# 10) Example main\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # 📁 파일 경로 설정 - 실제 경로에 맞게 수정하세요\n",
    "    TRAIN_PATH = \"../data/raw/train.csv\"              # 필수\n",
    "    TEST_PATH = \"../data/raw/test.csv\"                # 필수\n",
    "    BINFO_PATH = \"../data/raw/building_info.csv\"      # 선택적 (None 가능)\n",
    "    SUB_PATH = \"../data/raw/sample_submission.csv\"    # 선택적 (None 가능)\n",
    "\n",
    "    # Load & merge\n",
    "    _, _, binfo, sub = load_data(TRAIN_PATH, TEST_PATH, BINFO_PATH, SUB_PATH)\n",
    "\n",
    "    # building info merge (있는 경우만)\n",
    "    if binfo is not None:\n",
    "        train = train\n",
    "        test = test\n",
    "\n",
    "    print(f\"[data] Final data shapes - train: {train.shape}, test: {test.shape}, sub: {sub.shape}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # 🧪 TEST MODE vs PRODUCTION MODE 선택\n",
    "    # ============================================================\n",
    "\n",
    "    TEST_MODE = False  # 🔧 테스트 모드 ON/OFF\n",
    "\n",
    "    if TEST_MODE:\n",
    "        print(\"🧪 RUNNING IN TEST MODE\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # 빠른 테스트 실행 (3-5분 소요)\n",
    "        test_results = run_full_pipeline_test(\n",
    "            train=train,\n",
    "            test=test,\n",
    "            sub=sub,\n",
    "            stages=[1, 2, 4],  # 테스트할 스테이지 선택\n",
    "            test_config=TEST_MODE_CONFIG\n",
    "        )\n",
    "\n",
    "        if test_results[\"overall\"]:\n",
    "            print(\"\\n✅ 모든 테스트가 성공했습니다! 실제 학습을 진행해도 됩니다.\")\n",
    "            print(\"💡 TEST_MODE = False로 변경하고 다시 실행하세요.\")\n",
    "        else:\n",
    "            print(\"\\n❌ 일부 테스트가 실패했습니다. 코드를 확인해주세요.\")\n",
    "\n",
    "    else:\n",
    "        print(\"🚀 RUNNING IN PRODUCTION MODE\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # CV / search config\n",
    "        PARAM_SPACES = default_search_spaces()\n",
    "        cv_type = \"kfold\"   # or 'timesplit'\n",
    "        n_splits = 5\n",
    "        gap = 0\n",
    "        inner_cv_type = cv_type\n",
    "        inner_splits = 3\n",
    "        search_metric = DEFAULT_SEARCH_METRIC  # 'weighted_mse'|'smape'|'mae'|'mse'\n",
    "\n",
    "        # Choose stage here:\n",
    "        final_sub, preds, oofs = stage_4_triple_search_mean(train, test, sub,\n",
    "            cv_type=cv_type, n_splits=n_splits, param_spaces=PARAM_SPACES,\n",
    "            gap=gap, inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "            search_metric=search_metric, force_backend=None)\n",
    "\n",
    "        # Save under runs/{date_tag}/\n",
    "        run_dir = make_run_dir(tag=\"stage4_fixed\")\n",
    "        cfg = dict(\n",
    "            stage=\"4\",\n",
    "            cv_type=cv_type, n_splits=n_splits, gap=gap,\n",
    "            inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "            search_metric=search_metric,\n",
    "            backend=FORCE_BACKEND or HPO_BACKEND_FOR_STAGE[4],\n",
    "            budgets=BUDGETS,\n",
    "        )\n",
    "        save_artifacts(run_dir, final_sub, \"stage4\", preds, oofs, cfg)\n",
    "\n",
    "        print(f\"\\n🎉 Production run completed! Results saved to: {run_dir}\")"
   ],
   "id": "7d47b0dbe7715815",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Final data shapes - train: (204000, 44), test: (16800, 41), sub: (16800, 2)\n",
      "🧪 RUNNING IN TEST MODE\n",
      "==================================================\n",
      "\n",
      "🚀 STARTING FULL PIPELINE TEST\n",
      "Stages to test: [1, 2, 4]\n",
      "Test config: {'sample_frac': 0.1, 'max_buildings': 3, 'cv_splits': 2, 'hpo_trials': 1, 'seeds': [42], 'es_rounds_xgb': 10, 'es_rounds_lgbm': 10, 'mlp_max_iter': 50, 'mlp_n_no_change': 5}\n",
      "\n",
      "📋 Testing individual model specs...\n",
      "[TEST] Original data: train=6120, test=504\n",
      "[TEST] Sampled data: train=612, test=48\n",
      "[TEST] Buildings: [np.float32(1.0), np.float32(2.0), np.float32(3.0)]\n",
      "\n",
      "[TEST] Testing Spec A (per_building) with xgb...\n",
      "[TEST] ✅ Spec A-xgb: Train SMAPE = 1.21%\n",
      "\n",
      "[TEST] Testing Spec A (per_building) with lgbm...\n",
      "[TEST] ✅ Spec A-lgbm: Train SMAPE = 2.57%\n",
      "\n",
      "[TEST] Testing Spec B (per_building) with xgb...\n",
      "[TEST] ✅ Spec B-xgb: Train SMAPE = 1.01%\n",
      "\n",
      "[TEST] Testing Spec B (per_building) with lgbm...\n",
      "[TEST] ✅ Spec B-lgbm: Train SMAPE = 2.53%\n",
      "\n",
      "[TEST] Testing Spec C (global) with xgb...\n",
      "[TEST] ✅ Spec C-xgb: Train SMAPE = 2.49%\n",
      "\n",
      "[TEST] Testing Spec C (global) with lgbm...\n",
      "[TEST] ✅ Spec C-lgbm: Train SMAPE = 2.44%\n",
      "\n",
      "[TEST] Testing Spec D (global) with xgb...\n",
      "[TEST] ✅ Spec D-xgb: Train SMAPE = 2.37%\n",
      "\n",
      "[TEST] Testing Spec D (global) with lgbm...\n",
      "[TEST] ✅ Spec D-lgbm: Train SMAPE = 2.45%\n",
      "\n",
      "🔧 Testing HPO backends...\n",
      "\n",
      "[TEST] Testing HPO backends...\n",
      "[TEST] Testing random backend...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 01:33:38,833] A new study created in memory with name: no-name-a357415d-a220-4d4a-8f82-150c592d4d9e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] ✅ random backend works\n",
      "[TEST] Testing optuna backend...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 01:33:39,027] Trial 0 finished with value: -356.5613787881689 and parameters: {'meta__alpha': 3.0, 'est__n_estimators': 69, 'est__learning_rate': 0.1, 'est__max_depth': 4}. Best is trial 0 with value: -356.5613787881689.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] ✅ optuna backend works\n",
      "[TEST] Testing bayes backend...\n",
      "[bayes] n_iter=1 < 10, falling back to random search\n",
      "[TEST] ✅ bayes backend works\n",
      "\n",
      "🏃 Testing stages...\n",
      "\n",
      "============================================================\n",
      "🧪 TESTING STAGE 1\n",
      "============================================================\n",
      "[TEST] Original data: train=6120, test=504\n",
      "[TEST] Sampled data: train=612, test=48\n",
      "[TEST] Buildings: [np.float32(1.0), np.float32(2.0), np.float32(3.0)]\n",
      "[test] Original submission template: 16800 rows\n",
      "[test] Test submission template: 16800 rows\n",
      "[test] Test buildings: [np.float32(1.0), np.float32(2.0), np.float32(3.0)]\n",
      "\n",
      "=== Training Spec=A(per_building) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=6.768% | mae=367.560734\n",
      "[val] fold 2/2  SMAPE=6.868% | mae=345.029360\n",
      "[val] mean SMAPE: 6.818% (±0.050)\n",
      "[final-fit][Spec A | Algo xgb | B1.0] Final Train SMAPE=1.2072% | R2=0.9959 | Params={}\n",
      "[val] fold 1/2  SMAPE=6.019% | mae=104.540929\n",
      "[val] fold 2/2  SMAPE=5.910% | mae=108.937760\n",
      "[val] mean SMAPE: 5.964% (±0.055)\n",
      "[final-fit][Spec A | Algo xgb | B2.0] Final Train SMAPE=0.8562% | R2=0.9980 | Params={}\n",
      "[val] fold 1/2  SMAPE=2.754% | mae=527.433426\n",
      "[val] fold 2/2  SMAPE=2.698% | mae=492.080564\n",
      "[val] mean SMAPE: 2.726% (±0.028)\n",
      "[final-fit][Spec A | Algo xgb | B3.0] Final Train SMAPE=0.3993% | R2=0.9995 | Params={}\n",
      "\n",
      "=== Training Spec=B(per_building) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=6.506% | mae=345.032586\n",
      "[val] fold 2/2  SMAPE=6.701% | mae=338.323008\n",
      "[val] mean SMAPE: 6.603% (±0.098)\n",
      "[final-fit][Spec B | Algo xgb | B1.0] Final Train SMAPE=1.0101% | R2=0.9970 | Params={}\n",
      "[val] fold 1/2  SMAPE=4.856% | mae=84.836064\n",
      "[val] fold 2/2  SMAPE=5.340% | mae=97.785252\n",
      "[val] mean SMAPE: 5.098% (±0.242)\n",
      "[final-fit][Spec B | Algo xgb | B2.0] Final Train SMAPE=0.8193% | R2=0.9982 | Params={}\n",
      "[val] fold 1/2  SMAPE=2.663% | mae=509.093509\n",
      "[val] fold 2/2  SMAPE=2.229% | mae=399.739332\n",
      "[val] mean SMAPE: 2.446% (±0.217)\n",
      "[final-fit][Spec B | Algo xgb | B3.0] Final Train SMAPE=0.3833% | R2=0.9995 | Params={}\n",
      "\n",
      "=== Training Spec=C(global) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=5.395% | mae=334.102300\n",
      "[val] fold 2/2  SMAPE=5.309% | mae=302.823986\n",
      "[val] mean SMAPE: 5.352% (±0.043)\n",
      "[final-fit][Spec C | Algo xgb | GLOBAL] Final Train SMAPE=2.4934% | R2=0.9996 | Params={}\n",
      "\n",
      "=== Training Spec=D(global) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=5.056% | mae=308.960944\n",
      "[val] fold 2/2  SMAPE=5.174% | mae=292.754413\n",
      "[val] mean SMAPE: 5.115% (±0.059)\n",
      "[final-fit][Spec D | Algo xgb | GLOBAL] Final Train SMAPE=2.3682% | R2=0.9996 | Params={}\n",
      "[ensemble] Merging 4 prediction DataFrames\n",
      "[ensemble] DataFrame 0: shape=(48, 3), pred_cols=['pred_A_xgb']\n",
      "[ensemble] DataFrame 1: shape=(48, 3), pred_cols=['pred_B_xgb']\n",
      "[ensemble] DataFrame 2: shape=(48, 3), pred_cols=['pred_C_xgb']\n",
      "[ensemble] DataFrame 3: shape=(48, 3), pred_cols=['pred_D_xgb']\n",
      "[ensemble] Merged shape: (48, 6), pred_cols: ['pred_A_xgb', 'pred_B_xgb', 'pred_C_xgb', 'pred_D_xgb']\n",
      "[ensemble] Final result shape: (48, 3)\n",
      "[stage] Final ensemble shape: (48, 3)\n",
      "[stage] Final ensemble columns: ['num_date_time', 'building_number', 'answer']\n",
      "[stage] Submission template shape: (16800, 2)\n",
      "[stage] Submission template columns: ['num_date_time', 'answer']\n",
      "[stage] INFO: 16752/16800 rows have NaN in answer column\n",
      "[stage] Missing TIME_COL values in predictions: 16752 values\n",
      "[stage] This is normal in test mode with limited buildings\n",
      "[stage] High NaN ratio (99.7%) - keeping NaN for test mode\n",
      "[stage] Final output shape: (16800, 2)\n",
      "[stage] Final NaN count: 16752\n",
      "[TEST] ❌ Stage 1 FAILED: Too many NaN values: 16752/16800 (99.7%)\n",
      "[TEST] Error details:\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/3n/1gwr35l145vgz6xvjbtlj61w0000gn/T/ipykernel_99463/3613458981.py\", line 1339, in run_stage_test\n",
      "    raise AssertionError(f\"Too many NaN values: {nan_count}/{total_count} ({nan_ratio:.1%})\")\n",
      "AssertionError: Too many NaN values: 16752/16800 (99.7%)\n",
      "\n",
      "\n",
      "============================================================\n",
      "🧪 TESTING STAGE 2\n",
      "============================================================\n",
      "[TEST] Original data: train=6120, test=504\n",
      "[TEST] Sampled data: train=612, test=48\n",
      "[TEST] Buildings: [np.float32(1.0), np.float32(2.0), np.float32(3.0)]\n",
      "[test] Original submission template: 16800 rows\n",
      "[test] Test submission template: 16800 rows\n",
      "[test] Test buildings: [np.float32(1.0), np.float32(2.0), np.float32(3.0)]\n",
      "\n",
      "=== Training Spec=A(per_building) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=6.836% | mae=371.561928\n",
      "[val] fold 2/2  SMAPE=7.119% | mae=359.598552\n",
      "[val] mean SMAPE: 6.978% (±0.141)\n",
      "[final-fit][Spec A | Algo xgb | B1.0] Final Train SMAPE=2.2373% | R2=0.9856 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[val] fold 1/2  SMAPE=6.293% | mae=108.726735\n",
      "[val] fold 2/2  SMAPE=6.008% | mae=111.074937\n",
      "[val] mean SMAPE: 6.151% (±0.142)\n",
      "[final-fit][Spec A | Algo xgb | B2.0] Final Train SMAPE=1.7611% | R2=0.9916 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[val] fold 1/2  SMAPE=2.832% | mae=545.262547\n",
      "[val] fold 2/2  SMAPE=2.701% | mae=493.283189\n",
      "[val] mean SMAPE: 2.767% (±0.066)\n",
      "[final-fit][Spec A | Algo xgb | B3.0] Final Train SMAPE=0.7632% | R2=0.9981 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "\n",
      "=== Training Spec=B(per_building) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=6.629% | mae=351.641670\n",
      "[val] fold 2/2  SMAPE=6.967% | mae=352.312221\n",
      "[val] mean SMAPE: 6.798% (±0.169)\n",
      "[final-fit][Spec B | Algo xgb | B1.0] Final Train SMAPE=1.9233% | R2=0.9892 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[val] fold 1/2  SMAPE=5.114% | mae=88.700500\n",
      "[val] fold 2/2  SMAPE=5.475% | mae=99.911546\n",
      "[val] mean SMAPE: 5.294% (±0.181)\n",
      "[final-fit][Spec B | Algo xgb | B2.0] Final Train SMAPE=1.5939% | R2=0.9931 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[val] fold 1/2  SMAPE=2.737% | mae=527.747211\n",
      "[val] fold 2/2  SMAPE=2.267% | mae=407.365444\n",
      "[val] mean SMAPE: 2.502% (±0.235)\n",
      "[final-fit][Spec B | Algo xgb | B3.0] Final Train SMAPE=0.7668% | R2=0.9981 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "\n",
      "=== Training Spec=C(global) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=5.511% | mae=341.054981\n",
      "[val] fold 2/2  SMAPE=5.901% | mae=326.606390\n",
      "[val] mean SMAPE: 5.706% (±0.195)\n",
      "[final-fit][Spec C | Algo xgb | GLOBAL] Final Train SMAPE=3.4950% | R2=0.9991 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "\n",
      "=== Training Spec=D(global) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=5.164% | mae=318.540231\n",
      "[val] fold 2/2  SMAPE=5.845% | mae=319.683113\n",
      "[val] mean SMAPE: 5.505% (±0.341)\n",
      "[final-fit][Spec D | Algo xgb | GLOBAL] Final Train SMAPE=3.4327% | R2=0.9991 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[ensemble] Merging 4 prediction DataFrames\n",
      "[ensemble] DataFrame 0: shape=(48, 3), pred_cols=['pred_A_xgb']\n",
      "[ensemble] DataFrame 1: shape=(48, 3), pred_cols=['pred_B_xgb']\n",
      "[ensemble] DataFrame 2: shape=(48, 3), pred_cols=['pred_C_xgb']\n",
      "[ensemble] DataFrame 3: shape=(48, 3), pred_cols=['pred_D_xgb']\n",
      "[ensemble] Merged shape: (48, 6), pred_cols: ['pred_A_xgb', 'pred_B_xgb', 'pred_C_xgb', 'pred_D_xgb']\n",
      "[ensemble] Final result shape: (48, 3)\n",
      "[stage] Final ensemble shape: (48, 3)\n",
      "[stage] Final ensemble columns: ['num_date_time', 'building_number', 'answer']\n",
      "[stage] Submission template shape: (16800, 2)\n",
      "[stage] Submission template columns: ['num_date_time', 'answer']\n",
      "[stage] INFO: 16752/16800 rows have NaN in answer column\n",
      "[stage] Missing TIME_COL values in predictions: 16752 values\n",
      "[stage] This is normal in test mode with limited buildings\n",
      "[stage] High NaN ratio (99.7%) - keeping NaN for test mode\n",
      "[stage] Final output shape: (16800, 2)\n",
      "[stage] Final NaN count: 16752\n",
      "[TEST] ❌ Stage 2 FAILED: Too many NaN values: 16752/16800 (99.7%)\n",
      "[TEST] Error details:\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/3n/1gwr35l145vgz6xvjbtlj61w0000gn/T/ipykernel_99463/3613458981.py\", line 1339, in run_stage_test\n",
      "    raise AssertionError(f\"Too many NaN values: {nan_count}/{total_count} ({nan_ratio:.1%})\")\n",
      "AssertionError: Too many NaN values: 16752/16800 (99.7%)\n",
      "\n",
      "\n",
      "============================================================\n",
      "🧪 TESTING STAGE 4\n",
      "============================================================\n",
      "[TEST] Original data: train=6120, test=504\n",
      "[TEST] Sampled data: train=612, test=48\n",
      "[TEST] Buildings: [np.float32(1.0), np.float32(2.0), np.float32(3.0)]\n",
      "[test] Original submission template: 16800 rows\n",
      "[test] Test submission template: 16800 rows\n",
      "[test] Test buildings: [np.float32(1.0), np.float32(2.0), np.float32(3.0)]\n",
      "\n",
      "=== Training Spec=A(per_building) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=6.836% | mae=371.561928\n",
      "[val] fold 2/2  SMAPE=7.119% | mae=359.598552\n",
      "[val] mean SMAPE: 6.978% (±0.141)\n",
      "[final-fit][Spec A | Algo xgb | B1.0] Final Train SMAPE=2.2373% | R2=0.9856 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[val] fold 1/2  SMAPE=6.293% | mae=108.726735\n",
      "[val] fold 2/2  SMAPE=6.008% | mae=111.074937\n",
      "[val] mean SMAPE: 6.151% (±0.142)\n",
      "[final-fit][Spec A | Algo xgb | B2.0] Final Train SMAPE=1.7611% | R2=0.9916 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[val] fold 1/2  SMAPE=2.832% | mae=545.262547\n",
      "[val] fold 2/2  SMAPE=2.701% | mae=493.283189\n",
      "[val] mean SMAPE: 2.767% (±0.066)\n",
      "[final-fit][Spec A | Algo xgb | B3.0] Final Train SMAPE=0.7632% | R2=0.9981 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "\n",
      "=== Training Spec=A(per_building) Algo=lgbm | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=7.350% | mae=393.753373\n",
      "[val] fold 2/2  SMAPE=7.888% | mae=397.668887\n",
      "[val] mean SMAPE: 7.619% (±0.269)\n",
      "[final-fit][Spec A | Algo lgbm | B1.0] Final Train SMAPE=3.7209% | R2=0.9604 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__num_leaves': 16}\n",
      "[val] fold 1/2  SMAPE=6.745% | mae=120.308012\n",
      "[val] fold 2/2  SMAPE=6.431% | mae=123.167190\n",
      "[val] mean SMAPE: 6.588% (±0.157)\n",
      "[final-fit][Spec A | Algo lgbm | B2.0] Final Train SMAPE=3.3710% | R2=0.9462 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__num_leaves': 16}\n",
      "[val] fold 1/2  SMAPE=3.211% | mae=626.906627\n",
      "[val] fold 2/2  SMAPE=2.930% | mae=525.112605\n",
      "[val] mean SMAPE: 3.071% (±0.140)\n",
      "[final-fit][Spec A | Algo lgbm | B3.0] Final Train SMAPE=1.4783% | R2=0.9933 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__num_leaves': 16}\n",
      "\n",
      "=== Training Spec=B(per_building) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=6.629% | mae=351.641670\n",
      "[val] fold 2/2  SMAPE=6.967% | mae=352.312221\n",
      "[val] mean SMAPE: 6.798% (±0.169)\n",
      "[final-fit][Spec B | Algo xgb | B1.0] Final Train SMAPE=1.9233% | R2=0.9892 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[val] fold 1/2  SMAPE=5.114% | mae=88.700500\n",
      "[val] fold 2/2  SMAPE=5.475% | mae=99.911546\n",
      "[val] mean SMAPE: 5.294% (±0.181)\n",
      "[final-fit][Spec B | Algo xgb | B2.0] Final Train SMAPE=1.5939% | R2=0.9931 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "[val] fold 1/2  SMAPE=2.737% | mae=527.747211\n",
      "[val] fold 2/2  SMAPE=2.267% | mae=407.365444\n",
      "[val] mean SMAPE: 2.502% (±0.235)\n",
      "[final-fit][Spec B | Algo xgb | B3.0] Final Train SMAPE=0.7668% | R2=0.9981 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "\n",
      "=== Training Spec=B(per_building) Algo=lgbm | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=6.457% | mae=345.444402\n",
      "[val] fold 2/2  SMAPE=7.374% | mae=371.732638\n",
      "[val] mean SMAPE: 6.915% (±0.459)\n",
      "[final-fit][Spec B | Algo lgbm | B1.0] Final Train SMAPE=3.6069% | R2=0.9616 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__num_leaves': 16}\n",
      "[val] fold 1/2  SMAPE=7.007% | mae=124.720506\n",
      "[val] fold 2/2  SMAPE=6.745% | mae=128.822055\n",
      "[val] mean SMAPE: 6.876% (±0.131)\n",
      "[final-fit][Spec B | Algo lgbm | B2.0] Final Train SMAPE=3.2835% | R2=0.9469 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__num_leaves': 16}\n",
      "[val] fold 1/2  SMAPE=3.383% | mae=661.807134\n",
      "[val] fold 2/2  SMAPE=2.765% | mae=501.378224\n",
      "[val] mean SMAPE: 3.074% (±0.309)\n",
      "[final-fit][Spec B | Algo lgbm | B3.0] Final Train SMAPE=1.3977% | R2=0.9940 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__num_leaves': 16}\n",
      "\n",
      "=== Training Spec=C(global) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=5.511% | mae=341.054981\n",
      "[val] fold 2/2  SMAPE=5.901% | mae=326.606390\n",
      "[val] mean SMAPE: 5.706% (±0.195)\n",
      "[final-fit][Spec C | Algo xgb | GLOBAL] Final Train SMAPE=3.4950% | R2=0.9991 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "\n",
      "=== Training Spec=C(global) Algo=lgbm | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=4.974% | mae=325.068242\n",
      "[val] fold 2/2  SMAPE=5.713% | mae=343.687901\n",
      "[val] mean SMAPE: 5.343% (±0.370)\n",
      "[final-fit][Spec C | Algo lgbm | GLOBAL] Final Train SMAPE=3.4079% | R2=0.9990 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__num_leaves': 16}\n",
      "\n",
      "=== Training Spec=D(global) Algo=xgb | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=5.164% | mae=318.540231\n",
      "[val] fold 2/2  SMAPE=5.845% | mae=319.683113\n",
      "[val] mean SMAPE: 5.505% (±0.341)\n",
      "[final-fit][Spec D | Algo xgb | GLOBAL] Final Train SMAPE=3.4327% | R2=0.9991 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__max_depth': 4}\n",
      "\n",
      "=== Training Spec=D(global) Algo=lgbm | backend=random trials=1 ===\n",
      "[val] fold 1/2  SMAPE=4.924% | mae=303.367023\n",
      "[val] fold 2/2  SMAPE=5.565% | mae=321.225656\n",
      "[val] mean SMAPE: 5.244% (±0.321)\n",
      "[final-fit][Spec D | Algo lgbm | GLOBAL] Final Train SMAPE=3.2964% | R2=0.9991 | Params={'meta__alpha': 3.0, 'est__n_estimators': 50, 'est__learning_rate': 0.1, 'est__num_leaves': 16}\n",
      "[ensemble] Merging 8 prediction DataFrames\n",
      "[ensemble] DataFrame 0: shape=(48, 3), pred_cols=['pred_A_xgb']\n",
      "[ensemble] DataFrame 1: shape=(48, 3), pred_cols=['pred_A_lgbm']\n",
      "[ensemble] DataFrame 2: shape=(48, 3), pred_cols=['pred_B_xgb']\n",
      "[ensemble] DataFrame 3: shape=(48, 3), pred_cols=['pred_B_lgbm']\n",
      "[ensemble] DataFrame 4: shape=(48, 3), pred_cols=['pred_C_xgb']\n",
      "[ensemble] DataFrame 5: shape=(48, 3), pred_cols=['pred_C_lgbm']\n",
      "[ensemble] DataFrame 6: shape=(48, 3), pred_cols=['pred_D_xgb']\n",
      "[ensemble] DataFrame 7: shape=(48, 3), pred_cols=['pred_D_lgbm']\n",
      "[ensemble] Merged shape: (48, 10), pred_cols: ['pred_A_xgb', 'pred_A_lgbm', 'pred_B_xgb', 'pred_B_lgbm', 'pred_C_xgb', 'pred_C_lgbm', 'pred_D_xgb', 'pred_D_lgbm']\n",
      "[ensemble] Final result shape: (48, 3)\n",
      "[stage] Final ensemble shape: (48, 3)\n",
      "[stage] Final ensemble columns: ['num_date_time', 'building_number', 'answer']\n",
      "[stage] Submission template shape: (16800, 2)\n",
      "[stage] Submission template columns: ['num_date_time', 'answer']\n",
      "[stage] INFO: 16752/16800 rows have NaN in answer column\n",
      "[stage] Missing TIME_COL values in predictions: 16752 values\n",
      "[stage] This is normal in test mode with limited buildings\n",
      "[stage] High NaN ratio (99.7%) - keeping NaN for test mode\n",
      "[stage] Final output shape: (16800, 2)\n",
      "[stage] Final NaN count: 16752\n",
      "[TEST] ❌ Stage 4 FAILED: Too many NaN values: 16752/16800 (99.7%)\n",
      "[TEST] Error details:\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/3n/1gwr35l145vgz6xvjbtlj61w0000gn/T/ipykernel_99463/3613458981.py\", line 1339, in run_stage_test\n",
      "    raise AssertionError(f\"Too many NaN values: {nan_count}/{total_count} ({nan_ratio:.1%})\")\n",
      "AssertionError: Too many NaN values: 16752/16800 (99.7%)\n",
      "\n",
      "\n",
      "============================================================\n",
      "📊 TEST RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "🔸 Model Specs:\n",
      "   A_xgb: ✅ PASS\n",
      "   A_lgbm: ✅ PASS\n",
      "   B_xgb: ✅ PASS\n",
      "   B_lgbm: ✅ PASS\n",
      "   C_xgb: ✅ PASS\n",
      "   C_lgbm: ✅ PASS\n",
      "   D_xgb: ✅ PASS\n",
      "   D_lgbm: ✅ PASS\n",
      "\n",
      "🔸 HPO Backends:\n",
      "   random: ✅ PASS\n",
      "   optuna: ✅ PASS\n",
      "   bayes: ✅ PASS\n",
      "\n",
      "🔸 Stages:\n",
      "   Stage 1: ❌ FAIL\n",
      "   Stage 2: ❌ FAIL\n",
      "   Stage 4: ❌ FAIL\n",
      "\n",
      "⚠️  SOME TESTS FAILED\n",
      "\n",
      "❌ 일부 테스트가 실패했습니다. 코드를 확인해주세요.\n"
     ]
    }
   ],
   "execution_count": 64
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
