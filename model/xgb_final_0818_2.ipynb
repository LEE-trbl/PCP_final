{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip uninstall -y pandas seaborn numpy matplotlib scikit-learn xgboost --quiet",
   "id": "12edeb792f2a9da8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install pandas seaborn numpy matplotlib scikit-learn xgboost scikit-optimize lightgbm optuna --quiet",
   "id": "583abbb57525ff0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "print (f\"numpy version: {np.__version__}\")"
   ],
   "id": "3e6d80da8b3156fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip3 freeze > requirements.txt",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport sklearn\nimport xgboost\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nimport random as rn\nRANDOM_SEED = 2025\nnp.random.seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\nfrom datetime import datetime\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\n\n# ==========================================================\n# 상수 정의\n# ==========================================================\nBUILDING_COL = \"building_number\"\nTIME_COL = \"date_time\"\nTARGET_COL = \"power_consumption\"\n\n# ============================================================\n# 1) Feature Sets (corrected to match feature engineering)\n# ============================================================\nTYPE1_BASE = [\n    \"temperature\", \"humidity\", \"windspeed\",\n    \"day_of_week\", \"month\", \"week\", \"is_holiday\",\n    \"sin_hour\", \"cos_hour\",\n    \"day_hour_mean\", \"day_hour_std\",\n    \"holiday_hour_mean\", \"holiday_std\",\n    \"THI\", \"WCT\", \"CDH\", \"is_peak_season\"\n]\n\nTYPE1_DETAIL = [\n    \"summer_sin\", \"summer_cos\", \"day_max_temperature\", \"day_min_temperature\",\n    \"day_mean_temperature\", \"day_diff_temperature\"\n]\n\nTYPE2_BASE = [\n    \"temperature\", \"humidity\", \"windspeed\",\n    \"day_of_week\", \"month\", \"week\", \"is_holiday\",\n    \"sin_hour\", \"cos_hour\",\n    \"day_hour_mean\", \"day_hour_std\",\n    \"holiday_hour_mean\", \"holiday_std\",\n    \"THI\", \"WCT\", \"CDH\", \"is_peak_season\",\n    \"total_area\", \"cooling_area\",\n    \"building_number\", \"building_type\"\n]\n\nTYPE2_DETAIL = [\n    \"summer_sin\", \"summer_cos\", \"day_max_temperature\", \"day_min_temperature\",\n    \"day_mean_temperature\", \"day_diff_temperature\", \"pv_temp\", \"ess_pcs_std\"\n]",
   "id": "2e94096b17ece852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Load and Preprocessing",
   "id": "bd7f30f251e8936f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('../data/raw/train.csv')\n",
    "test = pd.read_csv('../data/raw/test.csv')\n",
    "building_info = pd.read_csv('../data/raw/building_info.csv')"
   ],
   "id": "613d6e4be9f2a0ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train = train.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(°C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "})\n",
    "# train.drop('num_date_time', axis = 1, inplace=True)\n",
    "\n",
    "test = test.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '일시': 'date_time',\n",
    "    '기온(°C)': 'temperature',\n",
    "    '강수량(mm)': 'rainfall',\n",
    "    '풍속(m/s)': 'windspeed',\n",
    "    '습도(%)': 'humidity',\n",
    "    '일조(hr)': 'sunshine',\n",
    "    '일사(MJ/m2)': 'solar_radiation',\n",
    "    '전력소비량(kWh)': 'power_consumption'\n",
    "})\n",
    "# test.drop('num_date_time', axis = 1, inplace=True)\n",
    "\n",
    "building_info = building_info.rename(columns={\n",
    "    '건물번호': 'building_number',\n",
    "    '건물유형': 'building_type',\n",
    "    '연면적(m2)': 'total_area',\n",
    "    '냉방면적(m2)': 'cooling_area',\n",
    "    '태양광용량(kW)': 'solar_power_capacity',\n",
    "    'ESS저장용량(kWh)': 'ess_capacity',\n",
    "    'PCS용량(kW)': 'pcs_capacity'\n",
    "})\n",
    "\n",
    "translation_dict = {\n",
    "    '건물기타': 'Other Buildings',\n",
    "    '공공': 'Public',\n",
    "    '학교': 'University',\n",
    "    '백화점': 'Department Store',\n",
    "    '병원': 'Hospital',\n",
    "    '상용': 'Commercial',\n",
    "    '아파트': 'Apartment',\n",
    "    '연구소': 'Research Institute',\n",
    "    'IDC(전화국)': 'IDC',\n",
    "    '호텔': 'Hotel'\n",
    "}\n",
    "\n",
    "building_info['building_type'] = building_info['building_type'].replace(translation_dict)\n",
    "\n",
    "building_info['solar_power_utility_binary'] = np.where(building_info.solar_power_capacity !='-',1,0)\n",
    "building_info['ess_utility_binary'] = np.where(building_info.ess_capacity !='-',1,0)\n",
    "\n",
    "building_info['solar_power_capacity'] = building_info['solar_power_capacity'].replace('-', '0').astype(float)\n",
    "building_info['ess_capacity'] = building_info['ess_capacity'].replace('-', '0').astype(float)\n",
    "\n",
    "train = pd.merge(train, building_info, on='building_number', how='left')\n",
    "test = pd.merge(test, building_info, on='building_number', how='left')"
   ],
   "id": "489a63aa46c3731c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ========================\n",
    "# 이상치 마킹\n",
    "# ========================"
   ],
   "id": "fc6de531817f8b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# outlier_detect 컬럼 초기화 (0: 정상, 1: 이상치)\n",
    "train['outlier_detect'] = 0\n",
    "test['outlier_detect'] = 0\n",
    "\n",
    "# power_consumption이 0인 경우를 이상치로 마킹 (앞뒤 1포인트 포함)\n",
    "zero_outlier_indices = train.index[train['power_consumption'] == 0].tolist()\n",
    "\n",
    "# 앞뒤 1포인트도 포함하여 확장된 이상치 인덱스 생성\n",
    "expanded_outlier_indices = set()\n",
    "for idx in zero_outlier_indices:\n",
    "    # 현재 인덱스와 앞뒤 1포인트 추가\n",
    "    for offset in [-1, 0, 1]:\n",
    "        new_idx = idx + offset\n",
    "        if new_idx >= 0 and new_idx < len(train):  # 인덱스 범위 체크\n",
    "            expanded_outlier_indices.add(new_idx)\n",
    "\n",
    "# 확장된 인덱스들을 이상치로 마킹\n",
    "train.loc[list(expanded_outlier_indices), 'outlier_detect'] = 1\n",
    "\n",
    "# 이상치 개수 확인\n",
    "outlier_count = (train['outlier_detect'] == 1).sum()\n",
    "total_count = len(train)\n",
    "print(f\"총 데이터 개수: {total_count}\")\n",
    "print(f\"power_consumption이 0인 원본 이상치 개수: {len(zero_outlier_indices)}\")\n",
    "print(f\"앞뒤 1포인트 포함 확장된 이상치 개수: {outlier_count}\")\n",
    "print(f\"확장된 이상치 비율: {outlier_count/total_count*100:.2f}%\")"
   ],
   "id": "8038ed907596a509",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# IQR 방식을 이용한 이상치 탐지 (앞뒤 1포인트 포함)\n",
    "def detect_outliers_iqr_expanded(df, column, building_col='building_number'):\n",
    "    \"\"\"\n",
    "    IQR 방식으로 이상치를 탐지하는 함수\n",
    "    건물별로 따로 계산하여 더 정확한 이상치 탐지\n",
    "    이상치 발견 시 앞뒤 1포인트도 함께 이상치로 마킹\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "\n",
    "    for building in df[building_col].unique():\n",
    "        building_mask = df[building_col] == building\n",
    "        building_data = df[building_mask][column]\n",
    "        building_indices = df[building_mask].index.tolist()\n",
    "        building_indices_set = set(building_indices)  # 빠른 검색을 위해 set 사용\n",
    "\n",
    "        # 0이 아닌 값들만으로 IQR 계산 (0은 이미 이상치로 마킹됨)\n",
    "        non_zero_data = building_data[building_data > 0]\n",
    "\n",
    "        if len(non_zero_data) > 0:\n",
    "            Q1 = non_zero_data.quantile(0.25)\n",
    "            Q3 = non_zero_data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "\n",
    "            # IQR 기반 경계값 계산\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # 해당 건물의 이상치 인덱스 찾기 (0값은 제외, 이미 마킹됨)\n",
    "            building_outliers_mask = ((building_data < lower_bound) | (building_data > upper_bound)) & (building_data > 0)\n",
    "            original_outlier_indices = building_data[building_outliers_mask].index.tolist()\n",
    "\n",
    "            # 앞뒤 1포인트 확장\n",
    "            expanded_outlier_indices = set()\n",
    "            for idx in original_outlier_indices:\n",
    "                # 현재 인덱스와 앞뒤 1포인트 추가\n",
    "                for offset in [-1, 0, 1]:\n",
    "                    new_idx = idx + offset\n",
    "                    # 인덱스 범위 체크 및 같은 건물 내의 인덱스인지 확인\n",
    "                    if 0 <= new_idx < len(df) and new_idx in building_indices_set:\n",
    "                        expanded_outlier_indices.add(new_idx)\n",
    "\n",
    "            outlier_indices.extend(list(expanded_outlier_indices))\n",
    "\n",
    "            print(f\"건물 {building}: Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}\")\n",
    "            print(f\"  Lower bound: {lower_bound:.2f}, Upper bound: {upper_bound:.2f}\")\n",
    "            print(f\"  원본 IQR 이상치 개수: {len(original_outlier_indices)}\")\n",
    "            print(f\"  확장된 IQR 이상치 개수: {len(expanded_outlier_indices)}\")\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "# power_consumption에 대해 IQR 이상치 탐지 (확장 버전)\n",
    "print(\"=== IQR 방식 이상치 탐지 (앞뒤 1포인트 포함) ===\")\n",
    "iqr_outlier_indices = detect_outliers_iqr_expanded(train, 'power_consumption')\n",
    "\n",
    "# IQR 이상치를 outlier_detect에 마킹 (기존 0값 이상치와 합쳐짐)\n",
    "train.loc[iqr_outlier_indices, 'outlier_detect'] = 1\n",
    "\n",
    "# 최종 이상치 개수 확인\n",
    "total_outliers = (train['outlier_detect'] == 1).sum()\n",
    "zero_outliers = len(zero_outlier_indices)  # 원본 0값 이상치 개수\n",
    "iqr_outliers = len(iqr_outlier_indices)\n",
    "\n",
    "print(f\"\\n=== 최종 이상치 현황 ===\")\n",
    "print(f\"power_consumption=0 원본 이상치: {zero_outliers}개\")\n",
    "print(f\"IQR 방식 확장 이상치: {iqr_outliers}개\")\n",
    "print(f\"총 이상치 개수: {total_outliers}개\")\n",
    "print(f\"전체 데이터 대비 이상치 비율: {total_outliers/len(train)*100:.2f}%\")"
   ],
   "id": "cbc417b4899ce38b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# 이상치 제거 원상복구\n",
    "# ==========================================================\n",
    "\n",
    "# 이상치 제거 원상복구 설정\n",
    "exclude_outlier_removal_buildings = [1, # 호텔\n",
    "                                     13, 15, 23, 94, # 연구소\n",
    "                                     8, 22, 46, 55, 87, # 학교\n",
    "                                     71, 25, 91, 93, #아파트\n",
    "                                     2, 51, 99, # 상용\n",
    "                                     30, 37, 43, 52, 64, # IDC(전화국)\n",
    "                                     96, # 건물기타\n",
    "                                     ]  # 이상치 제거를 하지 않을 건물 번호 리스트\n",
    "\n",
    "# 지정된 건물의 outlier_detect를 0으로 설정 (원상복구)\n",
    "for building_id in exclude_outlier_removal_buildings:\n",
    "    if building_id in train['building_number'].unique():\n",
    "        mask = train['building_number'] == building_id\n",
    "        train.loc[mask, 'outlier_detect'] = 0\n",
    "        print(f\"Building {building_id}: {mask.sum()} rows restored (outlier_detect set to 0)\")\n",
    "    else:\n",
    "        print(f\"Warning: Building {building_id} not found in dataset\")\n",
    "\n",
    "# 현재 이상치 상태 확인\n",
    "print(\"\\n=== 현재 이상치 마킹 상태 ===\")\n",
    "outlier_count_by_building = train.groupby('building_number')['outlier_detect'].sum().sort_index()\n",
    "total_outliers = train['outlier_detect'].sum()\n",
    "total_rows = len(train)\n",
    "\n",
    "print(f\"전체 데이터: {total_rows:,} rows\")\n",
    "print(f\"전체 이상치: {total_outliers:,} rows ({total_outliers/total_rows*100:.2f}%)\")\n",
    "print(f\"\\n건물별 이상치 개수:\")\n",
    "for building_id, count in outlier_count_by_building.items():\n",
    "    building_total = len(train[train['building_number'] == building_id])\n",
    "    print(f\"  Building {building_id:2d}: {count:4d} outliers ({count/building_total*100:5.2f}%)\")\n"
   ],
   "id": "ebb546a0e98bc543",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# 특정 기간 이상치 마킹\n",
    "# ==========================================================\n",
    "\n",
    "def mark_outlier_periods(df):\n",
    "    \"\"\"\n",
    "    reference/xgb_type.py에서 확인한 특정 기간들을 이상치로 마킹\n",
    "    기존에는 데이터를 drop했지만, 여기서는 outlier_detect=1로 마킹\n",
    "\n",
    "    기간 형식:\n",
    "    - 날짜만: ['2024-07-07', '2024-07-08']  # 전체 날짜\n",
    "    - 시간까지: ['2024-07-07 14:00', '2024-07-08 09:00']  # 특정 시간대\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== 특정 기간 이상치 마킹 시작 ===\")\n",
    "\n",
    "    # date_time 컬럼이 datetime 타입인지 확인하고 변환\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df['date_time']):\n",
    "        print(\"date_time을 datetime 타입으로 변환 중...\")\n",
    "        df['date_time'] = pd.to_datetime(df['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "    initial_outliers = (df['outlier_detect'] == 1).sum()\n",
    "\n",
    "    # 1. 건물기타 (Other Buildings) 등.\n",
    "    other_buildings_config = [\n",
    "        {'building_id': 7, 'periods': [\n",
    "            ['2024-07-07 10:00', '2024-07-08 11:00'],\n",
    "            ['2024-07-12 14:00', '2024-08-06 03:00']\n",
    "        ]},\n",
    "        {'building_id': 26, 'periods': [\n",
    "            ['2024-06-17 14:00', '2024-06-18 11:00']\n",
    "        ]},\n",
    "        {'building_id': 69, 'periods': [\n",
    "            ['2024-06-02', '2024-06-02']\n",
    "        ]},\n",
    "        {'building_id': 82, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-17 14:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 2. IDC(전화국) 등\n",
    "    idc_config = [\n",
    "        {'building_id': 30, 'periods': [\n",
    "            ['2024-07-13 20:00', '2024-07-13 20:00'],\n",
    "            ['2024-07-25 00:00', '2024-07-25 00:00']\n",
    "        ]},\n",
    "        {'building_id': 36, 'periods': [\n",
    "            ['2024-07-21 00:00', '2024-07-21 23:00']\n",
    "        ]},\n",
    "        {'building_id': 43, 'periods': [\n",
    "            ['2024-06-10 17:00', '2024-06-10 18:00'],\n",
    "            ['2024-08-12 16:00', '2024-08-12 17:00']\n",
    "        ]},\n",
    "        {'building_id': 52, 'periods': [\n",
    "            ['2024-08-10 00:00', '2024-08-10 02:00']\n",
    "        ]},\n",
    "        {'building_id': 57, 'periods': [\n",
    "            ['2024-06-01', '2024-06-07']\n",
    "        ]},\n",
    "        {'building_id': 67, 'periods': [\n",
    "            ['2024-07-26', '2024-07-27'],\n",
    "            ['2024-08-01 15:00', '2024-08-01 17:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 3. 병원 (Hospital) 등.\n",
    "    hospital_config = [\n",
    "        {'building_id': 17, 'periods': [\n",
    "            ['2024-06-25 20:00', '2024-06-26 08:00']\n",
    "        ]},\n",
    "        {'building_id': 44, 'periods': [\n",
    "            ['2024-06-06 12:00', '2024-06-06 14:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 4. 상용 (Commercial) 등\n",
    "    commercial_config = [\n",
    "        {'building_id': 20, 'periods': [\n",
    "            ['2024-06-01 10:00', '2024-06-01 11:00'],\n",
    "            ['2024-06-09 10:00', '2024-06-09 10:00']\n",
    "        ]},\n",
    "        {'building_id': 41, 'periods': [\n",
    "            ['2024-07-17 09:00', '2024-07-17 15:00']\n",
    "        ]},\n",
    "        {'building_id': 51, 'periods': [\n",
    "            ['2024-06-30', '2024-06-30']\n",
    "        ]},\n",
    "    ]\n",
    "\n",
    "    # 5. 아파트 (Apartment) 등\n",
    "    apartment_config = [\n",
    "        {'building_id': 25, 'periods': [\n",
    "            ['2024-07-04 12:00', '2024-07-04 14:00']\n",
    "        ]},\n",
    "        {'building_id': 65, 'periods': [\n",
    "            ['2024-06-01', '2024-06-09']\n",
    "        ]},\n",
    "        {'building_id': 70, 'periods': [\n",
    "            ['2024-06-04 09:00', '2024-06-05 08:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 6. 연구소 (Research Institute) 등.\n",
    "    research_config = [\n",
    "        {'building_id': 49, 'periods': [\n",
    "            ['2024-06-15 09:00', '2024-06-15 11:00'],\n",
    "            ['2024-07-06', '2024-07-07'],\n",
    "            ['2024-08-17', '2024-08-18'],\n",
    "            ['2024-08-22', '2024-08-22'],\n",
    "        ]},\n",
    "        {'building_id': 53, 'periods': [\n",
    "            ['2024-06-14 16:00', '2024-06-17 10:00'],\n",
    "            ['2024-08-18 15:00', '2024-08-19 09:00']\n",
    "        ]},\n",
    "        {'building_id': 83, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-17 14:00'],\n",
    "        ]},\n",
    "        {'building_id': 94, 'periods': [\n",
    "            ['2024-07-26 18:00', '2024-08-05 05:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 7. 학교 (University) 등.\n",
    "    university_config = [\n",
    "        {'building_id': 8, 'periods': [\n",
    "            ['2024-07-21 08:00', '2024-07-21 11:00'],\n",
    "            ['2024-08-24 09:00', '2024-08-24 23:00']\n",
    "        ]},\n",
    "        {'building_id': 12, 'periods': [\n",
    "            ['2024-07-21 08:00', '2024-07-21 11:00'],\n",
    "            ['2024-08-24 08:00', '2024-08-24 10:00']\n",
    "        ]},\n",
    "        {'building_id': 55, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-17 14:00'],\n",
    "        ]},\n",
    "        {'building_id': 87, 'periods': [\n",
    "            ['2024-06-01', '2024-06-30']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 8. 호텔 (Hotel) 등\n",
    "    hotel_config = [\n",
    "        {'building_id': 89, 'periods': [\n",
    "            ['2024-07-12 00:00', '2024-07-12 23:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 9. 공공 (Public) 등\n",
    "    public_config = [\n",
    "        {'building_id': 38, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-17 15:00']\n",
    "        ]},\n",
    "        {'building_id': 72, 'periods': [\n",
    "            ['2024-06-11 00:00', '2024-06-11 02:00']\n",
    "        ]},\n",
    "        {'building_id': 92, 'periods': [\n",
    "            ['2024-07-17 14:00', '2024-07-18 04:00']\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    # 10. 백화점 (Department Store) 등\n",
    "    department_store_config = [\n",
    "        {'building_id': 19, 'periods': [\n",
    "            ['2024-07-31 13:00', '2024-07-31 16:00']\n",
    "        ]},\n",
    "        {'building_id': 32, 'periods': [\n",
    "            ['2024-07-08 09:00', '2024-07-08 10:00']\n",
    "        ]},\n",
    "        {'building_id': 40, 'periods': [\n",
    "            ['2024-07-14 00:00', '2024-07-14 01:00']\n",
    "        ]},\n",
    "        {'building_id': 45, 'periods': [\n",
    "            ['2024-07-04 00:00', '2024-07-04 03:00']\n",
    "        ]},\n",
    "        {'building_id': 73, 'periods': [\n",
    "            ['2024-07-08 22:00', '2024-07-08 22:00']\n",
    "        ]},\n",
    "        {'building_id': 79, 'periods': [\n",
    "            ['2024-08-19 03:00', '2024-08-19 05:00']\n",
    "        ]},\n",
    "        {'building_id': 95, 'periods': [\n",
    "            ['2024-08-05 10:00', '2024-08-05 11:00']\n",
    "        ]},\n",
    "        # ===================== 월요일 제거 ==========================\n",
    "        #\n",
    "        # {'building_id': 19, 'periods': [\n",
    "        #     ['2024-06-10 00:00', '2024-06-10 23:00'],\n",
    "        #     ['2024-07-08 00:00', '2024-07-08 23:00'],\n",
    "        #     ['2024-08-19 00:00', '2024-08-19 23:00'],\n",
    "        # ]},\n",
    "        #\n",
    "        # {'building_id': 45, 'periods': [\n",
    "        #     ['2024-06-10 00:00', '2024-06-10 23:00'],\n",
    "        #     ['2024-07-08 00:00', '2024-07-08 23:00'],\n",
    "        #     ['2024-08-19 00:00', '2024-08-19 23:00'],\n",
    "        # ]},\n",
    "        #\n",
    "        # {'building_id': 54, 'periods': [\n",
    "        #     ['2024-06-17 00:00', '2024-06-17 23:00'],\n",
    "        #     ['2024-07-01 00:00', '2024-07-01 23:00'],\n",
    "        #     ['2024-08-19 00:00', '2024-08-19 23:00'],\n",
    "        # ]},\n",
    "        #\n",
    "        # {'building_id': 74, 'periods': [\n",
    "        #     ['2024-06-17 00:00', '2024-06-17 23:00'],\n",
    "        #     ['2024-07-01 00:00', '2024-07-01 23:00'],\n",
    "        # ]},\n",
    "        #\n",
    "        # {'building_id': 79, 'periods': [\n",
    "        #     ['2024-06-17 00:00', '2024-06-17 23:00'],\n",
    "        #     ['2024-07-01 00:00', '2024-07-01 23:00'],\n",
    "        #     ['2024-08-19 00:00', '2024-08-19 23:00'],\n",
    "        # ]},\n",
    "        # {'building_id': 95, 'periods': [\n",
    "        #     ['2024-07-08 00:00', '2024-07-08 23:00'],\n",
    "        #     ['2024-08-05 00:00', '2024-08-05 23:00'],\n",
    "        # ]},\n",
    "    ]\n",
    "\n",
    "    # 모든 설정을 통합 (10개 건물 타입 전체)\n",
    "    all_configs = [\n",
    "        ('건물기타 (Other Buildings)', other_buildings_config),\n",
    "        ('IDC(전화국)', idc_config),\n",
    "        ('병원 (Hospital)', hospital_config),\n",
    "        ('상용 (Commercial)', commercial_config),\n",
    "        ('아파트 (Apartment)', apartment_config),\n",
    "        ('연구소 (Research Institute)', research_config),\n",
    "        ('학교 (University)', university_config),\n",
    "        ('호텔 (Hotel)', hotel_config),\n",
    "        ('공공 (Public)', public_config),\n",
    "        ('백화점 (Department Store)', department_store_config)\n",
    "    ]\n",
    "\n",
    "    total_marked = 0\n",
    "\n",
    "    for building_type_name, configs in all_configs:\n",
    "        print(f\"\\\\n--- {building_type_name} 처리 ---\")\n",
    "        type_marked = 0\n",
    "\n",
    "        if not configs:  # 빈 리스트인 경우\n",
    "            print(f\"  {building_type_name}: 설정된 이상치 기간 없음\")\n",
    "            continue\n",
    "\n",
    "        for config in configs:\n",
    "            building_id = config['building_id']\n",
    "            periods = config['periods']\n",
    "\n",
    "            building_marked = 0\n",
    "            for period in periods:\n",
    "                # 시간이 포함되어 있는지 확인\n",
    "                if len(period[0].split()) > 1:  # 시간이 포함된 경우\n",
    "                    start_datetime = pd.to_datetime(period[0])\n",
    "                    end_datetime = pd.to_datetime(period[1])\n",
    "\n",
    "                    # 정확한 시간까지 비교\n",
    "                    condition = (df['building_number'] == building_id) & \\\n",
    "                               (df['date_time'] >= start_datetime) & \\\n",
    "                               (df['date_time'] <= end_datetime)\n",
    "\n",
    "                    print(f\"  건물 {building_id}: {period[0]} ~ {period[1]} (시간 포함)\", end=\" | \")\n",
    "                else:  # 날짜만 있는 경우 (기존 방식)\n",
    "                    start_date = pd.to_datetime(period[0]).date()\n",
    "                    end_date = pd.to_datetime(period[1]).date()\n",
    "\n",
    "                    # 해당 건물과 기간에 맞는 조건\n",
    "                    condition = (df['building_number'] == building_id) & \\\n",
    "                               (df['date_time'].dt.date >= start_date) & \\\n",
    "                               (df['date_time'].dt.date <= end_date)\n",
    "\n",
    "                    print(f\"  건물 {building_id}: {period[0]} ~ {period[1]} (전체 날짜)\", end=\" | \")\n",
    "\n",
    "                marked_count = condition.sum()\n",
    "                df.loc[condition, 'outlier_detect'] = 1\n",
    "                building_marked += marked_count\n",
    "                print(f\"{marked_count}개 마킹\")\n",
    "\n",
    "            type_marked += building_marked\n",
    "\n",
    "        total_marked += type_marked\n",
    "        print(f\"  {building_type_name} 총 마킹: {type_marked}개\")\n",
    "\n",
    "    final_outliers = (df['outlier_detect'] == 1).sum()\n",
    "    newly_marked = final_outliers - initial_outliers\n",
    "\n",
    "    print(f\"\\\\n=== 특정 기간 이상치 마킹 완료 ===\")\n",
    "    print(f\"기존 이상치: {initial_outliers}개\")\n",
    "    print(f\"새로 마킹된 이상치: {newly_marked}개\")\n",
    "    print(f\"최종 이상치: {final_outliers}개\")\n",
    "    print(f\"전체 데이터 대비 이상치 비율: {final_outliers/len(df)*100:.2f}%\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# date_time 컬럼을 먼저 datetime 타입으로 변환\n",
    "print(\"=== 날짜 데이터 타입 확인 및 변환 ===\")\n",
    "if not pd.api.types.is_datetime64_any_dtype(train['date_time']):\n",
    "    print(\"train 데이터의 date_time을 datetime 타입으로 변환 중...\")\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "if not pd.api.types.is_datetime64_any_dtype(test['date_time']):\n",
    "    print(\"test 데이터의 date_time을 datetime 타입으로 변환 중...\")\n",
    "    test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "# train 데이터에 특정 기간 이상치 마킹 적용\n",
    "train = mark_outlier_periods(train)\n",
    "\n",
    "# test 데이터에도 outlier_detect 컬럼이 있는지 확인하고 같은 기간 마킹\n",
    "if 'outlier_detect' in test.columns:\n",
    "    print(\"\\\\n=== Test 데이터에도 같은 기간 이상치 마킹 ===\")\n",
    "    test = mark_outlier_periods(test)\n",
    "else:\n",
    "    print(\"\\\\nTest 데이터에는 outlier_detect 컬럼이 없어 건너뜁니다.\")"
   ],
   "id": "1de6dde35097c4fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "feature engineering",
   "id": "836249f510ad8950"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ==========================================================\n",
    "# Feature Engineering - Type 1 & Type 2 Features\n",
    "# Based on FEATURE.md specifications\n",
    "# ==========================================================\n",
    "\n",
    "print(\"=== Feature Engineering 시작 ===\")\n",
    "\n",
    "# 날짜 변환 확인\n",
    "if not pd.api.types.is_datetime64_any_dtype(train['date_time']):\n",
    "    train['date_time'] = pd.to_datetime(train['date_time'], format='%Y%m%d %H')\n",
    "if not pd.api.types.is_datetime64_any_dtype(test['date_time']):\n",
    "    test['date_time'] = pd.to_datetime(test['date_time'], format='%Y%m%d %H')\n",
    "\n",
    "# ==========================================================\n",
    "# 기본 시간 피처 생성\n",
    "# ==========================================================\n",
    "print(\"1. 기본 시간 피처 생성...\")\n",
    "\n",
    "# 기본 시간 피처\n",
    "for df in [train, test]:\n",
    "    df['hour'] = df['date_time'].dt.hour\n",
    "    df['day'] = df['date_time'].dt.day\n",
    "    df['month'] = df['date_time'].dt.month\n",
    "    df['day_of_week'] = df['date_time'].dt.dayofweek  # 월요일=0, 일요일=6\n",
    "    df['week'] = df['date_time'].dt.isocalendar().week  # 주차\n",
    "\n",
    "# ==========================================================\n",
    "# 건물별 휴일 피처 (is_holiday) - HOLIDAY.md 기반\n",
    "# ==========================================================\n",
    "print(\"2. 건물별 휴일 피처 생성...\")\n",
    "\n",
    "def apply_building_specific_holidays(df):\n",
    "    \"\"\"HOLIDAY.md에 따른 건물별 휴일 설정\"\"\"\n",
    "    \n",
    "    # 초기화\n",
    "    df['is_holiday'] = 0\n",
    "    \n",
    "    # 휴일 설정 정의 (HOLIDAY.md 기반)\n",
    "    holiday_config = {\n",
    "        # Commercial\n",
    "        'Commercial': {\n",
    "            'buildings': [2, 6, 16, 20, 51, 86],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        'Commercial_56': {\n",
    "            'buildings': [56],\n",
    "            'holidays': ['8/15']\n",
    "        },\n",
    "        \n",
    "        # Department Store\n",
    "        'Department_Store_bimonthly_sunday': {\n",
    "            'buildings': [27, 40, 59, 63],\n",
    "            'holidays': ['bimonthly_sunday']  # 2번째, 4번째 일요일\n",
    "        },\n",
    "        'Department_Store_bimonthly_monday': {\n",
    "            'buildings': [32],\n",
    "            'holidays': ['bimonthly_monday']  # 2번째, 4번째 월요일\n",
    "        },\n",
    "        'Department_Store_45': {\n",
    "            'buildings': [45],\n",
    "            'holidays': ['6/10', '7/8', '8/19']\n",
    "        },\n",
    "        'Department_Store_54': {\n",
    "            'buildings': [54],\n",
    "            'holidays': ['6/17', '7/8', '8/19']\n",
    "        },\n",
    "        'Department_Store_74': {\n",
    "            'buildings': [74],\n",
    "            'holidays': ['6/17', '7/1']\n",
    "        },\n",
    "        'Department_Store_79': {\n",
    "            'buildings': [79],\n",
    "            'holidays': ['6/17', '7/1', '8/19']\n",
    "        },\n",
    "        'Department_Store_95': {\n",
    "            'buildings': [95],\n",
    "            'holidays': ['7/8', '8/5']\n",
    "        },\n",
    "        \n",
    "        # Hospital\n",
    "        'Hospital': {\n",
    "            'buildings': 'all_hospital',  # 모든 병원\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        \n",
    "        # IDC\n",
    "        'IDC_43_52': {\n",
    "            'buildings': [43, 52],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        'IDC_64_67': {\n",
    "            'buildings': [64, 67],\n",
    "            'holidays': ['8/15']\n",
    "        },\n",
    "        \n",
    "        # Other Buildings\n",
    "        'Other_Buildings': {\n",
    "            'buildings': [47, 67],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        \n",
    "        # Public\n",
    "        'Public': {\n",
    "            'buildings': [38, 50, 66, 68, 72, 80],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        \n",
    "        # Research Institute\n",
    "        'Research_Institute_basic': {\n",
    "            'buildings': [13, 15, 37, 49, 53, 62, 83],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        },\n",
    "        'Research_Institute_extended': {\n",
    "            'buildings': [23, 94],\n",
    "            'holidays': ['6/6', '6/7', '8/15', '8/16']\n",
    "        },\n",
    "        \n",
    "        # University\n",
    "        'University': {\n",
    "            'buildings': [5, 8, 12, 14, 22, 24, 46, 55, 60, 87],\n",
    "            'holidays': ['6/6', '8/15']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 각 설정 적용\n",
    "    for config_name, config in holiday_config.items():\n",
    "        buildings = config['buildings']\n",
    "        holidays = config['holidays']\n",
    "        \n",
    "        # 건물 선택\n",
    "        if buildings == 'all_hospital':\n",
    "            # 모든 병원 건물 선택 (building_type == 'Hospital')\n",
    "            building_mask = df['building_type'] == 'Hospital'\n",
    "        else:\n",
    "            # 특정 건물 번호들 선택\n",
    "            building_mask = df['building_number'].isin(buildings)\n",
    "        \n",
    "        # 각 휴일 적용\n",
    "        for holiday in holidays:\n",
    "            if holiday == 'bimonthly_sunday':\n",
    "                # 2번째, 4번째 일요일\n",
    "                is_sunday = df['date_time'].dt.dayofweek == 6\n",
    "                week_of_month = ((df['date_time'].dt.day - 1) // 7)\n",
    "                is_2nd_or_4th_week = (week_of_month == 1) | (week_of_month == 3)\n",
    "                holiday_condition = building_mask & is_sunday & is_2nd_or_4th_week\n",
    "                \n",
    "            elif holiday == 'bimonthly_monday':\n",
    "                # 2번째, 4번째 월요일\n",
    "                is_monday = df['date_time'].dt.dayofweek == 0\n",
    "                week_of_month = ((df['date_time'].dt.day - 1) // 7)\n",
    "                is_2nd_or_4th_week = (week_of_month == 1) | (week_of_month == 3)\n",
    "                holiday_condition = building_mask & is_monday & is_2nd_or_4th_week\n",
    "                \n",
    "            else:\n",
    "                # 특정 날짜 (MM/DD 형식)\n",
    "                month, day = map(int, holiday.split('/'))\n",
    "                date_condition = (df['date_time'].dt.month == month) & (df['date_time'].dt.day == day)\n",
    "                holiday_condition = building_mask & date_condition\n",
    "            \n",
    "            # 휴일 마킹\n",
    "            df.loc[holiday_condition, 'is_holiday'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# train과 test에 휴일 설정 적용\n",
    "train = apply_building_specific_holidays(train)\n",
    "test = apply_building_specific_holidays(test)\n",
    "\n",
    "print(f\"Train 휴일 데이터: {(train['is_holiday'] == 1).sum()}개\")\n",
    "print(f\"Test 휴일 데이터: {(test['is_holiday'] == 1).sum()}개\")\n",
    "\n",
    "# ==========================================================\n",
    "# 주기적 시간 피처 (sin_hour, cos_hour)\n",
    "# ==========================================================\n",
    "print(\"3. 주기적 시간 피처 생성...\")\n",
    "\n",
    "for df in [train, test]:\n",
    "    # 24시간 주기\n",
    "    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "\n",
    "# ==========================================================\n",
    "# 일별 온도 통계 피처\n",
    "# ==========================================================\n",
    "print(\"4. 일별 온도 통계 피처 생성...\")\n",
    "\n",
    "def calculate_daily_temperature_stats(dataframe):\n",
    "    \"\"\"일별 온도 통계 계산\"\"\"\n",
    "    # 건물별, 월별, 일별로 온도 통계 계산\n",
    "    daily_temp_stats = dataframe.groupby(['building_number', 'month', 'day'])['temperature'].agg([\n",
    "        ('day_max_temperature', 'max'),\n",
    "        ('day_min_temperature', 'min'),\n",
    "        ('day_mean_temperature', 'mean')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # 원본 데이터와 merge\n",
    "    merged = dataframe.merge(daily_temp_stats, on=['building_number', 'month', 'day'], how='left')\n",
    "    \n",
    "    # 일교차 계산\n",
    "    merged['day_diff_temperature'] = merged['day_max_temperature'] - merged['day_min_temperature']\n",
    "    \n",
    "    return merged\n",
    "\n",
    "train = calculate_daily_temperature_stats(train)\n",
    "test = calculate_daily_temperature_stats(test)\n",
    "\n",
    "# ==========================================================\n",
    "# 여름 계절 피처 (summer_sin, summer_cos)\n",
    "# ==========================================================\n",
    "print(\"5. 여름 계절 피처 생성...\")\n",
    "\n",
    "for df in [train, test]:\n",
    "    # 6월~8월을 여름으로 간주, 월별로 주기적 변화 표현\n",
    "    # 6월=0, 7월=1, 8월=2 -> 정규화하여 [0, 2π] 범위로 변환\n",
    "    summer_months = {6: 0, 7: 1, 8: 2}\n",
    "    df['summer_phase'] = df['month'].map(summer_months)\n",
    "    df['summer_sin'] = np.sin(2 * np.pi * df['summer_phase'] / 3.0)\n",
    "    df['summer_cos'] = np.cos(2 * np.pi * df['summer_phase'] / 3.0)\n",
    "    df = df.drop('summer_phase', axis=1)\n",
    "\n",
    "# ==========================================================\n",
    "# 날씨 지수 피처 (THI, WCT, CDH)\n",
    "# ==========================================================\n",
    "print(\"6. 날씨 지수 피처 생성...\")\n",
    "\n",
    "# THI (Temperature-Humidity Index) 계산\n",
    "for df in [train, test]:\n",
    "    df['THI'] = 9/5 * df['temperature'] - 0.55 * (1 - df['humidity']/100) * (9/5 * df['temperature'] - 26) + 32\n",
    "\n",
    "# WCT (Wind Chill Temperature) 계산\n",
    "for df in [train, test]:\n",
    "    df['WCT'] = 13.12 + 0.6125 * df['temperature'] - 11.37 * (df['windspeed'] ** 0.16) + \\\n",
    "                0.3965 * (df['windspeed'] ** 0.16) * df['temperature']\n",
    "\n",
    "# CDH (Cooling Degrees Hours) 계산\n",
    "def calculate_cdh(temperatures):\n",
    "    \"\"\"CDH 계산 - 26도를 기준으로 누적 냉방도 계산\"\"\"\n",
    "    # 수정: 26도 이하일 때는 0으로 처리\n",
    "    temp_diff = np.maximum(temperatures - 26, 0)\n",
    "    cumsum = np.cumsum(temp_diff)\n",
    "    # 11시간 이후부터는 sliding window 적용\n",
    "    return np.concatenate((cumsum[:11], cumsum[11:] - cumsum[:-11]))\n",
    "\n",
    "def calculate_and_add_cdh(dataframe):\n",
    "    \"\"\"건물별로 CDH 계산 후 추가\"\"\"\n",
    "    cdhs = []\n",
    "    for building_id in range(1, 101):\n",
    "        building_data = dataframe[dataframe['building_number'] == building_id]['temperature'].values\n",
    "        if len(building_data) > 0:\n",
    "            cdh = calculate_cdh(building_data)\n",
    "            cdhs.extend(cdh)\n",
    "        # 수정: else 블록 제거 (애초에 building_data가 비어있으면 위 조건문에 안 들어감)\n",
    "    return cdhs\n",
    "\n",
    "train['CDH'] = calculate_and_add_cdh(train)\n",
    "test['CDH'] = calculate_and_add_cdh(test)\n",
    "\n",
    "# ==========================================================\n",
    "# 성수기 피처 (is_peak_season)\n",
    "# ==========================================================\n",
    "print(\"7. 성수기 피처 생성...\")\n",
    "\n",
    "# 호텔 성수기 설정 (reference 기반)\n",
    "peak_season_config = {\n",
    "    9: [{'start': '2024-07-13', 'end': '2024-08-31'}],\n",
    "    10: [{'start': '2024-07-04', 'end': '2024-08-22'}],\n",
    "    28: [{'start': '2024-07-18', 'end': '2024-08-31'}],\n",
    "    77: [{'start': '2024-07-18', 'end': '2024-08-31'}],\n",
    "    89: [{'start': '2024-07-17', 'end': '2024-08-31'}],\n",
    "    98: [{'start': '2024-07-15', 'end': '2024-08-31'}],\n",
    "    100: [{'start': '2024-07-15', 'end': '2024-08-31'}],\n",
    "}\n",
    "\n",
    "for df in [train, test]:\n",
    "    df['is_peak_season'] = 0\n",
    "    for building_id, seasons in peak_season_config.items():\n",
    "        for season in seasons:\n",
    "            start_date = pd.to_datetime(season['start'])\n",
    "            end_date = pd.to_datetime(season['end'])\n",
    "            condition = (\n",
    "                (df['building_number'] == building_id) &\n",
    "                (df['date_time'] >= start_date) &\n",
    "                (df['date_time'] <= end_date)\n",
    "            )\n",
    "            df.loc[condition, 'is_peak_season'] = 1\n",
    "\n",
    "# ==========================================================\n",
    "# 통계 피처 계산용 클린 데이터 생성\n",
    "# ==========================================================\n",
    "print(\"8. 통계 피처 계산용 데이터 준비...\")\n",
    "\n",
    "# 이상치가 제거된 데이터로 통계 피처 계산\n",
    "train_for_stats = train[train['outlier_detect'] == 0].copy()\n",
    "\n",
    "# ==========================================================\n",
    "# 통계 피처 생성 (day_hour_mean, day_hour_std, holiday_hour_mean, holiday_std)\n",
    "# ==========================================================\n",
    "print(\"9. 통계 피처 생성...\")\n",
    "\n",
    "# 요일별 시간 평균/표준편차 (power_consumption 기준)\n",
    "day_hour_stats = train_for_stats.groupby(['building_number', 'hour', 'day_of_week'])['power_consumption'].agg([\n",
    "    ('day_hour_mean', 'mean'),\n",
    "    ('day_hour_std', 'std')\n",
    "]).reset_index()\n",
    "\n",
    "# NaN 처리\n",
    "day_hour_stats['day_hour_std'] = day_hour_stats['day_hour_std'].fillna(0)\n",
    "\n",
    "# 휴일 시간 평균/표준편차 - 안전한 처리\n",
    "holiday_data = train_for_stats[train_for_stats['is_holiday'] == 1]\n",
    "print(f\"휴일 데이터 개수: {len(holiday_data)}\")\n",
    "\n",
    "if len(holiday_data) > 0:\n",
    "    holiday_hour_stats = holiday_data.groupby(['building_number', 'hour'])['power_consumption'].agg([\n",
    "        ('holiday_hour_mean', 'mean'),\n",
    "        ('holiday_std', 'std')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # NaN 처리\n",
    "    holiday_hour_stats['holiday_std'] = holiday_hour_stats['holiday_std'].fillna(0)\n",
    "    print(f\"휴일 통계 생성: {len(holiday_hour_stats)}개\")\n",
    "else:\n",
    "    # 휴일 데이터가 없으면 빈 데이터프레임 생성\n",
    "    holiday_hour_stats = pd.DataFrame(columns=['building_number', 'hour', 'holiday_hour_mean', 'holiday_std'])\n",
    "    print(\"휴일 데이터가 없어서 빈 통계 생성\")\n",
    "\n",
    "# train과 test에 통계 피처 병합\n",
    "print(\"10. 통계 피처 병합...\")\n",
    "\n",
    "# 요일별 시간 통계 병합\n",
    "train = train.merge(day_hour_stats, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "test = test.merge(day_hour_stats, on=['building_number', 'hour', 'day_of_week'], how='left')\n",
    "\n",
    "# 휴일 통계 초기화\n",
    "train['holiday_hour_mean'] = 0.0\n",
    "train['holiday_std'] = 0.0\n",
    "test['holiday_hour_mean'] = 0.0\n",
    "test['holiday_std'] = 0.0\n",
    "\n",
    "# 휴일 통계가 있으면 병합\n",
    "if len(holiday_hour_stats) > 0:\n",
    "    for df in [train, test]:\n",
    "        holiday_mask = df['is_holiday'] == 1\n",
    "        if holiday_mask.sum() > 0:\n",
    "            holiday_df = df[holiday_mask][['building_number', 'hour']].merge(\n",
    "                holiday_hour_stats, on=['building_number', 'hour'], how='left'\n",
    "            )\n",
    "            \n",
    "            # 성공적으로 병합된 경우만 업데이트\n",
    "            if len(holiday_df) > 0:\n",
    "                df.loc[holiday_mask, 'holiday_hour_mean'] = holiday_df['holiday_hour_mean'].fillna(0).values\n",
    "                df.loc[holiday_mask, 'holiday_std'] = holiday_df['holiday_std'].fillna(0).values\n",
    "\n",
    "# ==========================================================\n",
    "# Type 2 전용 피처 (building features)\n",
    "# ==========================================================\n",
    "print(\"11. Type 2 전용 건물 피처 확인...\")\n",
    "\n",
    "# total_area, cooling_area는 이미 존재 (building_info에서 병합됨)\n",
    "# building_number, building_type도 이미 존재\n",
    "print(\"건물 관련 피처들이 이미 존재합니다.\")\n",
    "\n",
    "# ==========================================================\n",
    "# Type 2 detail_2 전용 피처\n",
    "# ==========================================================\n",
    "print(\"12. Type 2 detail_2 전용 피처 생성...\")\n",
    "\n",
    "# pv_temp (태양광 발전 온도 상호작용)\n",
    "for df in [train, test]:\n",
    "    df['pv_temp'] = df['solar_power_capacity'] * df['temperature']\n",
    "\n",
    "# ess_pcs_std (ESS 관련 표준편차)\n",
    "# 건물별 ESS 용량의 표준편차를 시간대별로 계산\n",
    "if len(train_for_stats) > 0:\n",
    "    ess_stats = train_for_stats.groupby(['building_number', 'hour'])['ess_capacity'].agg([\n",
    "        ('ess_pcs_std', 'std')\n",
    "    ]).reset_index()\n",
    "    \n",
    "    # 표준편차가 NaN인 경우 0으로 채움 (단일 값인 경우)\n",
    "    ess_stats['ess_pcs_std'] = ess_stats['ess_pcs_std'].fillna(0)\n",
    "    \n",
    "    # train과 test에 병합\n",
    "    train = train.merge(ess_stats, on=['building_number', 'hour'], how='left')\n",
    "    test = test.merge(ess_stats, on=['building_number', 'hour'], how='left')\n",
    "    \n",
    "    # 병합되지 않은 값들은 0으로 채움\n",
    "    train['ess_pcs_std'] = train['ess_pcs_std'].fillna(0)\n",
    "    test['ess_pcs_std'] = test['ess_pcs_std'].fillna(0)\n",
    "else:\n",
    "    # 통계 데이터가 없으면 0으로 초기화\n",
    "    train['ess_pcs_std'] = 0.0\n",
    "    test['ess_pcs_std'] = 0.0\n",
    "\n",
    "# ==========================================================\n",
    "# 피처 정리 및 확인\n",
    "# ==========================================================\n",
    "print(\"13. 피처 생성 완료 및 확인...\")\n",
    "\n",
    "# NaN 값 처리\n",
    "for df in [train, test]:\n",
    "    # 통계 피처의 NaN은 0으로 채움 (통계 정보가 없는 경우)\n",
    "    stats_columns = ['day_hour_mean', 'day_hour_std', 'holiday_hour_mean', 'holiday_std']\n",
    "    for col in stats_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "# Type 1 base_1 피처 목록 확인\n",
    "type1_base1_features = [\n",
    "    'temperature', 'humidity', 'windspeed', 'day_of_week', 'month', 'week',\n",
    "    'is_holiday', 'sin_hour', 'cos_hour', 'day_hour_mean', 'day_hour_std',\n",
    "    'holiday_hour_mean', 'holiday_std', 'THI', 'WCT', 'CDH', 'is_peak_season'\n",
    "]\n",
    "\n",
    "# Type 1 detail_1 추가 피처\n",
    "type1_detail1_features = [\n",
    "    'summer_sin', 'summer_cos', 'day_max_temperature', 'day_min_temperature',\n",
    "    'day_mean_temperature', 'day_diff_temperature'\n",
    "]\n",
    "\n",
    "# Type 2 base_2 피처 (Type 1 base_1 + building features)\n",
    "type2_base2_features = type1_base1_features + [\n",
    "    'total_area', 'cooling_area', 'building_number', 'building_type'\n",
    "]\n",
    "\n",
    "# Type 2 detail_2 추가 피처\n",
    "type2_detail2_features = [\n",
    "    'summer_sin', 'summer_cos', 'day_max_temperature', 'day_min_temperature',\n",
    "    'day_mean_temperature', 'day_diff_temperature', 'pv_temp', 'ess_pcs_std'\n",
    "]\n",
    "\n",
    "print(\"=== 피처 생성 완료 ===\")\n",
    "print(f\"Train 데이터 shape: {train.shape}\")\n",
    "print(f\"Test 데이터 shape: {test.shape}\")\n",
    "print(f\"Type 1 base_1 피처 수: {len(type1_base1_features)}\")\n",
    "print(f\"Type 1 detail_1 추가 피처 수: {len(type1_detail1_features)}\")\n",
    "print(f\"Type 2 base_2 피처 수: {len(type2_base2_features)}\")\n",
    "print(f\"Type 2 detail_2 추가 피처 수: {len(type2_detail2_features)}\")\n",
    "\n",
    "# 피처별 존재 여부 확인\n",
    "print(\"\\n=== 피처 존재 여부 확인 ===\")\n",
    "all_required_features = list(set(type1_base1_features + type1_detail1_features + type2_base2_features + type2_detail2_features))\n",
    "missing_features = []\n",
    "\n",
    "for feature in all_required_features:\n",
    "    if feature in train.columns:\n",
    "        print(f\"✓ {feature}\")\n",
    "    else:\n",
    "        print(f\"✗ {feature} - MISSING!\")\n",
    "        missing_features.append(feature)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n누락된 피처: {missing_features}\")\n",
    "else:\n",
    "    print(\"\\n모든 필수 피처가 생성되었습니다!\")\n",
    "\n",
    "# 건물별 휴일 데이터 확인\n",
    "print(f\"\\n=== 건물별 휴일 데이터 확인 ===\")\n",
    "holiday_by_building_type = train[train['is_holiday'] == 1].groupby('building_type').size()\n",
    "for building_type, count in holiday_by_building_type.items():\n",
    "    print(f\"{building_type}: {count}개\")\n",
    "\n",
    "holiday_count_train = (train['is_holiday'] == 1).sum()\n",
    "holiday_count_test = (test['is_holiday'] == 1).sum()\n",
    "print(f\"\\nTrain 총 휴일 데이터: {holiday_count_train}개\")\n",
    "print(f\"Test 총 휴일 데이터: {holiday_count_test}개\")"
   ],
   "id": "d9eea5dcfac5f882",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"=== 데이터 타입 수정 ===\")\n",
    "\n",
    "# XGBoost 호환 데이터 타입으로 변환\n",
    "def fix_dtypes_for_xgb(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # 숫자형 컬럼들을 float32로 변환\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'int32', 'uint32', 'uint64']).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].astype('float32')\n",
    "        print(f\"  {col}: {df[col].dtype} -> float32\")\n",
    "\n",
    "    # 범주형 컬럼 처리\n",
    "    if 'building_type' in df.columns:\n",
    "        df['building_type'] = df['building_type'].astype('category')\n",
    "\n",
    "    return df\n",
    "# train과 test 데이터 타입 수정\n",
    "train = fix_dtypes_for_xgb(train)\n",
    "test = fix_dtypes_for_xgb(test)\n",
    "\n",
    "print(f\"Train dtypes 확인:\")\n",
    "print(train.select_dtypes(include=['object', 'int64']).dtypes)\n",
    "print(f\"\\nTest dtypes 확인:\")\n",
    "print(test.select_dtypes(include=['object', 'int64']).dtypes)"
   ],
   "id": "ef1c4235d541ef21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train.columns",
   "id": "b59b1d788fa1244b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import xgboost\n",
    "\n",
    "print (f\"xgboost version: {xgboost.__version__}\")"
   ],
   "id": "6250929deb1fcb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Energy Load Forecasting — A→D Pipeline (Hybrid HPO + Early Stopping + Run Folder Saving)\n",
    "\n",
    "What's new vs prior version:\n",
    "- HPO backend by stage: stage 1–3 -> random, stage 4–6 -> optuna (auto mapping)\n",
    "- Optional Bayes backend (skopt) via FORCE_BACKEND='bayes'\n",
    "- Early stopping:\n",
    "  * CV fold: XGB/LGBM use eval_set on each fold; MLP uses early_stopping=True\n",
    "  * Final refit: hold out last 10% (time-based) as eval for XGB/LGBM, MLP uses internal val\n",
    "- Per-building vs Global: different HPO budgets\n",
    "- Saving outputs under runs/{YYYYMMDD_HHMM}_{tag}/\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import pathlib\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================================\n",
    "# 0) Global Config\n",
    "# ============================================================\n",
    "TARGET_COL = \"power_consumption\"\n",
    "TIME_COL = \"num_date_time\"         # submission ID와 동일한 시간/ID 컬럼 (이미 조합되어 있다고 가정)\n",
    "BUILDING_COL = \"building_number\"\n",
    "\n",
    "# CV defaults\n",
    "DEFAULT_CV_TYPE = \"kfold\"      # 'kfold' | 'timesplit'\n",
    "DEFAULT_N_SPLITS = 5\n",
    "DEFAULT_GAP = 0\n",
    "DEFAULT_RANDOM_STATE = 42\n",
    "\n",
    "# Seeds (stage 5/6)\n",
    "DEFAULT_SEEDS = [13, 21, 42, 77, 123]\n",
    "\n",
    "# Search metric & alpha for weighted MSE\n",
    "DEFAULT_SEARCH_METRIC = \"weighted_mse\"  # 'weighted_mse'|'smape'|'mae'|'mse'\n",
    "DEFAULT_ALPHA = 3.0\n",
    "\n",
    "# Early stopping config\n",
    "ES_ROUNDS_XGB = 100\n",
    "ES_ROUNDS_LGBM = 100\n",
    "MLP_EARLY_STOP = True\n",
    "MLP_N_NO_CHANGE = 20\n",
    "MLP_VAL_FRAC = 0.1\n",
    "\n",
    "# HPO budgets (n_trials) by scope & backend\n",
    "BUDGETS = {\n",
    "    \"per_building\": {\"random\": 20, \"optuna\": 40, \"bayes\": 40},\n",
    "    \"global\":       {\"random\": 60, \"optuna\":120, \"bayes\": 80},\n",
    "}\n",
    "\n",
    "# Backend mapping by stage\n",
    "HPO_BACKEND_FOR_STAGE = {\n",
    "    1: \"random\", 2: \"random\", 3: \"random\",\n",
    "    4: \"random\", 5: \"random\", 6: \"random\", #optuna\n",
    "}\n",
    "# To force a backend globally (e.g., 'bayes'), set to 'random'|'optuna'|'bayes' or None to auto\n",
    "FORCE_BACKEND: Optional[str] = None\n",
    "\n",
    "# ============================================================\n",
    "# 1) Feature Sets (as provided)\n",
    "# ============================================================\n",
    "TYPE1_BASE = [\n",
    "    \"temperature\", \"humidity\", \"windspeed\",\n",
    "    \"day_of_week\", \"month\", \"week\", \"is_holiday\",\n",
    "    \"sin_hour\", \"cos_hour\",\n",
    "    \"day_hour_mean\", \"day_hour_std\",\n",
    "    \"holiday_hour_mean\", \"holiday_std\",\n",
    "    \"THI\", \"WCT\", \"CDH\"\n",
    "]\n",
    "TYPE1_DETAIL = [\n",
    "    \"summer_sin\", \"summer_cos\",\n",
    "    \"day_max_temperature\", \"day_min_temperature\",\n",
    "    \"day_mean_temperature\", \"day_diff_temperature\"\n",
    "]\n",
    "\n",
    "TYPE2_BASE = [\n",
    "    \"temperature\", \"humidity\", \"windspeed\",\n",
    "    \"day_of_week\", \"month\", \"week\", \"is_holiday\",\n",
    "    \"sin_hour\", \"cos_hour\",\n",
    "    \"day_hour_mean\", \"day_hour_std\",\n",
    "    \"holiday_hour_mean\", \"holiday_std\",\n",
    "    \"THI\", \"WCT\", \"CDH\",\n",
    "    \"total_area\", \"cooling_area\",\n",
    "    \"building_number\", \"building_type\"\n",
    "]\n",
    "TYPE2_DETAIL = [\n",
    "    \"summer_sin\", \"summer_cos\",\n",
    "    \"day_max_temperature\", \"day_min_temperature\",\n",
    "    \"day_mean_temperature\", \"day_diff_temperature\",\n",
    "    \"pv_temp\", \"ess_pcs_std\"\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class ModelSpec:\n",
    "    name: str            # 'A'|'B'|'C'|'D'\n",
    "    scope: str           # 'per_building' | 'global'\n",
    "    features: List[str]\n",
    "\n",
    "def build_model_specs() -> List[ModelSpec]:\n",
    "    return [\n",
    "        ModelSpec(\"A\", \"per_building\", TYPE1_BASE),\n",
    "        ModelSpec(\"B\", \"per_building\", TYPE1_BASE + TYPE1_DETAIL),\n",
    "        ModelSpec(\"C\", \"global\", TYPE2_BASE),\n",
    "        ModelSpec(\"D\", \"global\", TYPE2_BASE + TYPE2_DETAIL),\n",
    "    ]\n",
    "\n",
    "# ============================================================\n",
    "# 2) Utils & Metrics\n",
    "# ============================================================\n",
    "def sanitize_features(df: pd.DataFrame, features: List[str]) -> List[str]:\n",
    "    present = [c for c in features if c in df.columns]\n",
    "    missing = [c for c in features if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[warn] Missing features ignored: {missing}\")\n",
    "    return present\n",
    "\n",
    "def load_data(train_path: str, test_path: str, binfo_path: Optional[str]=None, submission_path: Optional[str]=None):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    binfo = pd.read_csv(binfo_path) if binfo_path else None\n",
    "    sub = pd.read_csv(submission_path) if submission_path else None\n",
    "    return train, test, binfo, sub\n",
    "\n",
    "def merge_static(df: pd.DataFrame, binfo: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    if binfo is None:\n",
    "        return df\n",
    "    if BUILDING_COL not in df.columns:\n",
    "        raise KeyError(f\"'{BUILDING_COL}' not in df\")\n",
    "    return df.merge(binfo, on=BUILDING_COL, how=\"left\")\n",
    "\n",
    "def smape(y_true: np.ndarray, y_pred: np.ndarray, eps: float=1e-6) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred)) + eps\n",
    "    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom) * 100.0)\n",
    "\n",
    "def weighted_mse(y_true: np.ndarray, y_pred: np.ndarray, alpha: float=DEFAULT_ALPHA, eps: float=1e-6) -> float:\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    w = (np.abs(y_true) + eps) ** float(alpha)\n",
    "    se = (y_true - y_pred) ** 2\n",
    "    return float(np.average(se, weights=w))\n",
    "\n",
    "def make_search_scorer(metric: str = DEFAULT_SEARCH_METRIC):\n",
    "    metric = metric.lower()\n",
    "    def _score(estimator, X, y):\n",
    "        y_hat = estimator.predict(X)\n",
    "        if metric == \"weighted_mse\":\n",
    "            alpha = estimator.get_params().get(\"meta__alpha\", DEFAULT_ALPHA)\n",
    "            return -weighted_mse(y, y_hat, alpha=alpha)  # sklearn: higher is better\n",
    "        elif metric == \"smape\":\n",
    "            return -smape(y, y_hat)\n",
    "        elif metric == \"mae\":\n",
    "            return -mean_absolute_error(y, y_hat)\n",
    "        elif metric == \"mse\":\n",
    "            return -float(np.mean((y - y_hat) ** 2))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown search metric\")\n",
    "    return _score\n",
    "\n",
    "def _metric_value(y_true: np.ndarray, y_pred: np.ndarray, metric: str, alpha: float) -> float:\n",
    "    metric = metric.lower()\n",
    "    if metric == \"weighted_mse\":\n",
    "        return weighted_mse(y_true, y_pred, alpha=alpha)\n",
    "    if metric == \"smape\":\n",
    "        return smape(y_true, y_pred)\n",
    "    if metric == \"mae\":\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    if metric == \"mse\":\n",
    "        return float(np.mean((y_true - y_pred) ** 2))\n",
    "    raise ValueError(\"Unknown metric\")\n",
    "\n",
    "# ============================================================\n",
    "# 3) Preprocess & Models (+alpha carrier)\n",
    "# ============================================================\n",
    "class AlphaCarrier(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, alpha: float = DEFAULT_ALPHA):\n",
    "        self.alpha = alpha\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "def make_preprocessor(feature_cols: List[str], categorical_cols: List[str]) -> ColumnTransformer:\n",
    "    cats = [c for c in categorical_cols if c in feature_cols]\n",
    "    return ColumnTransformer(\n",
    "        transformers=[(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cats)],\n",
    "        remainder=\"passthrough\",\n",
    "        sparse_threshold=0.0,\n",
    "    )\n",
    "\n",
    "def make_pipeline(algo: str, feature_cols: List[str], categorical_cols: List[str], random_state: int, alpha: float=DEFAULT_ALPHA) -> Pipeline:\n",
    "    meta = (\"meta\", AlphaCarrier(alpha=alpha))\n",
    "    # LGBM, MLP 모델에만 전처리기 사용\n",
    "    if algo in [\"lgbm\", \"mlp\"]:\n",
    "        if not any(c in feature_cols for c in categorical_cols):\n",
    "            pre = (\"prep\", \"passthrough\")\n",
    "        else:\n",
    "            pre = (\"prep\", make_preprocessor(feature_cols, categorical_cols))\n",
    "    # XGBoost는 전처리기 없이 바로 모델로 연결\n",
    "    else: # algo == \"xgb\"\n",
    "        pre = (\"prep\", \"passthrough\")\n",
    "\n",
    "    if algo == \"xgb\":\n",
    "        est = XGBRegressor(\n",
    "            n_estimators=1000, learning_rate=0.05, max_depth=8,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            tree_method=\"hist\", random_state=random_state, n_jobs=-1,\n",
    "            enable_categorical=True\n",
    "        )\n",
    "        return Pipeline([meta, pre, (\"est\", est)])\n",
    "    if algo == \"lgbm\":\n",
    "        est = LGBMRegressor(\n",
    "            n_estimators=1000, learning_rate=0.05, num_leaves=64,\n",
    "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
    "            random_state=random_state, n_jobs=-1\n",
    "        )\n",
    "        return Pipeline([meta, pre, (\"est\", est)])\n",
    "    if algo == \"mlp\":\n",
    "        scaler = (\"scaler\", StandardScaler())\n",
    "        est = (\"est\", MLPRegressor(\n",
    "            hidden_layer_sizes=(256, 128), activation=\"relu\",\n",
    "            solver=\"adam\", learning_rate_init=1e-3, max_iter=500,\n",
    "            early_stopping=MLP_EARLY_STOP, n_iter_no_change=MLP_N_NO_CHANGE,\n",
    "            validation_fraction=MLP_VAL_FRAC, random_state=random_state\n",
    "        ))\n",
    "        return Pipeline([meta, pre, scaler, est])\n",
    "    raise ValueError(f\"Unknown algo: {algo}\")\n",
    "\n",
    "def default_search_spaces() -> Dict[str, Dict[str, List]]:\n",
    "    alpha_grid = [3.0] #\n",
    "    return {\n",
    "        \"xgb\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__n_estimators\": [600, 800, 1200, 1600],\n",
    "            \"est__learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
    "            \"est__max_depth\": [4, 6, 8, 10],\n",
    "            \"est__subsample\": [0.6, 0.8, 1.0],\n",
    "            \"est__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "            \"est__reg_lambda\": [0.0, 0.5, 1.0, 2.0],\n",
    "        },\n",
    "        \"lgbm\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__n_estimators\": [600, 800, 1200, 1600],\n",
    "            \"est__learning_rate\": [0.03, 0.05, 0.07, 0.1],\n",
    "            \"est__num_leaves\": [31, 63, 127, 255],\n",
    "            \"est__subsample\": [0.6, 0.8, 1.0],\n",
    "            \"est__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "            \"est__reg_lambda\": [0.0, 0.5, 1.0, 2.0],\n",
    "        },\n",
    "        \"mlp\": {\n",
    "            \"meta__alpha\": alpha_grid,\n",
    "            \"est__hidden_layer_sizes\": [(256,), (256,128), (512,256), (512,256,128)],\n",
    "            \"est__learning_rate_init\": [5e-4, 1e-3, 2e-3],\n",
    "            \"est__alpha\": [1e-5, 1e-4, 1e-3],\n",
    "            \"est__max_iter\": [300, 500, 800],\n",
    "        },\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 4) CV Splitter\n",
    "# ============================================================\n",
    "def make_splitter(cv_type: str=DEFAULT_CV_TYPE, n_splits: int=DEFAULT_N_SPLITS, gap: int=DEFAULT_GAP):\n",
    "    if cv_type.lower() == \"kfold\":\n",
    "        return KFold(n_splits=n_splits, shuffle=True, random_state=DEFAULT_RANDOM_STATE)\n",
    "    if cv_type.lower() == \"timesplit\":\n",
    "        return TimeSeriesSplit(n_splits=n_splits, gap=gap)\n",
    "    raise ValueError(\"cv_type must be 'kfold' or 'timesplit'\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) HPO Backends (random / optuna / bayes) with Early Stopping per fold\n",
    "# ============================================================\n",
    "def _fit_with_es(est: Pipeline, algo: str, X_tr, y_tr, X_val=None, y_val=None, verbose: int=0):\n",
    "    fit_params = {}\n",
    "    if algo == \"xgb\":\n",
    "        if X_val is not None and y_val is not None:\n",
    "            est.set_params(\n",
    "                est__early_stopping_rounds=ES_ROUNDS_XGB\n",
    "            )\n",
    "            fit_params.update({\n",
    "                \"est__eval_set\": [(X_val, y_val)],\n",
    "                # \"est__early_stopping_rounds\": ES_ROUNDS_XGB,\n",
    "                \"est__verbose\": verbose,\n",
    "            })\n",
    "    elif algo == \"lgbm\":\n",
    "        if X_val is not None and y_val is not None:\n",
    "            est.set_params(\n",
    "                est__early_stopping_rounds=ES_ROUNDS_LGBM\n",
    "            )\n",
    "            fit_params.update({\n",
    "                \"est__eval_set\": [(X_val, y_val)],\n",
    "                #\"est__early_stopping_rounds\": ES_ROUNDS_LGBM,\n",
    "                \"est__verbose\": verbose,\n",
    "            })\n",
    "    # MLP early stopping already in params\n",
    "    est.fit(X_tr, y_tr, **fit_params)\n",
    "    return est\n",
    "\n",
    "def _sample_from_list(rng, values):\n",
    "    return values[rng.integers(0, len(values))]\n",
    "\n",
    "def run_random_search_cv(est, X, y, param_space, cv, search_metric, algo, n_iter=50, random_state=42):\n",
    "    \"\"\"Manual random search so we can pass per-fold eval_set for early stopping.\"\"\"\n",
    "    rng = np.random.default_rng(seed=random_state)\n",
    "    scorer = make_search_scorer(search_metric)\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_params = {}\n",
    "    for t in range(n_iter):\n",
    "        # Random param dict\n",
    "        params = {k: _sample_from_list(rng, v) for k, v in param_space.items()}\n",
    "        est_ = clone(est).set_params(**params)\n",
    "\n",
    "        # inner-CV with ES\n",
    "        fold_scores = []\n",
    "        for tr_idx, va_idx in cv.split(X):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "            est_fold = clone(est_)\n",
    "\n",
    "            _fit_with_es(est_fold, algo, X_tr, y_tr, X_va, y_va, verbose=0)\n",
    "            fold_scores.append(scorer(est_fold, X_va, y_va))\n",
    "\n",
    "        mean_sc = float(np.mean(fold_scores))\n",
    "        if mean_sc > best_score:\n",
    "            best_score, best_params = mean_sc, params\n",
    "\n",
    "    best_est = clone(est).set_params(**best_params)\n",
    "    # fit on full data w/o val (or small split in final-refit step)\n",
    "    best_est.fit(X, y)\n",
    "    return best_est, best_params\n",
    "\n",
    "def run_optuna_cv(est, X, y, param_space, cv, search_metric, algo, n_iter=50, random_state=42):\n",
    "    import optuna\n",
    "    from sklearn.base import clone\n",
    "    scorer = make_search_scorer(search_metric)\n",
    "\n",
    "    def suggest_from_list(trial, name, values):\n",
    "        v0 = values[0]\n",
    "        # try numeric ranges\n",
    "        if all(isinstance(v, (int, np.integer)) for v in values):\n",
    "            return trial.suggest_int(name, int(min(values)), int(max(values)))\n",
    "        if all(isinstance(v, (float, np.floating)) for v in values):\n",
    "            return trial.suggest_float(name, float(min(values)), float(max(values)))\n",
    "        return trial.suggest_categorical(name, values)\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {k: suggest_from_list(trial, k, vals) for k, vals in param_space.items()}\n",
    "        est_ = clone(est).set_params(**params)\n",
    "        scores = []\n",
    "        for tr_idx, va_idx in cv.split(X):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "            est_fold = clone(est_)\n",
    "            _fit_with_es(est_fold, algo, X_tr, y_tr, X_va, y_va, verbose=0)\n",
    "            score = scorer(est_fold, X_va, y_va)\n",
    "            scores.append(score)\n",
    "            trial.report(score, step=len(scores))\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner, sampler=optuna.samplers.TPESampler(seed=random_state))\n",
    "    study.optimize(objective, n_trials=n_iter, gc_after_trial=True)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_est = clone(est).set_params(**best_params).fit(X, y)\n",
    "    return best_est, best_params\n",
    "\n",
    "def run_bayes_cv(est, X, y, param_space, cv, search_metric, algo, n_iter=50, random_state=42):\n",
    "    \"\"\"Bayesian optimization via skopt.gp_minimize with Categorical dims (simple & robust).\"\"\"\n",
    "    from skopt import gp_minimize\n",
    "    from skopt.space import Categorical\n",
    "    from sklearn.base import clone\n",
    "    scorer = make_search_scorer(search_metric)\n",
    "\n",
    "    dims = []\n",
    "    keys = []\n",
    "    for k, vals in param_space.items():\n",
    "        dims.append(Categorical(vals, name=k))\n",
    "        keys.append(k)\n",
    "\n",
    "    def objective(list_vals):\n",
    "        params = {k: v for k, v in zip(keys, list_vals)}\n",
    "        est_ = clone(est).set_params(**params)\n",
    "        scores = []\n",
    "        for tr_idx, va_idx in cv.split(X):\n",
    "            X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X.iloc[va_idx], y[va_idx]\n",
    "            est_fold = clone(est_)\n",
    "            _fit_with_es(est_fold, algo, X_tr, y_tr, X_va, y_va, verbose=0)\n",
    "            score = scorer(est_fold, X_va, y_va)\n",
    "            scores.append(score)\n",
    "        # gp_minimize minimizes → return negative (we want to maximize scorer)\n",
    "        return -float(np.mean(scores))\n",
    "\n",
    "    res = gp_minimize(\n",
    "        objective, dimensions=dims, n_calls=n_iter, random_state=random_state, verbose=False\n",
    "    )\n",
    "    best_params = {k: v for k, v in zip(keys, res.x)}\n",
    "    best_est = clone(est).set_params(**best_params).fit(X, y)\n",
    "    return best_est, best_params\n",
    "\n",
    "def run_hpo_backend(est, X, y, param_space, cv, search_metric, backend, algo, n_iter, random_state=42):\n",
    "    backend = backend.lower()\n",
    "    if backend == \"random\":\n",
    "        return run_random_search_cv(est, X, y, param_space, cv, search_metric, algo, n_iter=n_iter, random_state=random_state)\n",
    "    if backend == \"optuna\":\n",
    "        return run_optuna_cv(est, X, y, param_space, cv, search_metric, algo, n_iter=n_iter, random_state=random_state)\n",
    "    if backend == \"bayes\":\n",
    "        return run_bayes_cv(est, X, y, param_space, cv, search_metric, algo, n_iter=n_iter, random_state=random_state)\n",
    "    raise ValueError(\"backend must be 'random'|'optuna'|'bayes'\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) OOF Training + Final Refit with ES + Predictions\n",
    "# ============================================================\n",
    "def oof_fit_predict(\n",
    "    pipeline: Pipeline,\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    X_test: pd.DataFrame,\n",
    "    do_search: bool,\n",
    "    algo: str,\n",
    "    param_spaces: Dict[str, Dict[str, List]],\n",
    "    outer_splitter,\n",
    "    inner_cv_type: str,\n",
    "    inner_splits: int,\n",
    "    random_state: int,\n",
    "    search_metric: str,\n",
    "    search_backend: str,\n",
    "    search_n_iter: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, Pipeline, List[Dict], List[float]]:\n",
    "    \"\"\"Return OOF, test_pred(mean-of-fold), last_est, fold best_params, fold metric values.\"\"\"\n",
    "    oof = np.zeros(len(X))\n",
    "    test_fold_preds = []\n",
    "    last_est = None\n",
    "    fold_smapes = []\n",
    "    fold_best_params: List[Dict] = []\n",
    "    fold_metric_vals: List[float] = []\n",
    "\n",
    "    for fold, (tr_idx, val_idx) in enumerate(outer_splitter.split(X)):\n",
    "        X_tr, y_tr = X.iloc[tr_idx], y[tr_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y[val_idx]\n",
    "\n",
    "        est = clone(pipeline)\n",
    "        chosen_params = {}\n",
    "        if do_search:\n",
    "            inner_splitter = make_splitter(cv_type=inner_cv_type, n_splits=inner_splits, gap=0)\n",
    "            est, chosen_params = run_hpo_backend(\n",
    "                est, X_tr, y_tr,\n",
    "                param_space=param_spaces.get(algo, {}),\n",
    "                cv=inner_splitter,\n",
    "                search_metric=search_metric,\n",
    "                backend=search_backend,\n",
    "                algo=algo,\n",
    "                n_iter=search_n_iter,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        else:\n",
    "            # no search: just fit with ES on val\n",
    "            est = _fit_with_es(est, algo, X_tr, y_tr, X_val, y_val, verbose=0)\n",
    "\n",
    "        # Validate\n",
    "        y_hat_val = est.predict(X_val)\n",
    "        oof[val_idx] = y_hat_val\n",
    "        sm = smape(y_val, y_hat_val)\n",
    "        alpha_here = est.get_params().get(\"meta__alpha\", DEFAULT_ALPHA)\n",
    "        mv = _metric_value(y_val, y_hat_val, search_metric, alpha_here)\n",
    "        fold_smapes.append(sm)\n",
    "        fold_metric_vals.append(mv)\n",
    "        fold_best_params.append(chosen_params)\n",
    "        print(f\"[val] fold {fold+1}/{getattr(outer_splitter, 'n_splits', '?')}  SMAPE={sm:.3f}% | {search_metric}={mv:.6f}\")\n",
    "\n",
    "        # Test via current fold model (for completeness; final_refit will also run)\n",
    "        test_fold_preds.append(est.predict(X_test))\n",
    "        last_est = est\n",
    "\n",
    "    print(f\"[val] mean SMAPE: {np.mean(fold_smapes):.3f}% (±{np.std(fold_smapes):.3f})\")\n",
    "    test_pred = np.mean(test_fold_preds, axis=0)\n",
    "    return oof, test_pred, last_est, fold_best_params, fold_metric_vals\n",
    "\n",
    "def _final_refit_with_holdout(est: Pipeline, algo: str, X: pd.DataFrame, y: np.ndarray, holdout_frac: float=0.1):\n",
    "    \"\"\"Time-based final refit with early stopping using last `holdout_frac` of rows.\"\"\"\n",
    "    # n = len(X)\n",
    "    # if n >= 20 and holdout_frac > 0:\n",
    "    #     k = max(int(n * (1 - holdout_frac)), n - 48)  # at least 48 points in holdout if possible\n",
    "    #     X_tr, y_tr = X.iloc[:k], y[:k]\n",
    "    #     X_val, y_val = X.iloc[k:], y[k:]\n",
    "    #     est = _fit_with_es(est, algo, X_tr, y_tr, X_val, y_val, verbose=0)\n",
    "    # else:\n",
    "    #     est.fit(X, y)  # fallback\n",
    "    return est.fit(X, y) #est\n",
    "\n",
    "def train_predict_for_spec(\n",
    "    spec: ModelSpec,\n",
    "    algo: str,\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    do_search: bool,\n",
    "    seeds: List[int],\n",
    "    cv_type: str,\n",
    "    n_splits: int,\n",
    "    param_spaces: Dict[str, Dict[str, List]],\n",
    "    gap: int,\n",
    "    inner_cv_type: Optional[str],\n",
    "    inner_splits: Optional[int],\n",
    "    search_metric: str,\n",
    "    search_backend: str,\n",
    "    search_n_iter: int,\n",
    "    do_final_refit: bool=True,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Returns (pred_df, oof_df) with test predictions from final refit models.\"\"\"\n",
    "    inner_cv_type = inner_cv_type or cv_type\n",
    "    inner_splits = inner_splits or 3\n",
    "    feat_cols = sanitize_features(train_df, spec.features)\n",
    "    categorical = [\"building_type\"] if spec.scope == \"global\" else []\n",
    "\n",
    "    def _pipe(rs: int) -> Pipeline:\n",
    "        return make_pipeline(algo=algo, feature_cols=feat_cols, categorical_cols=categorical, random_state=rs, alpha=DEFAULT_ALPHA)\n",
    "\n",
    "    all_pred_parts = []\n",
    "    all_oof_parts = []\n",
    "\n",
    "    if spec.scope == \"per_building\":\n",
    "        for b_id, tr_b in train_df.groupby(BUILDING_COL):\n",
    "            te_b = test_df.loc[test_df[BUILDING_COL] == b_id]\n",
    "            if te_b.empty:\n",
    "                continue\n",
    "            X_tr = tr_b[feat_cols]\n",
    "            y_tr = tr_b[TARGET_COL].values\n",
    "            X_te = te_b[feat_cols]\n",
    "            outer_splitter = make_splitter(cv_type=cv_type, n_splits=n_splits, gap=gap)\n",
    "\n",
    "            seed_oofs = []\n",
    "            seed_final_preds = []\n",
    "            for s in seeds:\n",
    "                pipe = _pipe(s)\n",
    "                oof, _, last_est, fold_params, fold_scores = oof_fit_predict(\n",
    "                    pipe, X_tr, y_tr, X_te, do_search, algo, param_spaces,\n",
    "                    outer_splitter, inner_cv_type, inner_splits, s,\n",
    "                    search_metric=search_metric,\n",
    "                    search_backend=search_backend,\n",
    "                    search_n_iter=search_n_iter,\n",
    "                )\n",
    "                seed_oofs.append(oof)\n",
    "\n",
    "                best_idx = int(np.argmin(fold_scores)) if fold_scores else 0\n",
    "                best_params = fold_params[best_idx] if fold_params else {}\n",
    "\n",
    "                if do_final_refit:\n",
    "                    final_model = clone(_pipe(s))\n",
    "                    if best_params:\n",
    "                        final_model.set_params(**best_params)\n",
    "                    final_model = _final_refit_with_holdout(final_model, algo, X_tr, y_tr, holdout_frac=0.1)\n",
    "                    train_pred_full = final_model.predict(X_tr)\n",
    "                    f_smape = smape(y_tr, train_pred_full)\n",
    "                    f_r2 = r2_score(y_tr, train_pred_full)\n",
    "                    print(f\"[final-fit][Spec {spec.name} | Algo {algo} | B{b_id}] Final Train SMAPE={f_smape:.4f}% | R2={f_r2:.4f} | Params={best_params}\")\n",
    "                    seed_final_preds.append(final_model.predict(X_te))\n",
    "                else:\n",
    "                    seed_final_preds.append(last_est.predict(X_te))\n",
    "\n",
    "            te_avg = np.mean(seed_final_preds, axis=0)\n",
    "            oof_avg = np.mean(seed_oofs, axis=0)\n",
    "\n",
    "            pred_b = te_b[[TIME_COL, BUILDING_COL]].copy()\n",
    "            pred_b[f\"pred_{spec.name}_{algo}\"] = te_avg\n",
    "            all_pred_parts.append(pred_b)\n",
    "\n",
    "            oof_b = tr_b[[TIME_COL, BUILDING_COL]].copy()\n",
    "            oof_b[\"oof\"] = oof_avg\n",
    "            all_oof_parts.append(oof_b)\n",
    "\n",
    "        pred_df = pd.concat(all_pred_parts, ignore_index=True)\n",
    "        oof_df = pd.concat(all_oof_parts, ignore_index=True)\n",
    "\n",
    "    else:  # global\n",
    "        X_tr = train_df[feat_cols]; y_tr = train_df[TARGET_COL].values\n",
    "        X_te = test_df[feat_cols]\n",
    "        outer_splitter = make_splitter(cv_type=cv_type, n_splits=n_splits, gap=gap)\n",
    "\n",
    "        seed_oofs = []\n",
    "        seed_final_preds = []\n",
    "        for s in seeds:\n",
    "            pipe = _pipe(s)\n",
    "            oof, _, last_est, fold_params, fold_scores = oof_fit_predict(\n",
    "                pipe, X_tr, y_tr, X_te, do_search, algo, param_spaces,\n",
    "                outer_splitter, inner_cv_type, inner_splits, s,\n",
    "                search_metric=search_metric,\n",
    "                search_backend=search_backend,\n",
    "                search_n_iter=search_n_iter,\n",
    "            )\n",
    "            seed_oofs.append(oof)\n",
    "\n",
    "            best_idx = int(np.argmin(fold_scores)) if fold_scores else 0\n",
    "            best_params = fold_params[best_idx] if fold_params else {}\n",
    "\n",
    "            if do_final_refit:\n",
    "                final_model = clone(_pipe(s))\n",
    "                if best_params:\n",
    "                    final_model.set_params(**best_params)\n",
    "                final_model = _final_refit_with_holdout(final_model, algo, X_tr, y_tr, holdout_frac=0.1)\n",
    "                train_pred_full = final_model.predict(X_tr)\n",
    "                f_smape = smape(y_tr, train_pred_full)\n",
    "                f_r2 = r2_score(y_tr, train_pred_full)\n",
    "                print(f\"[final-fit][Spec {spec.name} | Algo {algo} | GLOBAL] Final Train SMAPE={f_smape:.4f}% | R2={f_r2:.4f} | Params={best_params}\")\n",
    "                seed_final_preds.append(final_model.predict(X_te))\n",
    "            else:\n",
    "                seed_final_preds.append(last_est.predict(X_te))\n",
    "\n",
    "        te_avg = np.mean(seed_final_preds, axis=0)\n",
    "        oof_avg = np.mean(seed_oofs, axis=0)\n",
    "\n",
    "        pred_df = test_df[[TIME_COL, BUILDING_COL]].copy()\n",
    "        pred_df[f\"pred_{spec.name}_{algo}\"] = te_avg\n",
    "\n",
    "        oof_df = train_df[[TIME_COL, BUILDING_COL]].copy()\n",
    "        oof_df[\"oof\"] = oof_avg\n",
    "\n",
    "    return pred_df, oof_df\n",
    "\n",
    "# ============================================================\n",
    "# 7) Ensembling\n",
    "# ============================================================\n",
    "from functools import reduce\n",
    "def simple_mean_ensemble(pred_dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    merged = reduce(lambda l, r: l.merge(r, on=[TIME_COL, BUILDING_COL], how=\"left\"), pred_dfs)\n",
    "    pred_cols = [c for c in merged.columns if c.startswith(\"pred_\")]\n",
    "    merged[\"answer\"] = merged[pred_cols].mean(axis=1)\n",
    "    return merged[[TIME_COL, BUILDING_COL, \"answer\"]]\n",
    "\n",
    "def stacking_ensemble(\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    oof_pack: List[Tuple[pd.DataFrame, str]],\n",
    "    pred_pack: List[Tuple[pd.DataFrame, str]],\n",
    "    meta_model = None,\n",
    ") -> pd.DataFrame:\n",
    "    meta_model = meta_model or Ridge(alpha=1.0, random_state=DEFAULT_RANDOM_STATE)\n",
    "    # Build oof matrix\n",
    "    oof_merged = None; names = []\n",
    "    for df, name in oof_pack:\n",
    "        names.append(name)\n",
    "        df2 = df.rename(columns={\"oof\": name})\n",
    "        oof_merged = df2 if oof_merged is None else oof_merged.merge(df2, on=[TIME_COL, BUILDING_COL], how=\"left\")\n",
    "    oof_merged = oof_merged.merge(train_df[[TIME_COL, BUILDING_COL, TARGET_COL]], on=[TIME_COL, BUILDING_COL], how=\"left\")\n",
    "\n",
    "    pred_merged = None\n",
    "    for df, name in pred_pack:\n",
    "        df2 = df.rename(columns={name: name})\n",
    "        pred_merged = df2 if pred_merged is None else pred_merged.merge(df2, on=[TIME_COL, BUILDING_COL], how=\"left\")\n",
    "\n",
    "    X_meta = oof_merged[names].values\n",
    "    y_meta = oof_merged[TARGET_COL].values\n",
    "    X_meta_test = pred_merged[names].values\n",
    "\n",
    "    meta_model.fit(X_meta, y_meta)\n",
    "    meta_pred = meta_model.predict(X_meta_test)\n",
    "\n",
    "    out = pred_merged[[TIME_COL, BUILDING_COL]].copy()\n",
    "    out[\"answer\"] = meta_pred\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 8) Stage Runner + Backend auto-select + Save helpers\n",
    "# ============================================================\n",
    "def _backend_for_stage(stage: int) -> str:\n",
    "    if FORCE_BACKEND:\n",
    "        return FORCE_BACKEND\n",
    "    return HPO_BACKEND_FOR_STAGE.get(stage, \"random\")\n",
    "\n",
    "def _budget_for(scope: str, backend: str) -> int:\n",
    "    return BUDGETS[scope][backend]\n",
    "\n",
    "def run_stage(\n",
    "    stage: int,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    submission_template: pd.DataFrame,\n",
    "    algorithms: List[str],\n",
    "    do_search: bool,\n",
    "    seeds: List[int],\n",
    "    cv_type: str,\n",
    "    n_splits: int,\n",
    "    param_spaces: Dict[str, Dict[str, List]],\n",
    "    stacking: bool = False,\n",
    "    gap: int = DEFAULT_GAP,\n",
    "    inner_cv_type: Optional[str] = None,\n",
    "    inner_splits: Optional[int] = None,\n",
    "    search_metric: str = DEFAULT_SEARCH_METRIC,\n",
    "    force_backend: Optional[str] = None,\n",
    ") -> Tuple[pd.DataFrame, List[pd.DataFrame], List[Tuple[pd.DataFrame, str]]]:\n",
    "    specs = build_model_specs()\n",
    "    preds = []; oofs = []\n",
    "\n",
    "    # backend choice\n",
    "    backend = force_backend or _backend_for_stage(stage)\n",
    "\n",
    "    for spec in specs:\n",
    "        # per-building vs global budget\n",
    "        search_n_iter = _budget_for(spec.scope, backend)\n",
    "        for algo in algorithms:\n",
    "            print(f\"\\n=== Training Spec={spec.name}({spec.scope}) Algo={algo} | backend={backend} trials={search_n_iter} ===\")\n",
    "            pred_df, oof_df = train_predict_for_spec(\n",
    "                spec=spec, algo=algo, train_df=train, test_df=test,\n",
    "                do_search=do_search, seeds=seeds,\n",
    "                cv_type=cv_type, n_splits=n_splits,\n",
    "                param_spaces=param_spaces, gap=gap,\n",
    "                inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "                search_metric=search_metric, search_backend=backend,\n",
    "                search_n_iter=search_n_iter,\n",
    "                do_final_refit=True,\n",
    "            )\n",
    "            preds.append(pred_df)\n",
    "            oofs.append((oof_df, f\"{spec.name}_{algo}\"))\n",
    "\n",
    "    if stacking:\n",
    "        # Build pred_pack with unified names\n",
    "        pred_pack = []\n",
    "        for (oof_df, name), pred_df in zip(oofs, preds):\n",
    "            col = [c for c in pred_df.columns if c.startswith(\"pred_\")][0]\n",
    "            pred_pack.append((pred_df.rename(columns={col: name}), name))\n",
    "        final = stacking_ensemble(train, test, oof_pack=oofs, pred_pack=pred_pack, meta_model=Ridge(alpha=1.0))\n",
    "    else:\n",
    "        final = simple_mean_ensemble(preds)\n",
    "\n",
    "    sub = submission_template.copy()\n",
    "    if TIME_COL in sub.columns:\n",
    "        out = sub[[TIME_COL]].merge(final[[TIME_COL, \"answer\"]], on=TIME_COL, how=\"left\")\n",
    "    else:\n",
    "        raise KeyError(\"submission_template must include TIME_COL\")\n",
    "\n",
    "    return out, preds, oofs\n",
    "\n",
    "# ---- Saving helpers ----\n",
    "def make_run_dir(tag: str = \"v1\") -> str:\n",
    "    dt = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    run_dir = pathlib.Path(\"runs\") / f\"{dt}_{tag}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return str(run_dir)\n",
    "\n",
    "def save_artifacts(run_dir: str, final_sub: pd.DataFrame, stage_name: str, preds: List[pd.DataFrame], oofs: List[Tuple[pd.DataFrame, str]], config: dict):\n",
    "    # submission\n",
    "    f_sub = os.path.join(run_dir, f\"submission_{stage_name}.csv\")\n",
    "    final_sub.to_csv(f_sub, index=False)\n",
    "    print(f\"[save] {f_sub}\")\n",
    "\n",
    "    # preds & oofs\n",
    "    for i, df in enumerate(preds):\n",
    "        df.to_csv(os.path.join(run_dir, f\"preds_{stage_name}_{i}.csv\"), index=False)\n",
    "    for (df, name) in oofs:\n",
    "        df.to_csv(os.path.join(run_dir, f\"oof_{stage_name}_{name}.csv\"), index=False)\n",
    "\n",
    "    # config\n",
    "    with open(os.path.join(run_dir, f\"config_{stage_name}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[save] artifacts saved to {run_dir}\")\n",
    "\n",
    "# ============================================================\n",
    "# 9) Stage Shortcuts\n",
    "# ============================================================\n",
    "def stage_1_xgb_mean(train, test, sub, **kwargs):\n",
    "    return run_stage(1, train, test, sub, algorithms=[\"xgb\"], do_search=False, seeds=[DEFAULT_RANDOM_STATE], **kwargs)\n",
    "\n",
    "def stage_2_xgb_search_mean(train, test, sub, **kwargs):\n",
    "    return run_stage(2, train, test, sub, algorithms=[\"xgb\"], do_search=True, seeds=[DEFAULT_RANDOM_STATE], **kwargs)\n",
    "\n",
    "def stage_3_triple_mean(train, test, sub, **kwargs):\n",
    "    return run_stage(3, train, test, sub, algorithms=[\"xgb\",\"lgbm\",\"mlp\"], do_search=False, seeds=[DEFAULT_RANDOM_STATE], **kwargs)\n",
    "\n",
    "def stage_4_triple_search_mean(train, test, sub, **kwargs):\n",
    "    return run_stage(4, train, test, sub, algorithms=[\"xgb\",\"lgbm\",\"mlp\"], do_search=True, seeds=[DEFAULT_RANDOM_STATE], **kwargs)\n",
    "\n",
    "def stage_5_triple_search_seedavg_mean(train, test, sub, seeds: List[int]=DEFAULT_SEEDS, **kwargs):\n",
    "    return run_stage(5, train, test, sub, algorithms=[\"xgb\",\"lgbm\",\"mlp\"], do_search=True, seeds=seeds, **kwargs)\n",
    "\n",
    "def stage_6_triple_search_seedavg_stacking(train, test, sub, seeds: List[int]=DEFAULT_SEEDS, **kwargs):\n",
    "    return run_stage(6, train, test, sub, algorithms=[\"xgb\",\"lgbm\",\"mlp\"], do_search=True, seeds=seeds, stacking=True, **kwargs)\n",
    "\n",
    "# ============================================================\n",
    "# 10) Example main\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    TRAIN_PATH = \"../data/raw/train.csv\"\n",
    "    TEST_PATH = \"../data/raw/test.csv\"\n",
    "    BINFO_PATH = \"../data/raw/building_info.csv\"     # or None\n",
    "    SUB_PATH = \"../data/raw/sample_submission.csv\"\n",
    "\n",
    "    # Load & merge\n",
    "    _, _, binfo, sub = load_data(TRAIN_PATH, TEST_PATH, BINFO_PATH, SUB_PATH)\n",
    "    train = train\n",
    "    test  = test\n",
    "\n",
    "    # CV / search config\n",
    "    PARAM_SPACES = default_search_spaces()\n",
    "    cv_type = \"kfold\"   # or 'timesplit'\n",
    "    n_splits = 5\n",
    "    gap = 0\n",
    "    inner_cv_type = cv_type\n",
    "    inner_splits = 3\n",
    "    search_metric = DEFAULT_SEARCH_METRIC  # 'weighted_mse'|'smape'|'mae'|'mse'\n",
    "\n",
    "    # # Choose stage here:\n",
    "    # final_sub, preds, oofs = stage_1_xgb_mean(train, test, sub,\n",
    "    #     cv_type=cv_type, n_splits=n_splits, param_spaces=PARAM_SPACES,\n",
    "    #     gap=gap, inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "    #     search_metric=search_metric, force_backend=None)\n",
    "\n",
    "    final_sub, preds, oofs = stage_2_xgb_search_mean(train, test, sub,\n",
    "        cv_type=cv_type, n_splits=n_splits, param_spaces=PARAM_SPACES,\n",
    "        gap=gap, inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "        search_metric=search_metric, force_backend=None)\n",
    "\n",
    "    # final_sub, preds, oofs = stage_3_triple_mean(train, test, sub,\n",
    "    #     cv_type=cv_type, n_splits=n_splits, param_spaces=PARAM_SPACES,\n",
    "    #     gap=gap, inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "    #     search_metric=search_metric, force_backend=None)\n",
    "    #\n",
    "    # final_sub, preds, oofs = stage_4_triple_search_mean(train, test, sub,\n",
    "    #     cv_type=cv_type, n_splits=n_splits, param_spaces=PARAM_SPACES,\n",
    "    #     gap=gap, inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "    #     search_metric=search_metric, force_backend=None)\n",
    "\n",
    "    # final_sub, preds, oofs = stage_5_triple_search_seedavg_mean(train, test, sub, seeds=DEFAULT_SEEDS,\n",
    "    #     cv_type=cv_type, n_splits=n_splits, param_spaces=PARAM_SPACES,\n",
    "    #     gap=gap, inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "    #     search_metric=search_metric, force_backend=None)\n",
    "\n",
    "    # final_sub, preds, oofs = stage_6_triple_search_seedavg_stacking(train, test, sub, seeds=DEFAULT_SEEDS,\n",
    "    #     cv_type=cv_type, n_splits=n_splits, param_spaces=PARAM_SPACES,\n",
    "    #     gap=gap, inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "    #     search_metric=search_metric, force_backend=None)\n",
    "\n",
    "    # Save under runs/{date_tag}/\n",
    "    run_dir = make_run_dir(tag=\"stage2\")\n",
    "    cfg = dict(\n",
    "        stage=\"2\",\n",
    "        cv_type=cv_type, n_splits=n_splits, gap=gap,\n",
    "        inner_cv_type=inner_cv_type, inner_splits=inner_splits,\n",
    "        search_metric=search_metric,\n",
    "        backend=FORCE_BACKEND or HPO_BACKEND_FOR_STAGE[2],\n",
    "        budgets=BUDGETS,\n",
    "    )\n",
    "    save_artifacts(run_dir, final_sub, \"stage2\", preds, oofs, cfg)"
   ],
   "id": "7d47b0dbe7715815",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
